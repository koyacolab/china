{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvpNHOs_nTQK",
    "outputId": "58e5e73c-d790-40a6-a1dc-1f6638a75f55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive/\n",
      "gdrive\tsample_data\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pDXvdYFuLYSI",
    "outputId": "db9c1dcc-d2d2-450c-bab9-29f775f571d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hy-tmp\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "home_dir = '/content/gdrive/My Drive/AChina' \n",
    "home_dir = '/hy-tmp'\n",
    "os.chdir(home_dir)\n",
    "!pwd\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WPwRIbsMnaq8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "\n",
    "# os.chdir(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYbhRWaxneDl",
    "outputId": "c79dccdb-c661-4f1c-ca13-a14d7bd2c1c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (1.8.1)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scipy) (1.22.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.12.0+cu113)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.8/dist-packages (1.8.4.post0)\n",
      "Requirement already satisfied: pytorch_forecasting in /usr/local/lib/python3.8/dist-packages (0.10.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: tensorboardX>=2.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2.5.1)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.22.3)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (0.11.0)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2022.11.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (21.3)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: lightning-utilities!=0.4.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.64.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (3.5.2)\n",
      "Requirement already satisfied: scipy<2.0,>=1.8 in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (1.8.1)\n",
      "Requirement already satisfied: scikit-learn<1.2,>=0.24 in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (1.1.1)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (0.13.5)\n",
      "Requirement already satisfied: optuna<3.0.0,>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (2.10.1)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (1.4.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.28.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.3)\n",
      "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch_forecasting) (0.9.0)\n",
      "Requirement already satisfied: cliff in /usr/local/lib/python3.8/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch_forecasting) (4.1.0)\n",
      "Requirement already satisfied: alembic in /usr/local/lib/python3.8/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch_forecasting) (1.8.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch_forecasting) (1.4.45)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.8/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch_forecasting) (6.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas<2.0.0,>=1.3.0->pytorch_forecasting) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas<2.0.0,>=1.3.0->pytorch_forecasting) (2022.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<1.2,>=0.24->pytorch_forecasting) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<1.2,>=0.24->pytorch_forecasting) (1.1.0)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=2.2->pytorch-lightning) (3.19.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pytorch_forecasting) (9.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pytorch_forecasting) (4.34.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pytorch_forecasting) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pytorch_forecasting) (1.4.3)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.8/dist-packages (from statsmodels->pytorch_forecasting) (0.5.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from patsy>=0.5.2->statsmodels->pytorch_forecasting) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy>=1.1.0->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (2.0.1)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (5.8.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (4.12.0)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.8/dist-packages (from alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (1.2.4)\n",
      "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (0.5.1)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (3.5.0)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (4.1.1)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (2.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.10)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (0.2.5)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (1.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (3.8.1)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from stevedore>=2.0.1->cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (5.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from Mako->alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (2.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install torch torchvision\n",
    "# !pip install pytorch_lightning\n",
    "# !pip install pytorch_forecasting\n",
    "\n",
    "# !pip install torch -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install pytorch-forecasting\n",
    "\n",
    "!pip install scipy\n",
    "!pip install torch pytorch-lightning pytorch_forecasting\n",
    "\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAPE, SMAPE, PoissonLoss, QuantileLoss\n",
    "# from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "65_fJ6MIbncZ",
    "outputId": "06e9c0ab-8d58-480b-86d1-4ca275159e4a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>years</th>\n",
       "      <th>yield</th>\n",
       "      <th>county</th>\n",
       "      <th>bands</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>bin0</th>\n",
       "      <th>bin1</th>\n",
       "      <th>bin2</th>\n",
       "      <th>bin3</th>\n",
       "      <th>...</th>\n",
       "      <th>bin502</th>\n",
       "      <th>bin503</th>\n",
       "      <th>bin504</th>\n",
       "      <th>bin505</th>\n",
       "      <th>bin506</th>\n",
       "      <th>bin507</th>\n",
       "      <th>bin508</th>\n",
       "      <th>bin509</th>\n",
       "      <th>bin510</th>\n",
       "      <th>bin511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 518 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  years    yield  county   bands  time_idx  bin0  bin1  bin2  \\\n",
       "0            0   2003  0.61295       0  band_0         0   0.0   0.0   0.0   \n",
       "1            1   2003  0.61295       0  band_1         0   0.0   0.0   0.0   \n",
       "2            2   2003  0.61295       0  band_2         0   0.0   0.0   0.0   \n",
       "3            3   2003  0.61295       0  band_3         0   0.0   0.0   0.0   \n",
       "4            4   2003  0.61295       0  band_4         0   0.0   0.0   0.0   \n",
       "5            5   2003  0.61295       0  band_5         0   0.0   0.0   0.0   \n",
       "6            6   2003  0.61295       0  band_6         0   0.0   0.0   0.0   \n",
       "7            7   2003  0.61295       0  band_7         0   0.0   0.0   0.0   \n",
       "8            8   2003  0.61295       0  band_8         0   0.0   0.0   0.0   \n",
       "9            9   2003  0.61295       0  band_0         1   0.0   0.0   0.0   \n",
       "10          10   2003  0.61295       0  band_1         1   0.0   0.0   0.0   \n",
       "11          11   2003  0.61295       0  band_2         1   0.0   0.0   0.0   \n",
       "12          12   2003  0.61295       0  band_3         1   0.0   0.0   0.0   \n",
       "13          13   2003  0.61295       0  band_4         1   0.0   0.0   0.0   \n",
       "14          14   2003  0.61295       0  band_5         1   0.0   0.0   0.0   \n",
       "\n",
       "    bin3  ...    bin502    bin503    bin504    bin505    bin506    bin507  \\\n",
       "0    0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1    0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2    0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3    0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4    0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5    0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6    0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7    0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8    0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9    0.0  ...  0.001077  0.000833  0.001126  0.000098  0.000784  0.000881   \n",
       "10   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "12   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "13   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "14   0.0  ...  0.000097  0.000484  0.000242  0.000339  0.000048  0.000000   \n",
       "\n",
       "      bin508    bin509    bin510    bin511  \n",
       "0   0.000000  0.000000  0.000000  0.000000  \n",
       "1   0.000000  0.000000  0.000000  0.000000  \n",
       "2   0.000000  0.000000  0.000000  0.000000  \n",
       "3   0.000000  0.000000  0.000000  0.000000  \n",
       "4   0.000000  0.000000  0.000000  0.000000  \n",
       "5   0.000000  0.000000  0.000000  0.000000  \n",
       "6   0.000000  0.000000  0.000000  0.000000  \n",
       "7   0.000000  0.000000  0.000000  0.000000  \n",
       "8   0.000000  0.000000  0.000000  0.000000  \n",
       "9   0.000979  0.000441  0.000294  0.000196  \n",
       "10  0.000000  0.000000  0.000000  0.000000  \n",
       "11  0.000000  0.000000  0.000000  0.000000  \n",
       "12  0.000000  0.000000  0.000000  0.000000  \n",
       "13  0.000000  0.000000  0.000000  0.000000  \n",
       "14  0.000145  0.000194  0.000000  0.000000  \n",
       "\n",
       "[15 rows x 518 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('corn_china_pandas.csv')  # encoding= 'unicode_escape')\n",
    "\n",
    "data[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TOncsJYonfF_"
   },
   "outputs": [],
   "source": [
    "# years = [x for x in range(2003, 2019)]\n",
    "\n",
    "# data.rename(columns={'time_idx' : 'time'}, inplace=True)  \n",
    "\n",
    "# # data[5:15]  \n",
    "# data.insert(1, \"time_idx\", data['years'])  \n",
    "# # df = data.assign(time_dx = data.time * 10)\n",
    "\n",
    "# time_idx = 0\n",
    "# for year in years:\n",
    "#     data['time_idx'] = data['time_idx'].replace([year], time_idx)\n",
    "#     time_idx = time_idx + 1\n",
    "    \n",
    "# data['years'] = data['years'].astype(str)\n",
    "# data['county'] = data['county'].astype(str)\n",
    "# data['time'] = data['time'].astype(str)\n",
    "\n",
    "# dff = data[ data['years'] == '2018' ]\n",
    "# dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "F-uXD_srqwLW",
    "outputId": "8a8a0424-2e67-4dd2-be62-b6b0bf46bfc3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>years</th>\n",
       "      <th>yield</th>\n",
       "      <th>county</th>\n",
       "      <th>bands</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>bin0</th>\n",
       "      <th>bin1</th>\n",
       "      <th>bin2</th>\n",
       "      <th>bin3</th>\n",
       "      <th>...</th>\n",
       "      <th>bin502</th>\n",
       "      <th>bin503</th>\n",
       "      <th>bin504</th>\n",
       "      <th>bin505</th>\n",
       "      <th>bin506</th>\n",
       "      <th>bin507</th>\n",
       "      <th>bin508</th>\n",
       "      <th>bin509</th>\n",
       "      <th>bin510</th>\n",
       "      <th>bin511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 518 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 years    yield county   bands  time_idx  bin0  bin1  bin2  bin3  \\\n",
       "0           0  2003  0.61295      0  band_0         0   0.0   0.0   0.0   0.0   \n",
       "1           1  2003  0.61295      0  band_1         0   0.0   0.0   0.0   0.0   \n",
       "2           2  2003  0.61295      0  band_2         0   0.0   0.0   0.0   0.0   \n",
       "3           3  2003  0.61295      0  band_3         0   0.0   0.0   0.0   0.0   \n",
       "4           4  2003  0.61295      0  band_4         0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   ...  bin502  bin503  bin504  bin505  bin506  bin507  bin508  bin509  \\\n",
       "0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   bin510  bin511  \n",
       "0     0.0     0.0  \n",
       "1     0.0     0.0  \n",
       "2     0.0     0.0  \n",
       "3     0.0     0.0  \n",
       "4     0.0     0.0  \n",
       "\n",
       "[5 rows x 518 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['years'] = data['years'].astype(str)\n",
    "data['county'] = data['county'].astype(str)\n",
    "data['time_idx'] = data['time_idx'].astype(np.int64)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "LL4gooLRnkdv",
    "outputId": "cb1d012b-c336-4683-d2d9-f454d61c1cb1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>yield</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>bin0</th>\n",
       "      <th>bin1</th>\n",
       "      <th>bin2</th>\n",
       "      <th>bin3</th>\n",
       "      <th>bin4</th>\n",
       "      <th>bin5</th>\n",
       "      <th>bin6</th>\n",
       "      <th>...</th>\n",
       "      <th>bin502</th>\n",
       "      <th>bin503</th>\n",
       "      <th>bin504</th>\n",
       "      <th>bin505</th>\n",
       "      <th>bin506</th>\n",
       "      <th>bin507</th>\n",
       "      <th>bin508</th>\n",
       "      <th>bin509</th>\n",
       "      <th>bin510</th>\n",
       "      <th>bin511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4175.500000</td>\n",
       "      <td>0.517953</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2411.027829</td>\n",
       "      <td>0.106652</td>\n",
       "      <td>9.233143</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.003375</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2087.750000</td>\n",
       "      <td>0.438383</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4175.500000</td>\n",
       "      <td>0.518895</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6263.250000</td>\n",
       "      <td>0.574336</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8351.000000</td>\n",
       "      <td>0.855448</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.025479</td>\n",
       "      <td>0.027569</td>\n",
       "      <td>0.024157</td>\n",
       "      <td>0.028934</td>\n",
       "      <td>0.022859</td>\n",
       "      <td>0.028043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026096</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.024242</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.021858</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.032663</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 515 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0         yield      time_idx          bin0          bin1  \\\n",
       "count  91872.000000  91872.000000  91872.000000  91872.000000  91872.000000   \n",
       "mean    4175.500000      0.517953     15.500000      0.000733      0.000152   \n",
       "std     2411.027829      0.106652      9.233143      0.003606      0.000685   \n",
       "min        0.000000      0.276526      0.000000      0.000000      0.000000   \n",
       "25%     2087.750000      0.438383      7.750000      0.000000      0.000000   \n",
       "50%     4175.500000      0.518895     15.500000      0.000000      0.000000   \n",
       "75%     6263.250000      0.574336     23.250000      0.000059      0.000020   \n",
       "max     8351.000000      0.855448     31.000000      0.111111      0.025479   \n",
       "\n",
       "               bin2          bin3          bin4          bin5          bin6  \\\n",
       "count  91872.000000  91872.000000  91872.000000  91872.000000  91872.000000   \n",
       "mean       0.000182      0.000171      0.000206      0.000194      0.000231   \n",
       "std        0.000838      0.000762      0.000930      0.000845      0.001023   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000023      0.000023      0.000026      0.000026      0.000028   \n",
       "max        0.027569      0.024157      0.028934      0.022859      0.028043   \n",
       "\n",
       "       ...        bin502        bin503        bin504        bin505  \\\n",
       "count  ...  91872.000000  91872.000000  91872.000000  91872.000000   \n",
       "mean   ...      0.000247      0.000256      0.000257      0.000240   \n",
       "std    ...      0.000711      0.000745      0.000783      0.000701   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      0.000148      0.000156      0.000151      0.000142   \n",
       "max    ...      0.026096      0.040541      0.054054      0.024242   \n",
       "\n",
       "             bin506        bin507        bin508        bin509        bin510  \\\n",
       "count  91872.000000  91872.000000  91872.000000  91872.000000  91872.000000   \n",
       "mean       0.000252      0.000254      0.000228      0.000244      0.000231   \n",
       "std        0.003375      0.001853      0.000673      0.000899      0.000685   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000141      0.000144      0.000127      0.000135      0.000129   \n",
       "max        1.000000      0.500000      0.021858      0.157895      0.032663   \n",
       "\n",
       "             bin511  \n",
       "count  91872.000000  \n",
       "mean       0.000254  \n",
       "std        0.000792  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000141  \n",
       "max        0.066667  \n",
       "\n",
       "[8 rows x 515 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class FullyConnectedModule(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_hidden_layers: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # input layer\n",
    "        module_list = [nn.Linear(input_size, hidden_size), nn.ReLU()]\n",
    "        # hidden layers\n",
    "        for _ in range(n_hidden_layers):\n",
    "            module_list.extend([nn.Linear(hidden_size, hidden_size), nn.ReLU()])\n",
    "        # output layer\n",
    "        module_list.append(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "        self.sequential = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x of shape: batch_size x n_timesteps_in\n",
    "        # output of shape batch_size x n_timesteps_out\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "# test that network works as intended\n",
    "network = FullyConnectedModule(input_size=5, output_size=2, hidden_size=10, n_hidden_layers=2)\n",
    "x = torch.rand(20, 5)\n",
    "network(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "from pytorch_forecasting.models.nn import MultiEmbedding\n",
    "from pytorch_forecasting.models import BaseModelWithCovariates\n",
    "\n",
    "class FullyConnectedModelWithCovariates(BaseModelWithCovariates):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        hidden_size: int,\n",
    "        n_hidden_layers: int,\n",
    "        x_reals: List[str],\n",
    "        x_categoricals: List[str],\n",
    "        embedding_sizes: Dict[str, Tuple[int, int]],\n",
    "        embedding_labels: Dict[str, List[str]],\n",
    "        static_categoricals: List[str],\n",
    "        static_reals: List[str],\n",
    "        time_varying_categoricals_encoder: List[str],\n",
    "        time_varying_categoricals_decoder: List[str],\n",
    "        time_varying_reals_encoder: List[str],\n",
    "        time_varying_reals_decoder: List[str],\n",
    "        embedding_paddings: List[str],\n",
    "        categorical_groups: Dict[str, List[str]],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n",
    "        self.save_hyperparameters()\n",
    "        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # create embedder - can be fed with x[\"encoder_cat\"] or x[\"decoder_cat\"] and will return\n",
    "        # dictionary of category names mapped to embeddings\n",
    "        self.input_embeddings = MultiEmbedding(\n",
    "            embedding_sizes=self.hparams.embedding_sizes,\n",
    "            categorical_groups=self.hparams.categorical_groups,\n",
    "            embedding_paddings=self.hparams.embedding_paddings,\n",
    "            x_categoricals=self.hparams.x_categoricals,\n",
    "            max_embedding_size=self.hparams.hidden_size,\n",
    "        )\n",
    "        \n",
    "        # calculate the size of all concatenated embeddings + continous variables\n",
    "        n_features = sum(\n",
    "            embedding_size for classes_size, embedding_size in self.hparams.embedding_sizes.values()\n",
    "        ) + len(self.reals)\n",
    "\n",
    "        # create network that will be fed with continious variables and embeddings\n",
    "        self.network = FullyConnectedModule(\n",
    "            input_size=self.hparams.input_size * n_features,\n",
    "            output_size=self.hparams.output_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            n_hidden_layers=self.hparams.n_hidden_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # x is a batch generated based on the TimeSeriesDataset\n",
    "        batch_size = x[\"encoder_lengths\"].size(0)\n",
    "        embeddings = self.input_embeddings(x[\"encoder_cat\"])  # returns dictionary with embedding tensors\n",
    "        network_input = torch.cat(\n",
    "            [x[\"encoder_cont\"]]\n",
    "            + [\n",
    "                emb\n",
    "                for name, emb in embeddings.items()\n",
    "                if name in self.encoder_variables or name in self.static_variables\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        prediction = self.network(network_input.view(batch_size, -1))\n",
    "\n",
    "        # rescale predictions into target space\n",
    "        prediction = self.transform_output(prediction, target_scale=x[\"target_scale\"])\n",
    "\n",
    "        # We need to return a dictionary that at least contains the prediction.\n",
    "        # The parameter can be directly forwarded from the input.\n",
    "        # The conversion to a named tuple can be directly achieved with the `to_network_output` function.\n",
    "        return self.to_network_output(prediction=prediction)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataset(cls, dataset: TimeSeriesDataSet, **kwargs):\n",
    "        new_kwargs = {\n",
    "            \"output_size\": dataset.max_prediction_length,\n",
    "            \"input_size\": dataset.max_encoder_length,\n",
    "        }\n",
    "        new_kwargs.update(kwargs)  # use to pass real hyperparameters and override defaults set by dataset\n",
    "        # example for dataset validation\n",
    "        assert dataset.max_prediction_length == dataset.min_prediction_length, \"Decoder only supports a fixed length\"\n",
    "        assert dataset.min_encoder_length == dataset.max_encoder_length, \"Encoder only supports a fixed length\"\n",
    "\n",
    "        return super().from_dataset(dataset, **new_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"categorical_groups\":                {}\n",
       "\"embedding_labels\":                  {'years': {'2003': 0, '2004': 1, '2005': 2, '2006': 3, '2007': 4, '2008': 5, '2009': 6, '2010': 7, '2011': 8, '2012': 9, '2013': 10, '2014': 11, '2015': 12, '2016': 13, '2017': 14, '2018': 15}, 'county': {'0': 0, '1': 1, '11': 2, '12': 3, '13': 4, '14': 5, '15': 6, '16': 7, '17': 8, '19': 9, '2': 10, '21': 11, '23': 12, '24': 13, '26': 14, '29': 15, '3': 16, '5': 17, '8': 18, '9': 19}}\n",
       "\"embedding_paddings\":                []\n",
       "\"embedding_sizes\":                   {'years': (16, 8), 'county': (20, 9)}\n",
       "\"hidden_size\":                       10\n",
       "\"input_size\":                        5\n",
       "\"learning_rate\":                     0.001\n",
       "\"log_gradient_flow\":                 False\n",
       "\"log_interval\":                      -1\n",
       "\"log_val_interval\":                  -1\n",
       "\"logging_metrics\":                   ModuleList()\n",
       "\"loss\":                              SMAPE()\n",
       "\"monotone_constaints\":               {}\n",
       "\"n_hidden_layers\":                   2\n",
       "\"optimizer\":                         ranger\n",
       "\"optimizer_params\":                  None\n",
       "\"output_size\":                       1\n",
       "\"output_transformer\":                GroupNormalizer(\n",
       "\tmethod='standard',\n",
       "\tgroups=[],\n",
       "\tcenter=True,\n",
       "\tscale_by_group=False,\n",
       "\ttransformation='relu',\n",
       "\tmethod_kwargs={}\n",
       ")\n",
       "\"reduce_on_plateau_min_lr\":          1e-05\n",
       "\"reduce_on_plateau_patience\":        1000\n",
       "\"reduce_on_plateau_reduction\":       2.0\n",
       "\"static_categoricals\":               ['years', 'county']\n",
       "\"static_reals\":                      []\n",
       "\"time_varying_categoricals_decoder\": ['county']\n",
       "\"time_varying_categoricals_encoder\": ['county']\n",
       "\"time_varying_reals_decoder\":        []\n",
       "\"time_varying_reals_encoder\":        ['yield']\n",
       "\"weight_decay\":                      0.0\n",
       "\"x_categoricals\":                    ['years', 'county', 'county']\n",
       "\"x_reals\":                           ['yield']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.utilities.model_summary import summarize\n",
    "# create the dataset from the pandas dataframe\n",
    "dataset_with_covariates = TimeSeriesDataSet(\n",
    "    data,\n",
    "    group_ids=[\"years\", \"county\", \"bands\"],\n",
    "    target=\"yield\",\n",
    "    time_idx=\"time_idx\",\n",
    "    min_encoder_length=5,\n",
    "    max_encoder_length=5,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=1,\n",
    "    time_varying_unknown_reals=[\"yield\"],\n",
    "    # time_varying_known_reals=[\"real_covariate\"],\n",
    "    time_varying_known_categoricals=[\"county\"],\n",
    "    static_categoricals=[\"years\", \"county\"],\n",
    ")\n",
    "\n",
    "model = FullyConnectedModelWithCovariates.from_dataset(dataset_with_covariates, hidden_size=10, n_hidden_layers=2)\n",
    "summarize(model,max_depth=-1)  # print model summary\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullyConnectedModelWithCovariates.from_dataset(dataset_with_covariates, hidden_size=10, n_hidden_layers=2)\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 128\n",
    "dataloader = dataset_with_covariates.to_dataloader(train=True, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name             | Type                 | Params\n",
      "----------------------------------------------------------\n",
      "0 | loss             | SMAPE                | 0     \n",
      "1 | logging_metrics  | ModuleList           | 0     \n",
      "2 | input_embeddings | MultiEmbedding       | 308   \n",
      "3 | network          | FullyConnectedModule | 1.1 K \n",
      "----------------------------------------------------------\n",
      "1.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 605/1210 [00:27<00:27, 22.24it/s, loss=0.0259, v_num=1, train_loss_step=0.027] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/605 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/605 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  50%|█████     | 606/1210 [00:27<00:27, 22.09it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  50%|█████     | 607/1210 [00:27<00:27, 22.10it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  50%|█████     | 608/1210 [00:27<00:27, 22.10it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  50%|█████     | 609/1210 [00:27<00:27, 22.13it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  50%|█████     | 610/1210 [00:27<00:27, 22.12it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  50%|█████     | 611/1210 [00:27<00:27, 22.14it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  51%|█████     | 612/1210 [00:27<00:27, 22.14it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  51%|█████     | 613/1210 [00:27<00:26, 22.15it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  51%|█████     | 614/1210 [00:27<00:26, 22.15it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  51%|█████     | 615/1210 [00:27<00:26, 22.16it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  51%|█████     | 616/1210 [00:27<00:26, 22.16it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  51%|█████     | 617/1210 [00:27<00:26, 22.18it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  51%|█████     | 618/1210 [00:27<00:26, 22.15it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  51%|█████     | 619/1210 [00:27<00:26, 22.17it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  51%|█████     | 620/1210 [00:27<00:26, 22.16it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  51%|█████▏    | 621/1210 [00:28<00:26, 22.17it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  51%|█████▏    | 622/1210 [00:28<00:26, 22.17it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  51%|█████▏    | 623/1210 [00:28<00:26, 22.18it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  52%|█████▏    | 624/1210 [00:28<00:26, 22.17it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  52%|█████▏    | 625/1210 [00:28<00:26, 22.18it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  52%|█████▏    | 626/1210 [00:28<00:26, 22.19it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  52%|█████▏    | 627/1210 [00:28<00:26, 22.20it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  52%|█████▏    | 628/1210 [00:28<00:26, 22.20it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  52%|█████▏    | 629/1210 [00:28<00:26, 22.21it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  52%|█████▏    | 630/1210 [00:28<00:26, 22.21it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  52%|█████▏    | 631/1210 [00:28<00:26, 22.23it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  52%|█████▏    | 632/1210 [00:28<00:26, 22.21it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  52%|█████▏    | 633/1210 [00:28<00:25, 22.23it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  52%|█████▏    | 634/1210 [00:28<00:25, 22.24it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  52%|█████▏    | 635/1210 [00:28<00:25, 22.25it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  53%|█████▎    | 636/1210 [00:28<00:25, 22.25it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  53%|█████▎    | 637/1210 [00:28<00:25, 22.27it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  53%|█████▎    | 638/1210 [00:28<00:25, 22.26it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  53%|█████▎    | 639/1210 [00:28<00:25, 22.28it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  53%|█████▎    | 640/1210 [00:28<00:25, 22.26it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  53%|█████▎    | 641/1210 [00:28<00:25, 22.27it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  53%|█████▎    | 642/1210 [00:28<00:25, 22.26it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  53%|█████▎    | 643/1210 [00:28<00:25, 22.28it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  53%|█████▎    | 644/1210 [00:28<00:25, 22.26it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  53%|█████▎    | 645/1210 [00:28<00:25, 22.27it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  53%|█████▎    | 646/1210 [00:29<00:25, 22.27it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  53%|█████▎    | 647/1210 [00:29<00:25, 22.30it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  54%|█████▎    | 648/1210 [00:29<00:25, 22.28it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  54%|█████▎    | 649/1210 [00:29<00:25, 22.30it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  54%|█████▎    | 650/1210 [00:29<00:25, 22.29it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  54%|█████▍    | 651/1210 [00:29<00:25, 22.32it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  54%|█████▍    | 652/1210 [00:29<00:25, 22.29it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  54%|█████▍    | 653/1210 [00:29<00:24, 22.30it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  54%|█████▍    | 654/1210 [00:29<00:24, 22.29it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  54%|█████▍    | 655/1210 [00:29<00:24, 22.32it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  54%|█████▍    | 656/1210 [00:29<00:24, 22.32it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  54%|█████▍    | 657/1210 [00:29<00:24, 22.35it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  54%|█████▍    | 658/1210 [00:29<00:24, 22.32it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  54%|█████▍    | 659/1210 [00:29<00:24, 22.35it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  55%|█████▍    | 660/1210 [00:29<00:24, 22.34it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  55%|█████▍    | 661/1210 [00:29<00:24, 22.37it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  55%|█████▍    | 662/1210 [00:29<00:24, 22.35it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  55%|█████▍    | 663/1210 [00:29<00:24, 22.37it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  55%|█████▍    | 664/1210 [00:29<00:24, 22.37it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  55%|█████▍    | 665/1210 [00:29<00:24, 22.39it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  55%|█████▌    | 666/1210 [00:29<00:24, 22.38it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  55%|█████▌    | 667/1210 [00:29<00:24, 22.39it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  55%|█████▌    | 668/1210 [00:29<00:24, 22.38it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  55%|█████▌    | 669/1210 [00:29<00:24, 22.40it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  55%|█████▌    | 670/1210 [00:29<00:24, 22.36it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  55%|█████▌    | 671/1210 [00:29<00:24, 22.39it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  56%|█████▌    | 672/1210 [00:29<00:24, 22.40it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  56%|█████▌    | 673/1210 [00:30<00:23, 22.41it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  56%|█████▌    | 674/1210 [00:30<00:23, 22.41it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  56%|█████▌    | 675/1210 [00:30<00:23, 22.43it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  56%|█████▌    | 676/1210 [00:30<00:23, 22.41it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  56%|█████▌    | 677/1210 [00:30<00:23, 22.44it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  56%|█████▌    | 678/1210 [00:30<00:23, 22.44it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  56%|█████▌    | 679/1210 [00:30<00:23, 22.44it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  56%|█████▌    | 680/1210 [00:30<00:23, 22.45it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  56%|█████▋    | 681/1210 [00:30<00:23, 22.46it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  56%|█████▋    | 682/1210 [00:30<00:23, 22.45it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  56%|█████▋    | 683/1210 [00:30<00:23, 22.47it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  57%|█████▋    | 684/1210 [00:30<00:23, 22.44it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  57%|█████▋    | 685/1210 [00:30<00:23, 22.46it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  57%|█████▋    | 686/1210 [00:30<00:23, 22.48it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  57%|█████▋    | 687/1210 [00:30<00:23, 22.49it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  57%|█████▋    | 688/1210 [00:30<00:23, 22.46it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  57%|█████▋    | 689/1210 [00:30<00:23, 22.49it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  57%|█████▋    | 690/1210 [00:30<00:23, 22.47it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  57%|█████▋    | 691/1210 [00:30<00:23, 22.50it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  57%|█████▋    | 692/1210 [00:30<00:23, 22.48it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  57%|█████▋    | 693/1210 [00:30<00:22, 22.51it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  57%|█████▋    | 694/1210 [00:30<00:22, 22.50it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  57%|█████▋    | 695/1210 [00:30<00:22, 22.53it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  58%|█████▊    | 696/1210 [00:30<00:22, 22.50it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  58%|█████▊    | 697/1210 [00:30<00:22, 22.51it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  58%|█████▊    | 698/1210 [00:31<00:22, 22.50it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  58%|█████▊    | 699/1210 [00:31<00:22, 22.52it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  58%|█████▊    | 700/1210 [00:31<00:22, 22.49it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  58%|█████▊    | 701/1210 [00:31<00:22, 22.51it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  58%|█████▊    | 702/1210 [00:31<00:22, 22.52it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  58%|█████▊    | 703/1210 [00:31<00:22, 22.54it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  58%|█████▊    | 704/1210 [00:31<00:22, 22.53it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  58%|█████▊    | 705/1210 [00:31<00:22, 22.56it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  58%|█████▊    | 706/1210 [00:31<00:22, 22.55it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  58%|█████▊    | 707/1210 [00:31<00:22, 22.57it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  59%|█████▊    | 708/1210 [00:31<00:22, 22.55it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  59%|█████▊    | 709/1210 [00:31<00:22, 22.58it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  59%|█████▊    | 710/1210 [00:31<00:22, 22.56it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  59%|█████▉    | 711/1210 [00:31<00:22, 22.58it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  59%|█████▉    | 712/1210 [00:31<00:22, 22.57it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  59%|█████▉    | 713/1210 [00:31<00:22, 22.59it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  59%|█████▉    | 714/1210 [00:31<00:21, 22.58it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  59%|█████▉    | 715/1210 [00:31<00:21, 22.60it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  59%|█████▉    | 716/1210 [00:31<00:21, 22.59it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  59%|█████▉    | 717/1210 [00:31<00:21, 22.61it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  59%|█████▉    | 718/1210 [00:31<00:21, 22.60it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  59%|█████▉    | 719/1210 [00:31<00:21, 22.62it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  60%|█████▉    | 720/1210 [00:31<00:21, 22.58it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  60%|█████▉    | 721/1210 [00:31<00:21, 22.61it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  60%|█████▉    | 722/1210 [00:31<00:21, 22.58it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  60%|█████▉    | 723/1210 [00:31<00:21, 22.60it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  60%|█████▉    | 724/1210 [00:32<00:21, 22.59it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  60%|█████▉    | 725/1210 [00:32<00:21, 22.61it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  60%|██████    | 726/1210 [00:32<00:21, 22.59it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  60%|██████    | 727/1210 [00:32<00:21, 22.62it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  60%|██████    | 728/1210 [00:32<00:21, 22.60it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  60%|██████    | 729/1210 [00:32<00:21, 22.62it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  60%|██████    | 730/1210 [00:32<00:21, 22.60it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  60%|██████    | 731/1210 [00:32<00:21, 22.62it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  60%|██████    | 732/1210 [00:32<00:21, 22.61it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  61%|██████    | 733/1210 [00:32<00:21, 22.63it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  61%|██████    | 734/1210 [00:32<00:21, 22.62it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  61%|██████    | 735/1210 [00:32<00:20, 22.64it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  61%|██████    | 736/1210 [00:32<00:20, 22.62it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  61%|██████    | 737/1210 [00:32<00:20, 22.64it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  61%|██████    | 738/1210 [00:32<00:20, 22.62it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  61%|██████    | 739/1210 [00:32<00:20, 22.64it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  61%|██████    | 740/1210 [00:32<00:20, 22.62it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  61%|██████    | 741/1210 [00:32<00:20, 22.65it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  61%|██████▏   | 742/1210 [00:32<00:20, 22.63it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  61%|██████▏   | 743/1210 [00:32<00:20, 22.65it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  61%|██████▏   | 744/1210 [00:32<00:20, 22.63it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  62%|██████▏   | 745/1210 [00:32<00:20, 22.66it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  62%|██████▏   | 746/1210 [00:32<00:20, 22.63it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  62%|██████▏   | 747/1210 [00:32<00:20, 22.65it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  62%|██████▏   | 748/1210 [00:33<00:20, 22.64it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  62%|██████▏   | 749/1210 [00:33<00:20, 22.66it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  62%|██████▏   | 750/1210 [00:33<00:20, 22.65it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  62%|██████▏   | 751/1210 [00:33<00:20, 22.68it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  62%|██████▏   | 752/1210 [00:33<00:20, 22.65it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  62%|██████▏   | 753/1210 [00:33<00:20, 22.68it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  62%|██████▏   | 754/1210 [00:33<00:20, 22.66it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  62%|██████▏   | 755/1210 [00:33<00:20, 22.69it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  62%|██████▏   | 756/1210 [00:33<00:20, 22.67it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  63%|██████▎   | 757/1210 [00:33<00:19, 22.69it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  63%|██████▎   | 758/1210 [00:33<00:19, 22.66it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  63%|██████▎   | 759/1210 [00:33<00:19, 22.68it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  63%|██████▎   | 760/1210 [00:33<00:19, 22.66it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  63%|██████▎   | 761/1210 [00:33<00:19, 22.69it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  63%|██████▎   | 762/1210 [00:33<00:19, 22.63it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  63%|██████▎   | 763/1210 [00:33<00:19, 22.64it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  63%|██████▎   | 764/1210 [00:33<00:19, 22.66it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  63%|██████▎   | 765/1210 [00:33<00:19, 22.68it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  63%|██████▎   | 766/1210 [00:33<00:19, 22.65it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  63%|██████▎   | 767/1210 [00:33<00:19, 22.67it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  63%|██████▎   | 768/1210 [00:33<00:19, 22.64it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  64%|██████▎   | 769/1210 [00:33<00:19, 22.66it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  64%|██████▎   | 770/1210 [00:34<00:19, 22.64it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  64%|██████▎   | 771/1210 [00:34<00:19, 22.66it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  64%|██████▍   | 772/1210 [00:34<00:19, 22.63it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  64%|██████▍   | 773/1210 [00:34<00:19, 22.66it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  64%|██████▍   | 774/1210 [00:34<00:19, 22.63it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  64%|██████▍   | 775/1210 [00:34<00:19, 22.65it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  64%|██████▍   | 776/1210 [00:34<00:19, 22.62it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  64%|██████▍   | 777/1210 [00:34<00:19, 22.64it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  64%|██████▍   | 778/1210 [00:34<00:19, 22.63it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  64%|██████▍   | 779/1210 [00:34<00:19, 22.65it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  64%|██████▍   | 780/1210 [00:34<00:18, 22.64it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  65%|██████▍   | 781/1210 [00:34<00:18, 22.66it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  65%|██████▍   | 782/1210 [00:34<00:18, 22.64it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  65%|██████▍   | 783/1210 [00:34<00:18, 22.67it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  65%|██████▍   | 784/1210 [00:34<00:18, 22.66it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  65%|██████▍   | 785/1210 [00:34<00:18, 22.68it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  65%|██████▍   | 786/1210 [00:34<00:18, 22.56it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  65%|██████▌   | 787/1210 [00:34<00:18, 22.58it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  65%|██████▌   | 788/1210 [00:34<00:18, 22.58it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  65%|██████▌   | 789/1210 [00:34<00:18, 22.60it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  65%|██████▌   | 790/1210 [00:34<00:18, 22.59it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  65%|██████▌   | 791/1210 [00:34<00:18, 22.61it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  65%|██████▌   | 792/1210 [00:35<00:18, 22.60it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  66%|██████▌   | 793/1210 [00:35<00:18, 22.62it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  66%|██████▌   | 794/1210 [00:35<00:18, 22.60it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  66%|██████▌   | 795/1210 [00:35<00:18, 22.62it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  66%|██████▌   | 796/1210 [00:35<00:18, 22.60it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  66%|██████▌   | 797/1210 [00:35<00:18, 22.62it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  66%|██████▌   | 798/1210 [00:35<00:18, 22.61it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  66%|██████▌   | 799/1210 [00:35<00:18, 22.63it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  66%|██████▌   | 800/1210 [00:35<00:18, 22.62it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  66%|██████▌   | 801/1210 [00:35<00:18, 22.64it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  66%|██████▋   | 802/1210 [00:35<00:18, 22.63it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  66%|██████▋   | 803/1210 [00:35<00:17, 22.66it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  66%|██████▋   | 804/1210 [00:35<00:17, 22.65it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  67%|██████▋   | 805/1210 [00:35<00:17, 22.67it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  67%|██████▋   | 806/1210 [00:35<00:17, 22.67it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  67%|██████▋   | 807/1210 [00:35<00:17, 22.69it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  67%|██████▋   | 808/1210 [00:35<00:17, 22.68it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  67%|██████▋   | 809/1210 [00:35<00:17, 22.70it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  67%|██████▋   | 810/1210 [00:35<00:17, 22.68it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  67%|██████▋   | 811/1210 [00:35<00:17, 22.70it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  67%|██████▋   | 812/1210 [00:35<00:17, 22.69it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  67%|██████▋   | 813/1210 [00:35<00:17, 22.71it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  67%|██████▋   | 814/1210 [00:35<00:17, 22.70it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  67%|██████▋   | 815/1210 [00:35<00:17, 22.72it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  67%|██████▋   | 816/1210 [00:35<00:17, 22.69it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  68%|██████▊   | 817/1210 [00:35<00:17, 22.72it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  68%|██████▊   | 818/1210 [00:36<00:17, 22.70it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  68%|██████▊   | 819/1210 [00:36<00:17, 22.72it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  68%|██████▊   | 820/1210 [00:36<00:17, 22.70it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  68%|██████▊   | 821/1210 [00:36<00:17, 22.72it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  68%|██████▊   | 822/1210 [00:36<00:17, 22.71it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  68%|██████▊   | 823/1210 [00:36<00:17, 22.73it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  68%|██████▊   | 824/1210 [00:36<00:16, 22.72it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  68%|██████▊   | 825/1210 [00:36<00:16, 22.74it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  68%|██████▊   | 826/1210 [00:36<00:16, 22.73it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  68%|██████▊   | 827/1210 [00:36<00:16, 22.75it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  68%|██████▊   | 828/1210 [00:36<00:16, 22.74it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  69%|██████▊   | 829/1210 [00:36<00:16, 22.76it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  69%|██████▊   | 830/1210 [00:36<00:16, 22.74it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  69%|██████▊   | 831/1210 [00:36<00:16, 22.76it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  69%|██████▉   | 832/1210 [00:36<00:16, 22.75it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  69%|██████▉   | 833/1210 [00:36<00:16, 22.77it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  69%|██████▉   | 834/1210 [00:36<00:16, 22.76it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  69%|██████▉   | 835/1210 [00:36<00:16, 22.78it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  69%|██████▉   | 836/1210 [00:36<00:16, 22.76it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  69%|██████▉   | 837/1210 [00:36<00:16, 22.78it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  69%|██████▉   | 838/1210 [00:36<00:16, 22.76it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  69%|██████▉   | 839/1210 [00:36<00:16, 22.78it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  69%|██████▉   | 840/1210 [00:36<00:16, 22.77it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  70%|██████▉   | 841/1210 [00:36<00:16, 22.80it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  70%|██████▉   | 842/1210 [00:36<00:16, 22.78it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  70%|██████▉   | 843/1210 [00:36<00:16, 22.80it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  70%|██████▉   | 844/1210 [00:37<00:16, 22.79it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  70%|██████▉   | 845/1210 [00:37<00:16, 22.81it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  70%|██████▉   | 846/1210 [00:37<00:15, 22.80it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  70%|███████   | 847/1210 [00:37<00:15, 22.82it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  70%|███████   | 848/1210 [00:37<00:15, 22.81it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  70%|███████   | 849/1210 [00:37<00:15, 22.83it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  70%|███████   | 850/1210 [00:37<00:15, 22.82it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  70%|███████   | 851/1210 [00:37<00:15, 22.84it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  70%|███████   | 852/1210 [00:37<00:15, 22.83it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  70%|███████   | 853/1210 [00:37<00:15, 22.85it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  71%|███████   | 854/1210 [00:37<00:15, 22.83it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  71%|███████   | 855/1210 [00:37<00:15, 22.85it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  71%|███████   | 856/1210 [00:37<00:15, 22.83it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  71%|███████   | 857/1210 [00:37<00:15, 22.85it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  71%|███████   | 858/1210 [00:37<00:15, 22.84it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  71%|███████   | 859/1210 [00:37<00:15, 22.86it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  71%|███████   | 860/1210 [00:37<00:15, 22.84it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  71%|███████   | 861/1210 [00:37<00:15, 22.84it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  71%|███████   | 862/1210 [00:37<00:15, 22.83it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  71%|███████▏  | 863/1210 [00:37<00:15, 22.85it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  71%|███████▏  | 864/1210 [00:37<00:15, 22.83it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  71%|███████▏  | 865/1210 [00:37<00:15, 22.85it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  72%|███████▏  | 866/1210 [00:37<00:15, 22.84it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  72%|███████▏  | 867/1210 [00:37<00:15, 22.86it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  72%|███████▏  | 868/1210 [00:37<00:14, 22.85it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  72%|███████▏  | 869/1210 [00:37<00:14, 22.87it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  72%|███████▏  | 870/1210 [00:38<00:14, 22.86it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  72%|███████▏  | 871/1210 [00:38<00:14, 22.88it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  72%|███████▏  | 872/1210 [00:38<00:14, 22.86it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  72%|███████▏  | 873/1210 [00:38<00:14, 22.88it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  72%|███████▏  | 874/1210 [00:38<00:14, 22.87it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  72%|███████▏  | 875/1210 [00:38<00:14, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  72%|███████▏  | 876/1210 [00:38<00:14, 22.87it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  72%|███████▏  | 877/1210 [00:38<00:14, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  73%|███████▎  | 878/1210 [00:38<00:14, 22.88it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  73%|███████▎  | 879/1210 [00:38<00:14, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  73%|███████▎  | 880/1210 [00:38<00:14, 22.88it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  73%|███████▎  | 881/1210 [00:38<00:14, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  73%|███████▎  | 882/1210 [00:38<00:14, 22.87it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  73%|███████▎  | 883/1210 [00:38<00:14, 22.88it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  73%|███████▎  | 884/1210 [00:38<00:14, 22.86it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  73%|███████▎  | 885/1210 [00:38<00:14, 22.88it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  73%|███████▎  | 886/1210 [00:38<00:14, 22.87it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  73%|███████▎  | 887/1210 [00:38<00:14, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  73%|███████▎  | 888/1210 [00:38<00:14, 22.86it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  73%|███████▎  | 889/1210 [00:38<00:14, 22.88it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  74%|███████▎  | 890/1210 [00:38<00:13, 22.87it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  74%|███████▎  | 891/1210 [00:38<00:13, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  74%|███████▎  | 892/1210 [00:39<00:13, 22.87it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  74%|███████▍  | 893/1210 [00:39<00:13, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  74%|███████▍  | 894/1210 [00:39<00:13, 22.87it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  74%|███████▍  | 895/1210 [00:39<00:13, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  74%|███████▍  | 896/1210 [00:39<00:13, 22.88it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  74%|███████▍  | 897/1210 [00:39<00:13, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  74%|███████▍  | 898/1210 [00:39<00:13, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  74%|███████▍  | 899/1210 [00:39<00:13, 22.91it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  74%|███████▍  | 900/1210 [00:39<00:13, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  74%|███████▍  | 901/1210 [00:39<00:13, 22.91it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  75%|███████▍  | 902/1210 [00:39<00:13, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  75%|███████▍  | 903/1210 [00:39<00:13, 22.91it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  75%|███████▍  | 904/1210 [00:39<00:13, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  75%|███████▍  | 905/1210 [00:39<00:13, 22.91it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  75%|███████▍  | 906/1210 [00:39<00:13, 22.88it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  75%|███████▍  | 907/1210 [00:39<00:13, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  75%|███████▌  | 908/1210 [00:39<00:13, 22.87it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  75%|███████▌  | 909/1210 [00:39<00:13, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  75%|███████▌  | 910/1210 [00:39<00:13, 22.88it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  75%|███████▌  | 911/1210 [00:39<00:13, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  75%|███████▌  | 912/1210 [00:39<00:13, 22.88it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  75%|███████▌  | 913/1210 [00:39<00:12, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  76%|███████▌  | 914/1210 [00:39<00:12, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  76%|███████▌  | 915/1210 [00:39<00:12, 22.91it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  76%|███████▌  | 916/1210 [00:40<00:12, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  76%|███████▌  | 917/1210 [00:40<00:12, 22.91it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  76%|███████▌  | 918/1210 [00:40<00:12, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  76%|███████▌  | 919/1210 [00:40<00:12, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  76%|███████▌  | 920/1210 [00:40<00:12, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  76%|███████▌  | 921/1210 [00:40<00:12, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  76%|███████▌  | 922/1210 [00:40<00:12, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  76%|███████▋  | 923/1210 [00:40<00:12, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  76%|███████▋  | 924/1210 [00:40<00:12, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  76%|███████▋  | 925/1210 [00:40<00:12, 22.91it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  77%|███████▋  | 926/1210 [00:40<00:12, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  77%|███████▋  | 927/1210 [00:40<00:12, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  77%|███████▋  | 928/1210 [00:40<00:12, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  77%|███████▋  | 929/1210 [00:40<00:12, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  77%|███████▋  | 930/1210 [00:40<00:12, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  77%|███████▋  | 931/1210 [00:40<00:12, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  77%|███████▋  | 932/1210 [00:40<00:12, 22.91it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  77%|███████▋  | 933/1210 [00:40<00:12, 22.93it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  77%|███████▋  | 934/1210 [00:40<00:12, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  77%|███████▋  | 935/1210 [00:40<00:11, 22.94it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  77%|███████▋  | 936/1210 [00:40<00:11, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  77%|███████▋  | 937/1210 [00:40<00:11, 22.94it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  78%|███████▊  | 938/1210 [00:40<00:11, 22.93it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  78%|███████▊  | 939/1210 [00:40<00:11, 22.94it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  78%|███████▊  | 940/1210 [00:41<00:11, 22.93it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  78%|███████▊  | 941/1210 [00:41<00:11, 22.95it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  78%|███████▊  | 942/1210 [00:41<00:11, 22.93it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  78%|███████▊  | 943/1210 [00:41<00:11, 22.95it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  78%|███████▊  | 944/1210 [00:41<00:11, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  78%|███████▊  | 945/1210 [00:41<00:11, 22.94it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  78%|███████▊  | 946/1210 [00:41<00:11, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  78%|███████▊  | 947/1210 [00:41<00:11, 22.94it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  78%|███████▊  | 948/1210 [00:41<00:11, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  78%|███████▊  | 949/1210 [00:41<00:11, 22.94it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  79%|███████▊  | 950/1210 [00:41<00:11, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  79%|███████▊  | 951/1210 [00:41<00:11, 22.94it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  79%|███████▊  | 952/1210 [00:41<00:11, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  79%|███████▉  | 953/1210 [00:41<00:11, 22.94it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  79%|███████▉  | 954/1210 [00:41<00:11, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  79%|███████▉  | 955/1210 [00:41<00:11, 22.94it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  79%|███████▉  | 956/1210 [00:41<00:11, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  79%|███████▉  | 957/1210 [00:41<00:11, 22.94it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  79%|███████▉  | 958/1210 [00:41<00:10, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  79%|███████▉  | 959/1210 [00:41<00:10, 22.94it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  79%|███████▉  | 960/1210 [00:41<00:10, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  79%|███████▉  | 961/1210 [00:41<00:10, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  80%|███████▉  | 962/1210 [00:42<00:10, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  80%|███████▉  | 963/1210 [00:42<00:10, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  80%|███████▉  | 964/1210 [00:42<00:10, 22.91it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  80%|███████▉  | 965/1210 [00:42<00:10, 22.93it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  80%|███████▉  | 966/1210 [00:42<00:10, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  80%|███████▉  | 967/1210 [00:42<00:10, 22.93it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  80%|████████  | 968/1210 [00:42<00:10, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  80%|████████  | 969/1210 [00:42<00:10, 22.94it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  80%|████████  | 970/1210 [00:42<00:10, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  80%|████████  | 971/1210 [00:42<00:10, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  80%|████████  | 972/1210 [00:42<00:10, 22.91it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  80%|████████  | 973/1210 [00:42<00:10, 22.93it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  80%|████████  | 974/1210 [00:42<00:10, 22.91it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  81%|████████  | 975/1210 [00:42<00:10, 22.93it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  81%|████████  | 976/1210 [00:42<00:10, 22.91it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  81%|████████  | 977/1210 [00:42<00:10, 22.93it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  81%|████████  | 978/1210 [00:42<00:10, 22.91it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  81%|████████  | 979/1210 [00:42<00:10, 22.93it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  81%|████████  | 980/1210 [00:42<00:10, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  81%|████████  | 981/1210 [00:42<00:09, 22.91it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  81%|████████  | 982/1210 [00:42<00:09, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  81%|████████  | 983/1210 [00:42<00:09, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  81%|████████▏ | 984/1210 [00:42<00:09, 22.91it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  81%|████████▏ | 985/1210 [00:42<00:09, 22.93it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  81%|████████▏ | 986/1210 [00:43<00:09, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  82%|████████▏ | 987/1210 [00:43<00:09, 22.93it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  82%|████████▏ | 988/1210 [00:43<00:09, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  82%|████████▏ | 989/1210 [00:43<00:09, 22.94it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  82%|████████▏ | 990/1210 [00:43<00:09, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  82%|████████▏ | 991/1210 [00:43<00:09, 22.94it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  82%|████████▏ | 992/1210 [00:43<00:09, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  82%|████████▏ | 993/1210 [00:43<00:09, 22.93it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  82%|████████▏ | 994/1210 [00:43<00:09, 22.93it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  82%|████████▏ | 995/1210 [00:43<00:09, 22.94it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  82%|████████▏ | 996/1210 [00:43<00:09, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  82%|████████▏ | 997/1210 [00:43<00:09, 22.94it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  82%|████████▏ | 998/1210 [00:43<00:09, 22.91it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  83%|████████▎ | 999/1210 [00:43<00:09, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  83%|████████▎ | 1000/1210 [00:43<00:09, 22.91it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  83%|████████▎ | 1001/1210 [00:43<00:09, 22.93it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  83%|████████▎ | 1002/1210 [00:43<00:09, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  83%|████████▎ | 1003/1210 [00:43<00:09, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  83%|████████▎ | 1004/1210 [00:43<00:08, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  83%|████████▎ | 1005/1210 [00:43<00:08, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  83%|████████▎ | 1006/1210 [00:43<00:08, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  83%|████████▎ | 1007/1210 [00:43<00:08, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  83%|████████▎ | 1008/1210 [00:44<00:08, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  83%|████████▎ | 1009/1210 [00:44<00:08, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  83%|████████▎ | 1010/1210 [00:44<00:08, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  84%|████████▎ | 1011/1210 [00:44<00:08, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  84%|████████▎ | 1012/1210 [00:44<00:08, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  84%|████████▎ | 1013/1210 [00:44<00:08, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  84%|████████▍ | 1014/1210 [00:44<00:08, 22.87it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  84%|████████▍ | 1015/1210 [00:44<00:08, 22.88it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  84%|████████▍ | 1016/1210 [00:44<00:08, 22.87it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  84%|████████▍ | 1017/1210 [00:44<00:08, 22.87it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  84%|████████▍ | 1018/1210 [00:44<00:08, 22.87it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  84%|████████▍ | 1019/1210 [00:44<00:08, 22.88it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  84%|████████▍ | 1020/1210 [00:44<00:08, 22.87it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  84%|████████▍ | 1021/1210 [00:44<00:08, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  84%|████████▍ | 1022/1210 [00:44<00:08, 22.87it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  85%|████████▍ | 1023/1210 [00:44<00:08, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  85%|████████▍ | 1024/1210 [00:44<00:08, 22.87it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  85%|████████▍ | 1025/1210 [00:44<00:08, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  85%|████████▍ | 1026/1210 [00:44<00:08, 22.87it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  85%|████████▍ | 1027/1210 [00:44<00:07, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  85%|████████▍ | 1028/1210 [00:44<00:07, 22.88it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  85%|████████▌ | 1029/1210 [00:44<00:07, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  85%|████████▌ | 1030/1210 [00:45<00:07, 22.88it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  85%|████████▌ | 1031/1210 [00:45<00:07, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  85%|████████▌ | 1032/1210 [00:45<00:07, 22.88it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  85%|████████▌ | 1033/1210 [00:45<00:07, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  85%|████████▌ | 1034/1210 [00:45<00:07, 22.87it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  86%|████████▌ | 1035/1210 [00:45<00:07, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  86%|████████▌ | 1036/1210 [00:45<00:07, 22.86it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  86%|████████▌ | 1037/1210 [00:45<00:07, 22.88it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  86%|████████▌ | 1038/1210 [00:45<00:07, 22.88it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  86%|████████▌ | 1039/1210 [00:45<00:07, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  86%|████████▌ | 1040/1210 [00:45<00:07, 22.88it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  86%|████████▌ | 1041/1210 [00:45<00:07, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  86%|████████▌ | 1042/1210 [00:45<00:07, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  86%|████████▌ | 1043/1210 [00:45<00:07, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  86%|████████▋ | 1044/1210 [00:45<00:07, 22.89it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  86%|████████▋ | 1045/1210 [00:45<00:07, 22.91it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  86%|████████▋ | 1046/1210 [00:45<00:07, 22.90it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  87%|████████▋ | 1047/1210 [00:45<00:07, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  87%|████████▋ | 1048/1210 [00:45<00:07, 22.91it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  87%|████████▋ | 1049/1210 [00:45<00:07, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  87%|████████▋ | 1050/1210 [00:45<00:06, 22.91it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  87%|████████▋ | 1051/1210 [00:45<00:06, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  87%|████████▋ | 1052/1210 [00:45<00:06, 22.92it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  87%|████████▋ | 1053/1210 [00:45<00:06, 22.93it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  87%|████████▋ | 1054/1210 [00:45<00:06, 22.93it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  87%|████████▋ | 1055/1210 [00:45<00:06, 22.94it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  87%|████████▋ | 1056/1210 [00:46<00:06, 22.93it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  87%|████████▋ | 1057/1210 [00:46<00:06, 22.94it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  87%|████████▋ | 1058/1210 [00:46<00:06, 22.93it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  88%|████████▊ | 1059/1210 [00:46<00:06, 22.94it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  88%|████████▊ | 1060/1210 [00:46<00:06, 22.95it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  88%|████████▊ | 1061/1210 [00:46<00:06, 22.95it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  88%|████████▊ | 1062/1210 [00:46<00:06, 22.95it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  88%|████████▊ | 1063/1210 [00:46<00:06, 22.96it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  88%|████████▊ | 1064/1210 [00:46<00:06, 22.96it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  88%|████████▊ | 1065/1210 [00:46<00:06, 22.97it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  88%|████████▊ | 1066/1210 [00:46<00:06, 22.96it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  88%|████████▊ | 1067/1210 [00:46<00:06, 22.97it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  88%|████████▊ | 1068/1210 [00:46<00:06, 22.96it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  88%|████████▊ | 1069/1210 [00:46<00:06, 22.97it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  88%|████████▊ | 1070/1210 [00:46<00:06, 22.96it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  89%|████████▊ | 1071/1210 [00:46<00:06, 22.97it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  89%|████████▊ | 1072/1210 [00:46<00:06, 22.97it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  89%|████████▊ | 1073/1210 [00:46<00:05, 22.99it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  89%|████████▉ | 1074/1210 [00:46<00:05, 22.97it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  89%|████████▉ | 1075/1210 [00:46<00:05, 22.99it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  89%|████████▉ | 1076/1210 [00:46<00:05, 22.97it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  89%|████████▉ | 1077/1210 [00:46<00:05, 22.98it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  89%|████████▉ | 1078/1210 [00:46<00:05, 22.98it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  89%|████████▉ | 1079/1210 [00:46<00:05, 22.99it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  89%|████████▉ | 1080/1210 [00:46<00:05, 22.98it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  89%|████████▉ | 1081/1210 [00:47<00:05, 23.00it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  89%|████████▉ | 1082/1210 [00:47<00:05, 22.99it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  90%|████████▉ | 1083/1210 [00:47<00:05, 23.00it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  90%|████████▉ | 1084/1210 [00:47<00:05, 22.99it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  90%|████████▉ | 1085/1210 [00:47<00:05, 23.01it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  90%|████████▉ | 1086/1210 [00:47<00:05, 23.00it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  90%|████████▉ | 1087/1210 [00:47<00:05, 23.01it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  90%|████████▉ | 1088/1210 [00:47<00:05, 23.00it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  90%|█████████ | 1089/1210 [00:47<00:05, 23.02it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  90%|█████████ | 1090/1210 [00:47<00:05, 23.01it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  90%|█████████ | 1091/1210 [00:47<00:05, 23.02it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  90%|█████████ | 1092/1210 [00:47<00:05, 23.01it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  90%|█████████ | 1093/1210 [00:47<00:05, 23.03it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  90%|█████████ | 1094/1210 [00:47<00:05, 23.02it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  90%|█████████ | 1095/1210 [00:47<00:04, 23.04it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  91%|█████████ | 1096/1210 [00:47<00:04, 23.03it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  91%|█████████ | 1097/1210 [00:47<00:04, 23.04it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  91%|█████████ | 1098/1210 [00:47<00:04, 23.03it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  91%|█████████ | 1099/1210 [00:47<00:04, 23.04it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  91%|█████████ | 1100/1210 [00:47<00:04, 23.03it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  91%|█████████ | 1101/1210 [00:47<00:04, 23.05it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  91%|█████████ | 1102/1210 [00:47<00:04, 23.03it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  91%|█████████ | 1103/1210 [00:47<00:04, 23.05it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  91%|█████████ | 1104/1210 [00:47<00:04, 23.03it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  91%|█████████▏| 1105/1210 [00:47<00:04, 23.05it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  91%|█████████▏| 1106/1210 [00:48<00:04, 23.04it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  91%|█████████▏| 1107/1210 [00:48<00:04, 23.06it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  92%|█████████▏| 1108/1210 [00:48<00:04, 23.04it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  92%|█████████▏| 1109/1210 [00:48<00:04, 23.06it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  92%|█████████▏| 1110/1210 [00:48<00:04, 23.05it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  92%|█████████▏| 1111/1210 [00:48<00:04, 23.07it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  92%|█████████▏| 1112/1210 [00:48<00:04, 23.06it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  92%|█████████▏| 1113/1210 [00:48<00:04, 23.07it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  92%|█████████▏| 1114/1210 [00:48<00:04, 23.06it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  92%|█████████▏| 1115/1210 [00:48<00:04, 23.07it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  92%|█████████▏| 1116/1210 [00:48<00:04, 23.07it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  92%|█████████▏| 1117/1210 [00:48<00:04, 23.08it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  92%|█████████▏| 1118/1210 [00:48<00:03, 23.07it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  92%|█████████▏| 1119/1210 [00:48<00:03, 23.08it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  93%|█████████▎| 1120/1210 [00:48<00:03, 23.07it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  93%|█████████▎| 1121/1210 [00:48<00:03, 23.08it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  93%|█████████▎| 1122/1210 [00:48<00:03, 23.07it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  93%|█████████▎| 1123/1210 [00:48<00:03, 23.08it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  93%|█████████▎| 1124/1210 [00:48<00:03, 23.08it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  93%|█████████▎| 1125/1210 [00:48<00:03, 23.08it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  93%|█████████▎| 1126/1210 [00:48<00:03, 23.07it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  93%|█████████▎| 1127/1210 [00:48<00:03, 23.09it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  93%|█████████▎| 1128/1210 [00:48<00:03, 23.08it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  93%|█████████▎| 1129/1210 [00:48<00:03, 23.09it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  93%|█████████▎| 1130/1210 [00:48<00:03, 23.09it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  93%|█████████▎| 1131/1210 [00:48<00:03, 23.10it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  94%|█████████▎| 1132/1210 [00:49<00:03, 23.09it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  94%|█████████▎| 1133/1210 [00:49<00:03, 23.11it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  94%|█████████▎| 1134/1210 [00:49<00:03, 23.10it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  94%|█████████▍| 1135/1210 [00:49<00:03, 23.11it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  94%|█████████▍| 1136/1210 [00:49<00:03, 23.11it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  94%|█████████▍| 1137/1210 [00:49<00:03, 23.12it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  94%|█████████▍| 1138/1210 [00:49<00:03, 23.11it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  94%|█████████▍| 1139/1210 [00:49<00:03, 23.13it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  94%|█████████▍| 1140/1210 [00:49<00:03, 23.12it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  94%|█████████▍| 1141/1210 [00:49<00:02, 23.13it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  94%|█████████▍| 1142/1210 [00:49<00:02, 23.12it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  94%|█████████▍| 1143/1210 [00:49<00:02, 23.13it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  95%|█████████▍| 1144/1210 [00:49<00:02, 23.11it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  95%|█████████▍| 1145/1210 [00:49<00:02, 23.13it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  95%|█████████▍| 1146/1210 [00:49<00:02, 23.12it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  95%|█████████▍| 1147/1210 [00:49<00:02, 23.13it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  95%|█████████▍| 1148/1210 [00:49<00:02, 23.12it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  95%|█████████▍| 1149/1210 [00:49<00:02, 23.14it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  95%|█████████▌| 1150/1210 [00:49<00:02, 23.13it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  95%|█████████▌| 1151/1210 [00:49<00:02, 23.14it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  95%|█████████▌| 1152/1210 [00:49<00:02, 23.14it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  95%|█████████▌| 1153/1210 [00:49<00:02, 23.15it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  95%|█████████▌| 1154/1210 [00:49<00:02, 23.14it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  95%|█████████▌| 1155/1210 [00:49<00:02, 23.15it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  96%|█████████▌| 1156/1210 [00:49<00:02, 23.14it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  96%|█████████▌| 1157/1210 [00:49<00:02, 23.16it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  96%|█████████▌| 1158/1210 [00:50<00:02, 23.16it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  96%|█████████▌| 1159/1210 [00:50<00:02, 23.17it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  96%|█████████▌| 1160/1210 [00:50<00:02, 23.16it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  96%|█████████▌| 1161/1210 [00:50<00:02, 23.17it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  96%|█████████▌| 1162/1210 [00:50<00:02, 23.17it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  96%|█████████▌| 1163/1210 [00:50<00:02, 23.17it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  96%|█████████▌| 1164/1210 [00:50<00:01, 23.17it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  96%|█████████▋| 1165/1210 [00:50<00:01, 23.18it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  96%|█████████▋| 1166/1210 [00:50<00:01, 23.17it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  96%|█████████▋| 1167/1210 [00:50<00:01, 23.17it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  97%|█████████▋| 1168/1210 [00:50<00:01, 23.17it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  97%|█████████▋| 1169/1210 [00:50<00:01, 23.17it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  97%|█████████▋| 1170/1210 [00:50<00:01, 23.18it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  97%|█████████▋| 1171/1210 [00:50<00:01, 23.19it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  97%|█████████▋| 1172/1210 [00:50<00:01, 23.18it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  97%|█████████▋| 1173/1210 [00:50<00:01, 23.19it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  97%|█████████▋| 1174/1210 [00:50<00:01, 23.19it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  97%|█████████▋| 1175/1210 [00:50<00:01, 23.19it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  97%|█████████▋| 1176/1210 [00:50<00:01, 23.19it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  97%|█████████▋| 1177/1210 [00:50<00:01, 23.19it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  97%|█████████▋| 1178/1210 [00:50<00:01, 23.19it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  97%|█████████▋| 1179/1210 [00:50<00:01, 23.20it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  98%|█████████▊| 1180/1210 [00:50<00:01, 23.19it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  98%|█████████▊| 1181/1210 [00:50<00:01, 23.20it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  98%|█████████▊| 1182/1210 [00:50<00:01, 23.19it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  98%|█████████▊| 1183/1210 [00:51<00:01, 23.20it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  98%|█████████▊| 1184/1210 [00:51<00:01, 23.20it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  98%|█████████▊| 1185/1210 [00:51<00:01, 23.20it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  98%|█████████▊| 1186/1210 [00:51<00:01, 23.18it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  98%|█████████▊| 1187/1210 [00:51<00:00, 23.20it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  98%|█████████▊| 1188/1210 [00:51<00:00, 23.18it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  98%|█████████▊| 1189/1210 [00:51<00:00, 23.20it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  98%|█████████▊| 1190/1210 [00:51<00:00, 23.19it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  98%|█████████▊| 1191/1210 [00:51<00:00, 23.20it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  99%|█████████▊| 1192/1210 [00:51<00:00, 23.20it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  99%|█████████▊| 1193/1210 [00:51<00:00, 23.22it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  99%|█████████▊| 1194/1210 [00:51<00:00, 23.21it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  99%|█████████▉| 1195/1210 [00:51<00:00, 23.22it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  99%|█████████▉| 1196/1210 [00:51<00:00, 23.21it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  99%|█████████▉| 1197/1210 [00:51<00:00, 23.22it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  99%|█████████▉| 1198/1210 [00:51<00:00, 23.21it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  99%|█████████▉| 1199/1210 [00:51<00:00, 23.22it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  99%|█████████▉| 1200/1210 [00:51<00:00, 23.21it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  99%|█████████▉| 1201/1210 [00:51<00:00, 23.22it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  99%|█████████▉| 1202/1210 [00:51<00:00, 23.20it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0:  99%|█████████▉| 1203/1210 [00:51<00:00, 23.21it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0: 100%|█████████▉| 1204/1210 [00:51<00:00, 23.20it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0: 100%|█████████▉| 1205/1210 [00:51<00:00, 23.21it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0: 100%|█████████▉| 1206/1210 [00:52<00:00, 23.19it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0: 100%|█████████▉| 1207/1210 [00:52<00:00, 23.20it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0: 100%|█████████▉| 1208/1210 [00:52<00:00, 23.19it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0: 100%|█████████▉| 1209/1210 [00:52<00:00, 23.20it/s, loss=0.0259, v_num=1, train_loss_step=0.027]\n",
      "Epoch 0: 100%|██████████| 1210/1210 [00:52<00:00, 23.21it/s, loss=0.0259, v_num=1, train_loss_step=0.027, val_loss=0.0241]\n",
      "Epoch 1:  50%|█████     | 605/1210 [00:28<00:28, 21.53it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/605 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/605 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  50%|█████     | 606/1210 [00:28<00:28, 21.39it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  50%|█████     | 607/1210 [00:28<00:28, 21.41it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  50%|█████     | 608/1210 [00:28<00:28, 21.42it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  50%|█████     | 609/1210 [00:28<00:28, 21.41it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  50%|█████     | 610/1210 [00:28<00:28, 21.41it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  50%|█████     | 611/1210 [00:28<00:27, 21.41it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  51%|█████     | 612/1210 [00:28<00:27, 21.42it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  51%|█████     | 613/1210 [00:28<00:27, 21.43it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  51%|█████     | 614/1210 [00:28<00:27, 21.42it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  51%|█████     | 615/1210 [00:28<00:27, 21.43it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  51%|█████     | 616/1210 [00:28<00:27, 21.43it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  51%|█████     | 617/1210 [00:28<00:27, 21.44it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  51%|█████     | 618/1210 [00:28<00:27, 21.44it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  51%|█████     | 619/1210 [00:28<00:27, 21.45it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  51%|█████     | 620/1210 [00:28<00:27, 21.42it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  51%|█████▏    | 621/1210 [00:28<00:27, 21.45it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  51%|█████▏    | 622/1210 [00:29<00:27, 21.45it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  51%|█████▏    | 623/1210 [00:29<00:27, 21.47it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  52%|█████▏    | 624/1210 [00:29<00:27, 21.46it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  52%|█████▏    | 625/1210 [00:29<00:27, 21.48it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  52%|█████▏    | 626/1210 [00:29<00:27, 21.46it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  52%|█████▏    | 627/1210 [00:29<00:27, 21.48it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  52%|█████▏    | 628/1210 [00:29<00:27, 21.46it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  52%|█████▏    | 629/1210 [00:29<00:27, 21.49it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  52%|█████▏    | 630/1210 [00:29<00:27, 21.47it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  52%|█████▏    | 631/1210 [00:29<00:26, 21.49it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  52%|█████▏    | 632/1210 [00:29<00:26, 21.46it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  52%|█████▏    | 633/1210 [00:29<00:26, 21.49it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  52%|█████▏    | 634/1210 [00:29<00:26, 21.47it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  52%|█████▏    | 635/1210 [00:29<00:26, 21.49it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  53%|█████▎    | 636/1210 [00:29<00:26, 21.46it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  53%|█████▎    | 637/1210 [00:29<00:26, 21.49it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  53%|█████▎    | 638/1210 [00:29<00:26, 21.47it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  53%|█████▎    | 639/1210 [00:29<00:26, 21.50it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  53%|█████▎    | 640/1210 [00:29<00:26, 21.49it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  53%|█████▎    | 641/1210 [00:29<00:26, 21.51it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  53%|█████▎    | 642/1210 [00:29<00:26, 21.49it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  53%|█████▎    | 643/1210 [00:29<00:26, 21.51it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  53%|█████▎    | 644/1210 [00:29<00:26, 21.50it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  53%|█████▎    | 645/1210 [00:29<00:26, 21.51it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  53%|█████▎    | 646/1210 [00:30<00:26, 21.50it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  53%|█████▎    | 647/1210 [00:30<00:26, 21.51it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  54%|█████▎    | 648/1210 [00:30<00:26, 21.51it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  54%|█████▎    | 649/1210 [00:30<00:26, 21.51it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  54%|█████▎    | 650/1210 [00:30<00:26, 21.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  54%|█████▍    | 651/1210 [00:30<00:25, 21.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  54%|█████▍    | 652/1210 [00:30<00:25, 21.51it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  54%|█████▍    | 653/1210 [00:30<00:25, 21.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  54%|█████▍    | 654/1210 [00:30<00:25, 21.51it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  54%|█████▍    | 655/1210 [00:30<00:25, 21.51it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  54%|█████▍    | 656/1210 [00:30<00:25, 21.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  54%|█████▍    | 657/1210 [00:30<00:25, 21.53it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  54%|█████▍    | 658/1210 [00:30<00:25, 21.53it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  54%|█████▍    | 659/1210 [00:30<00:25, 21.53it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  55%|█████▍    | 660/1210 [00:30<00:25, 21.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  55%|█████▍    | 661/1210 [00:30<00:25, 21.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  55%|█████▍    | 662/1210 [00:30<00:25, 21.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  55%|█████▍    | 663/1210 [00:30<00:25, 21.53it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  55%|█████▍    | 664/1210 [00:30<00:25, 21.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  55%|█████▍    | 665/1210 [00:30<00:25, 21.53it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  55%|█████▌    | 666/1210 [00:30<00:25, 21.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  55%|█████▌    | 667/1210 [00:30<00:25, 21.53it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  55%|█████▌    | 668/1210 [00:31<00:25, 21.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  55%|█████▌    | 669/1210 [00:31<00:25, 21.53it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  55%|█████▌    | 670/1210 [00:31<00:25, 21.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  55%|█████▌    | 671/1210 [00:31<00:25, 21.54it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  56%|█████▌    | 672/1210 [00:31<00:25, 21.51it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  56%|█████▌    | 673/1210 [00:31<00:24, 21.54it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  56%|█████▌    | 674/1210 [00:31<00:24, 21.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  56%|█████▌    | 675/1210 [00:31<00:24, 21.54it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  56%|█████▌    | 676/1210 [00:31<00:24, 21.53it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  56%|█████▌    | 677/1210 [00:31<00:24, 21.53it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  56%|█████▌    | 678/1210 [00:31<00:24, 21.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  56%|█████▌    | 679/1210 [00:31<00:24, 21.54it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  56%|█████▌    | 680/1210 [00:31<00:24, 21.53it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  56%|█████▋    | 681/1210 [00:31<00:24, 21.54it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  56%|█████▋    | 682/1210 [00:31<00:24, 21.53it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  56%|█████▋    | 683/1210 [00:31<00:24, 21.54it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  57%|█████▋    | 684/1210 [00:31<00:24, 21.53it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  57%|█████▋    | 685/1210 [00:31<00:24, 21.54it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  57%|█████▋    | 686/1210 [00:31<00:24, 21.55it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  57%|█████▋    | 687/1210 [00:31<00:24, 21.56it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  57%|█████▋    | 688/1210 [00:31<00:24, 21.56it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  57%|█████▋    | 689/1210 [00:31<00:24, 21.57it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  57%|█████▋    | 690/1210 [00:32<00:24, 21.55it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  57%|█████▋    | 691/1210 [00:32<00:24, 21.55it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  57%|█████▋    | 692/1210 [00:32<00:24, 21.55it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  57%|█████▋    | 693/1210 [00:32<00:23, 21.56it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  57%|█████▋    | 694/1210 [00:32<00:23, 21.55it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  57%|█████▋    | 695/1210 [00:32<00:23, 21.56it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  58%|█████▊    | 696/1210 [00:32<00:23, 21.56it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  58%|█████▊    | 697/1210 [00:32<00:23, 21.57it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  58%|█████▊    | 698/1210 [00:32<00:23, 21.56it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  58%|█████▊    | 699/1210 [00:32<00:23, 21.57it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  58%|█████▊    | 700/1210 [00:32<00:23, 21.56it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  58%|█████▊    | 701/1210 [00:32<00:23, 21.57it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  58%|█████▊    | 702/1210 [00:32<00:23, 21.57it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  58%|█████▊    | 703/1210 [00:32<00:23, 21.58it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  58%|█████▊    | 704/1210 [00:32<00:23, 21.57it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  58%|█████▊    | 705/1210 [00:32<00:23, 21.58it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  58%|█████▊    | 706/1210 [00:32<00:23, 21.56it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  58%|█████▊    | 707/1210 [00:32<00:23, 21.58it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  59%|█████▊    | 708/1210 [00:32<00:23, 21.57it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  59%|█████▊    | 709/1210 [00:32<00:23, 21.59it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  59%|█████▊    | 710/1210 [00:32<00:23, 21.57it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  59%|█████▉    | 711/1210 [00:32<00:23, 21.58it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  59%|█████▉    | 712/1210 [00:32<00:23, 21.58it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  59%|█████▉    | 713/1210 [00:33<00:23, 21.59it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  59%|█████▉    | 714/1210 [00:33<00:22, 21.59it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  59%|█████▉    | 715/1210 [00:33<00:22, 21.60it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  59%|█████▉    | 716/1210 [00:33<00:22, 21.60it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  59%|█████▉    | 717/1210 [00:33<00:22, 21.61it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  59%|█████▉    | 718/1210 [00:33<00:22, 21.62it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  59%|█████▉    | 719/1210 [00:33<00:22, 21.63it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  60%|█████▉    | 720/1210 [00:33<00:22, 21.63it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  60%|█████▉    | 721/1210 [00:33<00:22, 21.64it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  60%|█████▉    | 722/1210 [00:33<00:22, 21.64it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  60%|█████▉    | 723/1210 [00:33<00:22, 21.64it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  60%|█████▉    | 724/1210 [00:33<00:22, 21.65it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  60%|█████▉    | 725/1210 [00:33<00:22, 21.65it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  60%|██████    | 726/1210 [00:33<00:22, 21.67it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  60%|██████    | 727/1210 [00:33<00:22, 21.65it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  60%|██████    | 728/1210 [00:33<00:22, 21.67it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  60%|██████    | 729/1210 [00:33<00:22, 21.65it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  60%|██████    | 730/1210 [00:33<00:22, 21.67it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  60%|██████    | 731/1210 [00:33<00:22, 21.67it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  60%|██████    | 732/1210 [00:33<00:22, 21.70it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  61%|██████    | 733/1210 [00:33<00:22, 21.66it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  61%|██████    | 734/1210 [00:33<00:21, 21.68it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  61%|██████    | 735/1210 [00:33<00:21, 21.67it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  61%|██████    | 736/1210 [00:33<00:21, 21.69it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  61%|██████    | 737/1210 [00:34<00:21, 21.68it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  61%|██████    | 738/1210 [00:34<00:21, 21.70it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  61%|██████    | 739/1210 [00:34<00:21, 21.68it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  61%|██████    | 740/1210 [00:34<00:21, 21.70it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  61%|██████    | 741/1210 [00:34<00:21, 21.68it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  61%|██████▏   | 742/1210 [00:34<00:21, 21.71it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  61%|██████▏   | 743/1210 [00:34<00:21, 21.69it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  61%|██████▏   | 744/1210 [00:34<00:21, 21.71it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  62%|██████▏   | 745/1210 [00:34<00:21, 21.70it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  62%|██████▏   | 746/1210 [00:34<00:21, 21.72it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  62%|██████▏   | 747/1210 [00:34<00:21, 21.70it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  62%|██████▏   | 748/1210 [00:34<00:21, 21.72it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  62%|██████▏   | 749/1210 [00:34<00:21, 21.70it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  62%|██████▏   | 750/1210 [00:34<00:21, 21.72it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  62%|██████▏   | 751/1210 [00:34<00:21, 21.70it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  62%|██████▏   | 752/1210 [00:34<00:21, 21.72it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  62%|██████▏   | 753/1210 [00:34<00:21, 21.70it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  62%|██████▏   | 754/1210 [00:34<00:20, 21.72it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  62%|██████▏   | 755/1210 [00:34<00:20, 21.71it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  62%|██████▏   | 756/1210 [00:34<00:20, 21.73it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  63%|██████▎   | 757/1210 [00:34<00:20, 21.71it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  63%|██████▎   | 758/1210 [00:34<00:20, 21.73it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  63%|██████▎   | 759/1210 [00:34<00:20, 21.71it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  63%|██████▎   | 760/1210 [00:34<00:20, 21.74it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  63%|██████▎   | 761/1210 [00:35<00:20, 21.72it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  63%|██████▎   | 762/1210 [00:35<00:20, 21.74it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  63%|██████▎   | 763/1210 [00:35<00:20, 21.72it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  63%|██████▎   | 764/1210 [00:35<00:20, 21.74it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  63%|██████▎   | 765/1210 [00:35<00:20, 21.73it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  63%|██████▎   | 766/1210 [00:35<00:20, 21.70it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  63%|██████▎   | 767/1210 [00:35<00:20, 21.62it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  63%|██████▎   | 768/1210 [00:35<00:20, 21.64it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  64%|██████▎   | 769/1210 [00:35<00:20, 21.62it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  64%|██████▎   | 770/1210 [00:35<00:20, 21.64it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  64%|██████▎   | 771/1210 [00:35<00:20, 21.62it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  64%|██████▍   | 772/1210 [00:35<00:20, 21.63it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  64%|██████▍   | 773/1210 [00:35<00:20, 21.62it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  64%|██████▍   | 774/1210 [00:35<00:20, 21.64it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  64%|██████▍   | 775/1210 [00:35<00:20, 21.62it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  64%|██████▍   | 776/1210 [00:35<00:20, 21.64it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  64%|██████▍   | 777/1210 [00:35<00:20, 21.63it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  64%|██████▍   | 778/1210 [00:35<00:19, 21.65it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  64%|██████▍   | 779/1210 [00:36<00:19, 21.63it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  64%|██████▍   | 780/1210 [00:36<00:19, 21.64it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  65%|██████▍   | 781/1210 [00:36<00:19, 21.63it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  65%|██████▍   | 782/1210 [00:36<00:19, 21.65it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  65%|██████▍   | 783/1210 [00:36<00:19, 21.64it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  65%|██████▍   | 784/1210 [00:36<00:19, 21.66it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  65%|██████▍   | 785/1210 [00:36<00:19, 21.65it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  65%|██████▍   | 786/1210 [00:36<00:19, 21.67it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  65%|██████▌   | 787/1210 [00:36<00:19, 21.65it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  65%|██████▌   | 788/1210 [00:36<00:19, 21.67it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  65%|██████▌   | 789/1210 [00:36<00:19, 21.66it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  65%|██████▌   | 790/1210 [00:36<00:19, 21.68it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  65%|██████▌   | 791/1210 [00:36<00:19, 21.66it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  65%|██████▌   | 792/1210 [00:36<00:19, 21.67it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  66%|██████▌   | 793/1210 [00:36<00:19, 21.67it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  66%|██████▌   | 794/1210 [00:36<00:19, 21.68it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  66%|██████▌   | 795/1210 [00:36<00:19, 21.68it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  66%|██████▌   | 796/1210 [00:36<00:19, 21.70it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  66%|██████▌   | 797/1210 [00:36<00:19, 21.68it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  66%|██████▌   | 798/1210 [00:36<00:18, 21.70it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  66%|██████▌   | 799/1210 [00:36<00:18, 21.69it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  66%|██████▌   | 800/1210 [00:36<00:18, 21.70it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  66%|██████▌   | 801/1210 [00:36<00:18, 21.69it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  66%|██████▋   | 802/1210 [00:36<00:18, 21.71it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  66%|██████▋   | 803/1210 [00:37<00:18, 21.69it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  66%|██████▋   | 804/1210 [00:37<00:18, 21.71it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  67%|██████▋   | 805/1210 [00:37<00:18, 21.70it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  67%|██████▋   | 806/1210 [00:37<00:18, 21.72it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  67%|██████▋   | 807/1210 [00:37<00:18, 21.71it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  67%|██████▋   | 808/1210 [00:37<00:18, 21.73it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  67%|██████▋   | 809/1210 [00:37<00:18, 21.72it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  67%|██████▋   | 810/1210 [00:37<00:18, 21.74it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  67%|██████▋   | 811/1210 [00:37<00:18, 21.72it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  67%|██████▋   | 812/1210 [00:37<00:18, 21.74it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  67%|██████▋   | 813/1210 [00:37<00:18, 21.73it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  67%|██████▋   | 814/1210 [00:37<00:18, 21.75it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  67%|██████▋   | 815/1210 [00:37<00:18, 21.73it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  67%|██████▋   | 816/1210 [00:37<00:18, 21.75it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  68%|██████▊   | 817/1210 [00:37<00:18, 21.72it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  68%|██████▊   | 818/1210 [00:37<00:18, 21.74it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  68%|██████▊   | 819/1210 [00:37<00:17, 21.72it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  68%|██████▊   | 820/1210 [00:37<00:17, 21.74it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  68%|██████▊   | 821/1210 [00:37<00:17, 21.72it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  68%|██████▊   | 822/1210 [00:37<00:17, 21.74it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  68%|██████▊   | 823/1210 [00:37<00:17, 21.74it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  68%|██████▊   | 824/1210 [00:37<00:17, 21.75it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  68%|██████▊   | 825/1210 [00:37<00:17, 21.75it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  68%|██████▊   | 826/1210 [00:37<00:17, 21.76it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  68%|██████▊   | 827/1210 [00:38<00:17, 21.75it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  68%|██████▊   | 828/1210 [00:38<00:17, 21.76it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  69%|██████▊   | 829/1210 [00:38<00:17, 21.76it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  69%|██████▊   | 830/1210 [00:38<00:17, 21.78it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  69%|██████▊   | 831/1210 [00:38<00:17, 21.76it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  69%|██████▉   | 832/1210 [00:38<00:17, 21.79it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  69%|██████▉   | 833/1210 [00:38<00:17, 21.78it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  69%|██████▉   | 834/1210 [00:38<00:17, 21.79it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  69%|██████▉   | 835/1210 [00:38<00:17, 21.78it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  69%|██████▉   | 836/1210 [00:38<00:17, 21.80it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  69%|██████▉   | 837/1210 [00:38<00:17, 21.79it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  69%|██████▉   | 838/1210 [00:38<00:17, 21.81it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  69%|██████▉   | 839/1210 [00:38<00:17, 21.81it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  69%|██████▉   | 840/1210 [00:38<00:16, 21.83it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  70%|██████▉   | 841/1210 [00:38<00:16, 21.79it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  70%|██████▉   | 842/1210 [00:38<00:16, 21.81it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  70%|██████▉   | 843/1210 [00:38<00:16, 21.81it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  70%|██████▉   | 844/1210 [00:38<00:16, 21.83it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  70%|██████▉   | 845/1210 [00:38<00:16, 21.81it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  70%|██████▉   | 846/1210 [00:38<00:16, 21.83it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  70%|███████   | 847/1210 [00:38<00:16, 21.81it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  70%|███████   | 848/1210 [00:38<00:16, 21.83it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  70%|███████   | 849/1210 [00:38<00:16, 21.83it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  70%|███████   | 850/1210 [00:38<00:16, 21.84it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  70%|███████   | 851/1210 [00:38<00:16, 21.83it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  70%|███████   | 852/1210 [00:39<00:16, 21.84it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  70%|███████   | 853/1210 [00:39<00:16, 21.84it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  71%|███████   | 854/1210 [00:39<00:16, 21.85it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  71%|███████   | 855/1210 [00:39<00:16, 21.83it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  71%|███████   | 856/1210 [00:39<00:16, 21.85it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  71%|███████   | 857/1210 [00:39<00:16, 21.84it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  71%|███████   | 858/1210 [00:39<00:16, 21.86it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  71%|███████   | 859/1210 [00:39<00:16, 21.84it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  71%|███████   | 860/1210 [00:39<00:16, 21.86it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  71%|███████   | 861/1210 [00:39<00:15, 21.84it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  71%|███████   | 862/1210 [00:39<00:15, 21.85it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  71%|███████▏  | 863/1210 [00:39<00:15, 21.85it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  71%|███████▏  | 864/1210 [00:39<00:15, 21.87it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  71%|███████▏  | 865/1210 [00:39<00:15, 21.86it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  72%|███████▏  | 866/1210 [00:39<00:15, 21.88it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  72%|███████▏  | 867/1210 [00:39<00:15, 21.86it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  72%|███████▏  | 868/1210 [00:39<00:15, 21.88it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  72%|███████▏  | 869/1210 [00:39<00:15, 21.86it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  72%|███████▏  | 870/1210 [00:39<00:15, 21.88it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  72%|███████▏  | 871/1210 [00:39<00:15, 21.87it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  72%|███████▏  | 872/1210 [00:39<00:15, 21.89it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  72%|███████▏  | 873/1210 [00:39<00:15, 21.88it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  72%|███████▏  | 874/1210 [00:39<00:15, 21.90it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  72%|███████▏  | 875/1210 [00:39<00:15, 21.89it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  72%|███████▏  | 876/1210 [00:39<00:15, 21.91it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  72%|███████▏  | 877/1210 [00:40<00:15, 21.89it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  73%|███████▎  | 878/1210 [00:40<00:15, 21.91it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  73%|███████▎  | 879/1210 [00:40<00:15, 21.90it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  73%|███████▎  | 880/1210 [00:40<00:15, 21.93it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  73%|███████▎  | 881/1210 [00:40<00:15, 21.91it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  73%|███████▎  | 882/1210 [00:40<00:14, 21.93it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  73%|███████▎  | 883/1210 [00:40<00:14, 21.91it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  73%|███████▎  | 884/1210 [00:40<00:14, 21.93it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  73%|███████▎  | 885/1210 [00:40<00:14, 21.93it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  73%|███████▎  | 886/1210 [00:40<00:14, 21.95it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  73%|███████▎  | 887/1210 [00:40<00:14, 21.93it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  73%|███████▎  | 888/1210 [00:40<00:14, 21.95it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  73%|███████▎  | 889/1210 [00:40<00:14, 21.94it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  74%|███████▎  | 890/1210 [00:40<00:14, 21.96it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  74%|███████▎  | 891/1210 [00:40<00:14, 21.94it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  74%|███████▎  | 892/1210 [00:40<00:14, 21.95it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  74%|███████▍  | 893/1210 [00:40<00:14, 21.95it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  74%|███████▍  | 894/1210 [00:40<00:14, 21.97it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  74%|███████▍  | 895/1210 [00:40<00:14, 21.95it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  74%|███████▍  | 896/1210 [00:40<00:14, 21.97it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  74%|███████▍  | 897/1210 [00:40<00:14, 21.96it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  74%|███████▍  | 898/1210 [00:40<00:14, 21.98it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  74%|███████▍  | 899/1210 [00:40<00:14, 21.97it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  74%|███████▍  | 900/1210 [00:40<00:14, 21.99it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  74%|███████▍  | 901/1210 [00:40<00:14, 21.98it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  75%|███████▍  | 902/1210 [00:41<00:14, 22.00it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  75%|███████▍  | 903/1210 [00:41<00:13, 21.98it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  75%|███████▍  | 904/1210 [00:41<00:13, 22.00it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  75%|███████▍  | 905/1210 [00:41<00:13, 21.99it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  75%|███████▍  | 906/1210 [00:41<00:13, 22.01it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  75%|███████▍  | 907/1210 [00:41<00:13, 22.00it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  75%|███████▌  | 908/1210 [00:41<00:13, 22.01it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  75%|███████▌  | 909/1210 [00:41<00:13, 22.00it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  75%|███████▌  | 910/1210 [00:41<00:13, 22.02it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  75%|███████▌  | 911/1210 [00:41<00:13, 22.01it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  75%|███████▌  | 912/1210 [00:41<00:13, 22.01it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  75%|███████▌  | 913/1210 [00:41<00:13, 22.01it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  76%|███████▌  | 914/1210 [00:41<00:13, 22.03it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  76%|███████▌  | 915/1210 [00:41<00:13, 22.00it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  76%|███████▌  | 916/1210 [00:41<00:13, 22.02it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  76%|███████▌  | 917/1210 [00:41<00:13, 22.01it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  76%|███████▌  | 918/1210 [00:41<00:13, 22.03it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  76%|███████▌  | 919/1210 [00:41<00:13, 22.01it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  76%|███████▌  | 920/1210 [00:41<00:13, 22.03it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  76%|███████▌  | 921/1210 [00:41<00:13, 22.02it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  76%|███████▌  | 922/1210 [00:41<00:13, 22.04it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  76%|███████▋  | 923/1210 [00:41<00:13, 22.02it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  76%|███████▋  | 924/1210 [00:41<00:12, 22.04it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  76%|███████▋  | 925/1210 [00:41<00:12, 22.03it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  77%|███████▋  | 926/1210 [00:42<00:12, 22.04it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  77%|███████▋  | 927/1210 [00:42<00:12, 22.04it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  77%|███████▋  | 928/1210 [00:42<00:12, 22.05it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  77%|███████▋  | 929/1210 [00:42<00:12, 22.04it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  77%|███████▋  | 930/1210 [00:42<00:12, 22.05it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  77%|███████▋  | 931/1210 [00:42<00:12, 22.05it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  77%|███████▋  | 932/1210 [00:42<00:12, 22.06it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  77%|███████▋  | 933/1210 [00:42<00:12, 22.05it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  77%|███████▋  | 934/1210 [00:42<00:12, 22.06it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  77%|███████▋  | 935/1210 [00:42<00:12, 22.05it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  77%|███████▋  | 936/1210 [00:42<00:12, 22.05it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  77%|███████▋  | 937/1210 [00:42<00:12, 22.05it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  78%|███████▊  | 938/1210 [00:42<00:12, 22.06it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  78%|███████▊  | 939/1210 [00:42<00:12, 22.06it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  78%|███████▊  | 940/1210 [00:42<00:12, 22.07it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  78%|███████▊  | 941/1210 [00:42<00:12, 22.05it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  78%|███████▊  | 942/1210 [00:42<00:12, 22.06it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  78%|███████▊  | 943/1210 [00:42<00:12, 22.06it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  78%|███████▊  | 944/1210 [00:42<00:12, 22.07it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  78%|███████▊  | 945/1210 [00:42<00:12, 22.07it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  78%|███████▊  | 946/1210 [00:42<00:11, 22.07it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  78%|███████▊  | 947/1210 [00:42<00:11, 22.06it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  78%|███████▊  | 948/1210 [00:42<00:11, 22.07it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  78%|███████▊  | 949/1210 [00:42<00:11, 22.07it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  79%|███████▊  | 950/1210 [00:43<00:11, 22.08it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  79%|███████▊  | 951/1210 [00:43<00:11, 22.07it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  79%|███████▊  | 952/1210 [00:43<00:11, 22.08it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  79%|███████▉  | 953/1210 [00:43<00:11, 22.07it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  79%|███████▉  | 954/1210 [00:43<00:11, 22.06it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  79%|███████▉  | 955/1210 [00:43<00:11, 22.06it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  79%|███████▉  | 956/1210 [00:43<00:11, 22.07it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  79%|███████▉  | 957/1210 [00:43<00:11, 22.04it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  79%|███████▉  | 958/1210 [00:43<00:11, 22.05it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  79%|███████▉  | 959/1210 [00:43<00:11, 22.07it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  79%|███████▉  | 960/1210 [00:43<00:11, 22.04it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  79%|███████▉  | 961/1210 [00:43<00:11, 22.06it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  80%|███████▉  | 962/1210 [00:43<00:11, 22.06it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  80%|███████▉  | 963/1210 [00:43<00:11, 22.07it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  80%|███████▉  | 964/1210 [00:43<00:11, 22.06it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  80%|███████▉  | 965/1210 [00:43<00:11, 22.08it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  80%|███████▉  | 966/1210 [00:43<00:11, 22.07it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  80%|███████▉  | 967/1210 [00:43<00:11, 22.08it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  80%|████████  | 968/1210 [00:43<00:10, 22.08it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  80%|████████  | 969/1210 [00:43<00:10, 22.09it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  80%|████████  | 970/1210 [00:43<00:10, 22.08it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  80%|████████  | 971/1210 [00:43<00:10, 22.09it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  80%|████████  | 972/1210 [00:43<00:10, 22.09it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  80%|████████  | 973/1210 [00:44<00:10, 22.10it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  80%|████████  | 974/1210 [00:44<00:10, 22.09it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  81%|████████  | 975/1210 [00:44<00:10, 22.11it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  81%|████████  | 976/1210 [00:44<00:10, 22.10it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  81%|████████  | 977/1210 [00:44<00:10, 22.11it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  81%|████████  | 978/1210 [00:44<00:10, 22.11it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  81%|████████  | 979/1210 [00:44<00:10, 22.12it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  81%|████████  | 980/1210 [00:44<00:10, 22.12it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  81%|████████  | 981/1210 [00:44<00:10, 22.14it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  81%|████████  | 982/1210 [00:44<00:10, 22.10it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  81%|████████  | 983/1210 [00:44<00:10, 22.12it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  81%|████████▏ | 984/1210 [00:44<00:10, 22.11it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  81%|████████▏ | 985/1210 [00:44<00:10, 22.13it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  81%|████████▏ | 986/1210 [00:44<00:10, 22.13it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  82%|████████▏ | 987/1210 [00:44<00:10, 22.13it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  82%|████████▏ | 988/1210 [00:44<00:10, 22.14it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  82%|████████▏ | 989/1210 [00:44<00:09, 22.15it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  82%|████████▏ | 990/1210 [00:44<00:09, 22.14it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  82%|████████▏ | 991/1210 [00:44<00:09, 22.15it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  82%|████████▏ | 992/1210 [00:44<00:09, 22.15it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  82%|████████▏ | 993/1210 [00:44<00:09, 22.16it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  82%|████████▏ | 994/1210 [00:44<00:09, 22.16it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  82%|████████▏ | 995/1210 [00:44<00:09, 22.17it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  82%|████████▏ | 996/1210 [00:44<00:09, 22.17it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  82%|████████▏ | 997/1210 [00:44<00:09, 22.18it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  82%|████████▏ | 998/1210 [00:45<00:09, 22.18it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  83%|████████▎ | 999/1210 [00:45<00:09, 22.18it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  83%|████████▎ | 1000/1210 [00:45<00:09, 22.18it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  83%|████████▎ | 1001/1210 [00:45<00:09, 22.19it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  83%|████████▎ | 1002/1210 [00:45<00:09, 22.19it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  83%|████████▎ | 1003/1210 [00:45<00:09, 22.19it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  83%|████████▎ | 1004/1210 [00:45<00:09, 22.18it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  83%|████████▎ | 1005/1210 [00:45<00:09, 22.19it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  83%|████████▎ | 1006/1210 [00:45<00:09, 22.18it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  83%|████████▎ | 1007/1210 [00:45<00:09, 22.19it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  83%|████████▎ | 1008/1210 [00:45<00:09, 22.19it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  83%|████████▎ | 1009/1210 [00:45<00:09, 22.21it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  83%|████████▎ | 1010/1210 [00:45<00:09, 22.19it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  84%|████████▎ | 1011/1210 [00:45<00:08, 22.20it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  84%|████████▎ | 1012/1210 [00:45<00:08, 22.19it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  84%|████████▎ | 1013/1210 [00:45<00:08, 22.21it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  84%|████████▍ | 1014/1210 [00:45<00:08, 22.20it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  84%|████████▍ | 1015/1210 [00:45<00:08, 22.22it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  84%|████████▍ | 1016/1210 [00:45<00:08, 22.21it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  84%|████████▍ | 1017/1210 [00:45<00:08, 22.23it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  84%|████████▍ | 1018/1210 [00:45<00:08, 22.22it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  84%|████████▍ | 1019/1210 [00:45<00:08, 22.23it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  84%|████████▍ | 1020/1210 [00:45<00:08, 22.22it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  84%|████████▍ | 1021/1210 [00:45<00:08, 22.24it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  84%|████████▍ | 1022/1210 [00:45<00:08, 22.23it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  85%|████████▍ | 1023/1210 [00:45<00:08, 22.24it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  85%|████████▍ | 1024/1210 [00:46<00:08, 22.23it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  85%|████████▍ | 1025/1210 [00:46<00:08, 22.25it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  85%|████████▍ | 1026/1210 [00:46<00:08, 22.23it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  85%|████████▍ | 1027/1210 [00:46<00:08, 22.25it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  85%|████████▍ | 1028/1210 [00:46<00:08, 22.24it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  85%|████████▌ | 1029/1210 [00:46<00:08, 22.25it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  85%|████████▌ | 1030/1210 [00:46<00:08, 22.24it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  85%|████████▌ | 1031/1210 [00:46<00:08, 22.26it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  85%|████████▌ | 1032/1210 [00:46<00:08, 22.24it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  85%|████████▌ | 1033/1210 [00:46<00:07, 22.26it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  85%|████████▌ | 1034/1210 [00:46<00:07, 22.25it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  86%|████████▌ | 1035/1210 [00:46<00:07, 22.27it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  86%|████████▌ | 1036/1210 [00:46<00:07, 22.26it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  86%|████████▌ | 1037/1210 [00:46<00:07, 22.27it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  86%|████████▌ | 1038/1210 [00:46<00:07, 22.26it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  86%|████████▌ | 1039/1210 [00:46<00:07, 22.28it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  86%|████████▌ | 1040/1210 [00:46<00:07, 22.27it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  86%|████████▌ | 1041/1210 [00:46<00:07, 22.29it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  86%|████████▌ | 1042/1210 [00:46<00:07, 22.28it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  86%|████████▌ | 1043/1210 [00:46<00:07, 22.29it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  86%|████████▋ | 1044/1210 [00:46<00:07, 22.28it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  86%|████████▋ | 1045/1210 [00:46<00:07, 22.30it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  86%|████████▋ | 1046/1210 [00:46<00:07, 22.28it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  87%|████████▋ | 1047/1210 [00:46<00:07, 22.30it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  87%|████████▋ | 1048/1210 [00:46<00:07, 22.31it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  87%|████████▋ | 1049/1210 [00:47<00:07, 22.31it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  87%|████████▋ | 1050/1210 [00:47<00:07, 22.31it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  87%|████████▋ | 1051/1210 [00:47<00:07, 22.32it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  87%|████████▋ | 1052/1210 [00:47<00:07, 22.32it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  87%|████████▋ | 1053/1210 [00:47<00:07, 22.32it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  87%|████████▋ | 1054/1210 [00:47<00:06, 22.32it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  87%|████████▋ | 1055/1210 [00:47<00:06, 22.32it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  87%|████████▋ | 1056/1210 [00:47<00:06, 22.31it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  87%|████████▋ | 1057/1210 [00:47<00:06, 22.32it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  87%|████████▋ | 1058/1210 [00:47<00:06, 22.32it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  88%|████████▊ | 1059/1210 [00:47<00:06, 22.33it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  88%|████████▊ | 1060/1210 [00:47<00:06, 22.33it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  88%|████████▊ | 1061/1210 [00:47<00:06, 22.34it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  88%|████████▊ | 1062/1210 [00:47<00:06, 22.34it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  88%|████████▊ | 1063/1210 [00:47<00:06, 22.34it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  88%|████████▊ | 1064/1210 [00:47<00:06, 22.35it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  88%|████████▊ | 1065/1210 [00:47<00:06, 22.34it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  88%|████████▊ | 1066/1210 [00:47<00:06, 22.35it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  88%|████████▊ | 1067/1210 [00:47<00:06, 22.35it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  88%|████████▊ | 1068/1210 [00:47<00:06, 22.35it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  88%|████████▊ | 1069/1210 [00:47<00:06, 22.35it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  88%|████████▊ | 1070/1210 [00:47<00:06, 22.36it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  89%|████████▊ | 1071/1210 [00:47<00:06, 22.34it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  89%|████████▊ | 1072/1210 [00:47<00:06, 22.36it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  89%|████████▊ | 1073/1210 [00:48<00:06, 22.34it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  89%|████████▉ | 1074/1210 [00:48<00:06, 22.36it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  89%|████████▉ | 1075/1210 [00:48<00:06, 22.36it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  89%|████████▉ | 1076/1210 [00:48<00:05, 22.37it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  89%|████████▉ | 1077/1210 [00:48<00:05, 22.36it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  89%|████████▉ | 1078/1210 [00:48<00:05, 22.38it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  89%|████████▉ | 1079/1210 [00:48<00:05, 22.36it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  89%|████████▉ | 1080/1210 [00:48<00:05, 22.38it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  89%|████████▉ | 1081/1210 [00:48<00:05, 22.37it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  89%|████████▉ | 1082/1210 [00:48<00:05, 22.38it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  90%|████████▉ | 1083/1210 [00:48<00:05, 22.38it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  90%|████████▉ | 1084/1210 [00:48<00:05, 22.38it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  90%|████████▉ | 1085/1210 [00:48<00:05, 22.38it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  90%|████████▉ | 1086/1210 [00:48<00:05, 22.39it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  90%|████████▉ | 1087/1210 [00:48<00:05, 22.39it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  90%|████████▉ | 1088/1210 [00:48<00:05, 22.39it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  90%|█████████ | 1089/1210 [00:48<00:05, 22.39it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  90%|█████████ | 1090/1210 [00:48<00:05, 22.39it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  90%|█████████ | 1091/1210 [00:48<00:05, 22.40it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  90%|█████████ | 1092/1210 [00:48<00:05, 22.39it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  90%|█████████ | 1093/1210 [00:48<00:05, 22.40it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  90%|█████████ | 1094/1210 [00:48<00:05, 22.39it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  90%|█████████ | 1095/1210 [00:48<00:05, 22.40it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  91%|█████████ | 1096/1210 [00:48<00:05, 22.40it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  91%|█████████ | 1097/1210 [00:48<00:05, 22.41it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  91%|█████████ | 1098/1210 [00:49<00:04, 22.41it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  91%|█████████ | 1099/1210 [00:49<00:04, 22.41it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  91%|█████████ | 1100/1210 [00:49<00:04, 22.40it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  91%|█████████ | 1101/1210 [00:49<00:04, 22.42it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  91%|█████████ | 1102/1210 [00:49<00:04, 22.41it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  91%|█████████ | 1103/1210 [00:49<00:04, 22.42it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  91%|█████████ | 1104/1210 [00:49<00:04, 22.41it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  91%|█████████▏| 1105/1210 [00:49<00:04, 22.43it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  91%|█████████▏| 1106/1210 [00:49<00:04, 22.41it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  91%|█████████▏| 1107/1210 [00:49<00:04, 22.42it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  92%|█████████▏| 1108/1210 [00:49<00:04, 22.41it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  92%|█████████▏| 1109/1210 [00:49<00:04, 22.42it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  92%|█████████▏| 1110/1210 [00:49<00:04, 22.42it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  92%|█████████▏| 1111/1210 [00:49<00:04, 22.43it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  92%|█████████▏| 1112/1210 [00:49<00:04, 22.42it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  92%|█████████▏| 1113/1210 [00:49<00:04, 22.43it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  92%|█████████▏| 1114/1210 [00:49<00:04, 22.42it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  92%|█████████▏| 1115/1210 [00:49<00:04, 22.43it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  92%|█████████▏| 1116/1210 [00:49<00:04, 22.42it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  92%|█████████▏| 1117/1210 [00:49<00:04, 22.44it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  92%|█████████▏| 1118/1210 [00:49<00:04, 22.42it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  92%|█████████▏| 1119/1210 [00:49<00:04, 22.44it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  93%|█████████▎| 1120/1210 [00:49<00:04, 22.43it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  93%|█████████▎| 1121/1210 [00:49<00:03, 22.44it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  93%|█████████▎| 1122/1210 [00:50<00:03, 22.43it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  93%|█████████▎| 1123/1210 [00:50<00:03, 22.44it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  93%|█████████▎| 1124/1210 [00:50<00:03, 22.43it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  93%|█████████▎| 1125/1210 [00:50<00:03, 22.45it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  93%|█████████▎| 1126/1210 [00:50<00:03, 22.44it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  93%|█████████▎| 1127/1210 [00:50<00:03, 22.45it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  93%|█████████▎| 1128/1210 [00:50<00:03, 22.43it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  93%|█████████▎| 1129/1210 [00:50<00:03, 22.44it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  93%|█████████▎| 1130/1210 [00:50<00:03, 22.43it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  93%|█████████▎| 1131/1210 [00:50<00:03, 22.45it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  94%|█████████▎| 1132/1210 [00:50<00:03, 22.44it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  94%|█████████▎| 1133/1210 [00:50<00:03, 22.45it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  94%|█████████▎| 1134/1210 [00:50<00:03, 22.44it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  94%|█████████▍| 1135/1210 [00:50<00:03, 22.46it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  94%|█████████▍| 1136/1210 [00:50<00:03, 22.44it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  94%|█████████▍| 1137/1210 [00:50<00:03, 22.45it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  94%|█████████▍| 1138/1210 [00:50<00:03, 22.43it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  94%|█████████▍| 1139/1210 [00:50<00:03, 22.45it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  94%|█████████▍| 1140/1210 [00:50<00:03, 22.44it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  94%|█████████▍| 1141/1210 [00:50<00:03, 22.45it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  94%|█████████▍| 1142/1210 [00:50<00:03, 22.44it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  94%|█████████▍| 1143/1210 [00:50<00:02, 22.45it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  95%|█████████▍| 1144/1210 [00:50<00:02, 22.44it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  95%|█████████▍| 1145/1210 [00:50<00:02, 22.46it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  95%|█████████▍| 1146/1210 [00:51<00:02, 22.44it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  95%|█████████▍| 1147/1210 [00:51<00:02, 22.46it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  95%|█████████▍| 1148/1210 [00:51<00:02, 22.44it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  95%|█████████▍| 1149/1210 [00:51<00:02, 22.45it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  95%|█████████▌| 1150/1210 [00:51<00:02, 22.44it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  95%|█████████▌| 1151/1210 [00:51<00:02, 22.45it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  95%|█████████▌| 1152/1210 [00:51<00:02, 22.44it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  95%|█████████▌| 1153/1210 [00:51<00:02, 22.46it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  95%|█████████▌| 1154/1210 [00:51<00:02, 22.45it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  95%|█████████▌| 1155/1210 [00:51<00:02, 22.46it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  96%|█████████▌| 1156/1210 [00:51<00:02, 22.44it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  96%|█████████▌| 1157/1210 [00:51<00:02, 22.46it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  96%|█████████▌| 1158/1210 [00:51<00:02, 22.44it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  96%|█████████▌| 1159/1210 [00:51<00:02, 22.46it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  96%|█████████▌| 1160/1210 [00:51<00:02, 22.45it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  96%|█████████▌| 1161/1210 [00:51<00:02, 22.46it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  96%|█████████▌| 1162/1210 [00:51<00:02, 22.46it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  96%|█████████▌| 1163/1210 [00:51<00:02, 22.47it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  96%|█████████▌| 1164/1210 [00:51<00:02, 22.46it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  96%|█████████▋| 1165/1210 [00:51<00:02, 22.48it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  96%|█████████▋| 1166/1210 [00:51<00:01, 22.47it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  96%|█████████▋| 1167/1210 [00:51<00:01, 22.48it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  97%|█████████▋| 1168/1210 [00:51<00:01, 22.47it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  97%|█████████▋| 1169/1210 [00:51<00:01, 22.49it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  97%|█████████▋| 1170/1210 [00:52<00:01, 22.47it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  97%|█████████▋| 1171/1210 [00:52<00:01, 22.48it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  97%|█████████▋| 1172/1210 [00:52<00:01, 22.47it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  97%|█████████▋| 1173/1210 [00:52<00:01, 22.49it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  97%|█████████▋| 1174/1210 [00:52<00:01, 22.48it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  97%|█████████▋| 1175/1210 [00:52<00:01, 22.49it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  97%|█████████▋| 1176/1210 [00:52<00:01, 22.49it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  97%|█████████▋| 1177/1210 [00:52<00:01, 22.50it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  97%|█████████▋| 1178/1210 [00:52<00:01, 22.50it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  97%|█████████▋| 1179/1210 [00:52<00:01, 22.50it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  98%|█████████▊| 1180/1210 [00:52<00:01, 22.50it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  98%|█████████▊| 1181/1210 [00:52<00:01, 22.51it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  98%|█████████▊| 1182/1210 [00:52<00:01, 22.50it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  98%|█████████▊| 1183/1210 [00:52<00:01, 22.51it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  98%|█████████▊| 1184/1210 [00:52<00:01, 22.51it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  98%|█████████▊| 1185/1210 [00:52<00:01, 22.51it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  98%|█████████▊| 1186/1210 [00:52<00:01, 22.51it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  98%|█████████▊| 1187/1210 [00:52<00:01, 22.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  98%|█████████▊| 1188/1210 [00:52<00:00, 22.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  98%|█████████▊| 1189/1210 [00:52<00:00, 22.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  98%|█████████▊| 1190/1210 [00:52<00:00, 22.53it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  98%|█████████▊| 1191/1210 [00:52<00:00, 22.51it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  99%|█████████▊| 1192/1210 [00:52<00:00, 22.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  99%|█████████▊| 1193/1210 [00:52<00:00, 22.51it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  99%|█████████▊| 1194/1210 [00:53<00:00, 22.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  99%|█████████▉| 1195/1210 [00:53<00:00, 22.51it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  99%|█████████▉| 1196/1210 [00:53<00:00, 22.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  99%|█████████▉| 1197/1210 [00:53<00:00, 22.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  99%|█████████▉| 1198/1210 [00:53<00:00, 22.53it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  99%|█████████▉| 1199/1210 [00:53<00:00, 22.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  99%|█████████▉| 1200/1210 [00:53<00:00, 22.53it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  99%|█████████▉| 1201/1210 [00:53<00:00, 22.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  99%|█████████▉| 1202/1210 [00:53<00:00, 22.53it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1:  99%|█████████▉| 1203/1210 [00:53<00:00, 22.52it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1: 100%|█████████▉| 1204/1210 [00:53<00:00, 22.54it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1: 100%|█████████▉| 1205/1210 [00:53<00:00, 22.53it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1: 100%|█████████▉| 1206/1210 [00:53<00:00, 22.54it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1: 100%|█████████▉| 1207/1210 [00:53<00:00, 22.53it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1: 100%|█████████▉| 1208/1210 [00:53<00:00, 22.54it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1: 100%|█████████▉| 1209/1210 [00:53<00:00, 22.54it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0241, train_loss_epoch=0.119]\n",
      "Epoch 1: 100%|██████████| 1210/1210 [00:53<00:00, 22.55it/s, loss=0.00481, v_num=1, train_loss_step=0.00523, val_loss=0.0049, train_loss_epoch=0.119]\n",
      "Epoch 2:  50%|█████     | 605/1210 [00:27<00:27, 21.91it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/605 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/605 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  50%|█████     | 606/1210 [00:27<00:27, 21.68it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  50%|█████     | 607/1210 [00:27<00:27, 21.70it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  50%|█████     | 608/1210 [00:28<00:27, 21.69it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  50%|█████     | 609/1210 [00:28<00:27, 21.72it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  50%|█████     | 610/1210 [00:28<00:27, 21.71it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  50%|█████     | 611/1210 [00:28<00:27, 21.73it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  51%|█████     | 612/1210 [00:28<00:27, 21.72it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  51%|█████     | 613/1210 [00:28<00:27, 21.75it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  51%|█████     | 614/1210 [00:28<00:27, 21.75it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  51%|█████     | 615/1210 [00:28<00:27, 21.76it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  51%|█████     | 616/1210 [00:28<00:27, 21.76it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  51%|█████     | 617/1210 [00:28<00:27, 21.76it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  51%|█████     | 618/1210 [00:28<00:27, 21.76it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  51%|█████     | 619/1210 [00:28<00:27, 21.77it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  51%|█████     | 620/1210 [00:28<00:27, 21.77it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  51%|█████▏    | 621/1210 [00:28<00:27, 21.78it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  51%|█████▏    | 622/1210 [00:28<00:26, 21.78it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  51%|█████▏    | 623/1210 [00:28<00:26, 21.80it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  52%|█████▏    | 624/1210 [00:28<00:26, 21.78it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  52%|█████▏    | 625/1210 [00:28<00:26, 21.80it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  52%|█████▏    | 626/1210 [00:28<00:26, 21.80it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  52%|█████▏    | 627/1210 [00:28<00:26, 21.81it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  52%|█████▏    | 628/1210 [00:28<00:26, 21.79it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  52%|█████▏    | 629/1210 [00:28<00:26, 21.80it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  52%|█████▏    | 630/1210 [00:28<00:26, 21.79it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  52%|█████▏    | 631/1210 [00:28<00:26, 21.81it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  52%|█████▏    | 632/1210 [00:28<00:26, 21.79it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  52%|█████▏    | 633/1210 [00:29<00:26, 21.82it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  52%|█████▏    | 634/1210 [00:29<00:26, 21.80it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  52%|█████▏    | 635/1210 [00:29<00:26, 21.82it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  53%|█████▎    | 636/1210 [00:29<00:26, 21.81it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  53%|█████▎    | 637/1210 [00:29<00:26, 21.83it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  53%|█████▎    | 638/1210 [00:29<00:26, 21.81it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  53%|█████▎    | 639/1210 [00:29<00:26, 21.84it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  53%|█████▎    | 640/1210 [00:29<00:26, 21.81it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  53%|█████▎    | 641/1210 [00:29<00:26, 21.84it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  53%|█████▎    | 642/1210 [00:29<00:26, 21.81it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  53%|█████▎    | 643/1210 [00:29<00:25, 21.84it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  53%|█████▎    | 644/1210 [00:29<00:25, 21.81it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  53%|█████▎    | 645/1210 [00:29<00:25, 21.84it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  53%|█████▎    | 646/1210 [00:29<00:25, 21.82it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  53%|█████▎    | 647/1210 [00:29<00:25, 21.85it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  54%|█████▎    | 648/1210 [00:29<00:25, 21.82it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  54%|█████▎    | 649/1210 [00:29<00:25, 21.84it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  54%|█████▎    | 650/1210 [00:29<00:25, 21.83it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  54%|█████▍    | 651/1210 [00:29<00:25, 21.85it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  54%|█████▍    | 652/1210 [00:29<00:25, 21.83it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  54%|█████▍    | 653/1210 [00:29<00:25, 21.85it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  54%|█████▍    | 654/1210 [00:29<00:25, 21.83it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  54%|█████▍    | 655/1210 [00:29<00:25, 21.86it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  54%|█████▍    | 656/1210 [00:30<00:25, 21.84it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  54%|█████▍    | 657/1210 [00:30<00:25, 21.86it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  54%|█████▍    | 658/1210 [00:30<00:25, 21.83it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  54%|█████▍    | 659/1210 [00:30<00:25, 21.86it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  55%|█████▍    | 660/1210 [00:30<00:25, 21.84it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  55%|█████▍    | 661/1210 [00:30<00:25, 21.87it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  55%|█████▍    | 662/1210 [00:30<00:25, 21.85it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  55%|█████▍    | 663/1210 [00:30<00:25, 21.87it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  55%|█████▍    | 664/1210 [00:30<00:24, 21.85it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  55%|█████▍    | 665/1210 [00:30<00:24, 21.88it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  55%|█████▌    | 666/1210 [00:30<00:24, 21.85it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  55%|█████▌    | 667/1210 [00:30<00:24, 21.88it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  55%|█████▌    | 668/1210 [00:30<00:24, 21.84it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  55%|█████▌    | 669/1210 [00:30<00:24, 21.86it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  55%|█████▌    | 670/1210 [00:30<00:24, 21.84it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  55%|█████▌    | 671/1210 [00:30<00:24, 21.87it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  56%|█████▌    | 672/1210 [00:30<00:24, 21.85it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  56%|█████▌    | 673/1210 [00:30<00:24, 21.88it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  56%|█████▌    | 674/1210 [00:30<00:24, 21.86it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  56%|█████▌    | 675/1210 [00:30<00:24, 21.88it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  56%|█████▌    | 676/1210 [00:30<00:24, 21.87it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  56%|█████▌    | 677/1210 [00:30<00:24, 21.89it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  56%|█████▌    | 678/1210 [00:31<00:24, 21.87it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  56%|█████▌    | 679/1210 [00:31<00:24, 21.89it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  56%|█████▌    | 680/1210 [00:31<00:24, 21.87it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  56%|█████▋    | 681/1210 [00:31<00:24, 21.90it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  56%|█████▋    | 682/1210 [00:31<00:24, 21.88it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  56%|█████▋    | 683/1210 [00:31<00:24, 21.90it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  57%|█████▋    | 684/1210 [00:31<00:24, 21.88it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  57%|█████▋    | 685/1210 [00:31<00:23, 21.91it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  57%|█████▋    | 686/1210 [00:31<00:23, 21.89it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  57%|█████▋    | 687/1210 [00:31<00:23, 21.91it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  57%|█████▋    | 688/1210 [00:31<00:23, 21.88it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  57%|█████▋    | 689/1210 [00:31<00:23, 21.90it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  57%|█████▋    | 690/1210 [00:31<00:23, 21.89it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  57%|█████▋    | 691/1210 [00:31<00:23, 21.90it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  57%|█████▋    | 692/1210 [00:31<00:23, 21.90it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  57%|█████▋    | 693/1210 [00:31<00:23, 21.92it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  57%|█████▋    | 694/1210 [00:31<00:23, 21.92it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  57%|█████▋    | 695/1210 [00:31<00:23, 21.92it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  58%|█████▊    | 696/1210 [00:31<00:23, 21.91it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  58%|█████▊    | 697/1210 [00:31<00:23, 21.93it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  58%|█████▊    | 698/1210 [00:31<00:23, 21.92it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  58%|█████▊    | 699/1210 [00:31<00:23, 21.95it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  58%|█████▊    | 700/1210 [00:31<00:23, 21.92it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  58%|█████▊    | 701/1210 [00:31<00:23, 21.95it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  58%|█████▊    | 702/1210 [00:32<00:23, 21.93it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  58%|█████▊    | 703/1210 [00:32<00:23, 21.95it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  58%|█████▊    | 704/1210 [00:32<00:23, 21.93it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  58%|█████▊    | 705/1210 [00:32<00:22, 21.96it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  58%|█████▊    | 706/1210 [00:32<00:22, 21.94it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  58%|█████▊    | 707/1210 [00:32<00:22, 21.96it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  59%|█████▊    | 708/1210 [00:32<00:22, 21.92it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  59%|█████▊    | 709/1210 [00:32<00:22, 21.95it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  59%|█████▊    | 710/1210 [00:32<00:22, 21.92it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  59%|█████▉    | 711/1210 [00:32<00:22, 21.95it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  59%|█████▉    | 712/1210 [00:32<00:22, 21.93it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  59%|█████▉    | 713/1210 [00:32<00:22, 21.96it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  59%|█████▉    | 714/1210 [00:32<00:22, 21.93it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  59%|█████▉    | 715/1210 [00:32<00:22, 21.96it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  59%|█████▉    | 716/1210 [00:32<00:22, 21.95it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  59%|█████▉    | 717/1210 [00:32<00:22, 21.98it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  59%|█████▉    | 718/1210 [00:32<00:22, 21.96it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  59%|█████▉    | 719/1210 [00:32<00:22, 21.98it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  60%|█████▉    | 720/1210 [00:32<00:22, 21.97it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  60%|█████▉    | 721/1210 [00:32<00:22, 21.99it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  60%|█████▉    | 722/1210 [00:32<00:22, 21.97it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  60%|█████▉    | 723/1210 [00:32<00:22, 22.00it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  60%|█████▉    | 724/1210 [00:32<00:22, 21.97it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  60%|█████▉    | 725/1210 [00:32<00:22, 21.99it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  60%|██████    | 726/1210 [00:33<00:22, 21.98it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  60%|██████    | 727/1210 [00:33<00:21, 22.00it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  60%|██████    | 728/1210 [00:33<00:21, 21.97it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  60%|██████    | 729/1210 [00:33<00:21, 21.99it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  60%|██████    | 730/1210 [00:33<00:21, 21.97it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  60%|██████    | 731/1210 [00:33<00:21, 21.99it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  60%|██████    | 732/1210 [00:33<00:21, 21.97it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  61%|██████    | 733/1210 [00:33<00:21, 21.99it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  61%|██████    | 734/1210 [00:33<00:21, 21.84it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  61%|██████    | 735/1210 [00:33<00:21, 21.86it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  61%|██████    | 736/1210 [00:33<00:21, 21.85it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  61%|██████    | 737/1210 [00:33<00:21, 21.86it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  61%|██████    | 738/1210 [00:33<00:21, 21.86it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  61%|██████    | 739/1210 [00:33<00:21, 21.88it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  61%|██████    | 740/1210 [00:33<00:21, 21.86it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  61%|██████    | 741/1210 [00:33<00:21, 21.88it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  61%|██████▏   | 742/1210 [00:33<00:21, 21.86it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  61%|██████▏   | 743/1210 [00:33<00:21, 21.89it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  61%|██████▏   | 744/1210 [00:34<00:21, 21.87it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  62%|██████▏   | 745/1210 [00:34<00:21, 21.87it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  62%|██████▏   | 746/1210 [00:34<00:21, 21.85it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  62%|██████▏   | 747/1210 [00:34<00:21, 21.87it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  62%|██████▏   | 748/1210 [00:34<00:21, 21.85it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  62%|██████▏   | 749/1210 [00:34<00:21, 21.86it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  62%|██████▏   | 750/1210 [00:34<00:21, 21.85it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  62%|██████▏   | 751/1210 [00:34<00:20, 21.87it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  62%|██████▏   | 752/1210 [00:34<00:20, 21.84it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  62%|██████▏   | 753/1210 [00:34<00:20, 21.86it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  62%|██████▏   | 754/1210 [00:34<00:20, 21.85it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  62%|██████▏   | 755/1210 [00:34<00:20, 21.86it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  62%|██████▏   | 756/1210 [00:34<00:20, 21.84it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  63%|██████▎   | 757/1210 [00:34<00:20, 21.86it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  63%|██████▎   | 758/1210 [00:34<00:20, 21.84it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  63%|██████▎   | 759/1210 [00:34<00:20, 21.86it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  63%|██████▎   | 760/1210 [00:34<00:20, 21.84it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  63%|██████▎   | 761/1210 [00:34<00:20, 21.86it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  63%|██████▎   | 762/1210 [00:34<00:20, 21.84it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  63%|██████▎   | 763/1210 [00:34<00:20, 21.87it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  63%|██████▎   | 764/1210 [00:34<00:20, 21.85it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  63%|██████▎   | 765/1210 [00:34<00:20, 21.88it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  63%|██████▎   | 766/1210 [00:35<00:20, 21.87it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  63%|██████▎   | 767/1210 [00:35<00:20, 21.89it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  63%|██████▎   | 768/1210 [00:35<00:20, 21.87it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  64%|██████▎   | 769/1210 [00:35<00:20, 21.89it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  64%|██████▎   | 770/1210 [00:35<00:20, 21.87it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  64%|██████▎   | 771/1210 [00:35<00:20, 21.89it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  64%|██████▍   | 772/1210 [00:35<00:20, 21.90it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  64%|██████▍   | 773/1210 [00:35<00:19, 21.91it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  64%|██████▍   | 774/1210 [00:35<00:19, 21.91it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  64%|██████▍   | 775/1210 [00:35<00:19, 21.92it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  64%|██████▍   | 776/1210 [00:35<00:19, 21.90it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  64%|██████▍   | 777/1210 [00:35<00:19, 21.92it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  64%|██████▍   | 778/1210 [00:35<00:19, 21.92it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  64%|██████▍   | 779/1210 [00:35<00:19, 21.93it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  64%|██████▍   | 780/1210 [00:35<00:19, 21.91it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  65%|██████▍   | 781/1210 [00:35<00:19, 21.93it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  65%|██████▍   | 782/1210 [00:35<00:19, 21.90it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  65%|██████▍   | 783/1210 [00:35<00:19, 21.92it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  65%|██████▍   | 784/1210 [00:35<00:19, 21.92it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  65%|██████▍   | 785/1210 [00:35<00:19, 21.94it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  65%|██████▍   | 786/1210 [00:35<00:19, 21.91it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  65%|██████▌   | 787/1210 [00:35<00:19, 21.92it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  65%|██████▌   | 788/1210 [00:35<00:19, 21.91it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  65%|██████▌   | 789/1210 [00:35<00:19, 21.93it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  65%|██████▌   | 790/1210 [00:36<00:19, 21.93it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  65%|██████▌   | 791/1210 [00:36<00:19, 21.95it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  65%|██████▌   | 792/1210 [00:36<00:19, 21.93it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  66%|██████▌   | 793/1210 [00:36<00:19, 21.95it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  66%|██████▌   | 794/1210 [00:36<00:18, 21.94it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  66%|██████▌   | 795/1210 [00:36<00:18, 21.95it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  66%|██████▌   | 796/1210 [00:36<00:18, 21.95it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  66%|██████▌   | 797/1210 [00:36<00:18, 21.97it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  66%|██████▌   | 798/1210 [00:36<00:18, 21.96it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  66%|██████▌   | 799/1210 [00:36<00:18, 21.97it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  66%|██████▌   | 800/1210 [00:36<00:18, 21.97it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  66%|██████▌   | 801/1210 [00:36<00:18, 21.96it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  66%|██████▋   | 802/1210 [00:36<00:18, 21.95it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  66%|██████▋   | 803/1210 [00:36<00:18, 21.96it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  66%|██████▋   | 804/1210 [00:36<00:18, 21.96it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  67%|██████▋   | 805/1210 [00:36<00:18, 21.97it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  67%|██████▋   | 806/1210 [00:36<00:18, 21.96it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  67%|██████▋   | 807/1210 [00:36<00:18, 21.98it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  67%|██████▋   | 808/1210 [00:36<00:18, 21.97it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  67%|██████▋   | 809/1210 [00:36<00:18, 21.98it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  67%|██████▋   | 810/1210 [00:36<00:18, 21.98it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  67%|██████▋   | 811/1210 [00:36<00:18, 21.99it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  67%|██████▋   | 812/1210 [00:36<00:18, 21.99it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  67%|██████▋   | 813/1210 [00:36<00:18, 22.00it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  67%|██████▋   | 814/1210 [00:37<00:18, 22.00it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  67%|██████▋   | 815/1210 [00:37<00:17, 22.01it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  67%|██████▋   | 816/1210 [00:37<00:17, 22.00it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  68%|██████▊   | 817/1210 [00:37<00:17, 22.02it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  68%|██████▊   | 818/1210 [00:37<00:17, 22.02it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  68%|██████▊   | 819/1210 [00:37<00:17, 22.02it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  68%|██████▊   | 820/1210 [00:37<00:17, 22.02it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  68%|██████▊   | 821/1210 [00:37<00:17, 22.01it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  68%|██████▊   | 822/1210 [00:37<00:17, 22.01it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  68%|██████▊   | 823/1210 [00:37<00:17, 22.01it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  68%|██████▊   | 824/1210 [00:37<00:17, 22.01it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  68%|██████▊   | 825/1210 [00:37<00:17, 22.02it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  68%|██████▊   | 826/1210 [00:37<00:17, 22.01it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  68%|██████▊   | 827/1210 [00:37<00:17, 22.02it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  68%|██████▊   | 828/1210 [00:37<00:17, 22.02it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  69%|██████▊   | 829/1210 [00:37<00:17, 22.03it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  69%|██████▊   | 830/1210 [00:37<00:17, 22.02it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  69%|██████▊   | 831/1210 [00:37<00:17, 22.03it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  69%|██████▉   | 832/1210 [00:37<00:17, 22.02it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  69%|██████▉   | 833/1210 [00:37<00:17, 22.04it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  69%|██████▉   | 834/1210 [00:37<00:17, 22.04it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  69%|██████▉   | 835/1210 [00:37<00:16, 22.06it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  69%|██████▉   | 836/1210 [00:37<00:16, 22.04it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  69%|██████▉   | 837/1210 [00:37<00:16, 22.06it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  69%|██████▉   | 838/1210 [00:38<00:16, 22.05it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  69%|██████▉   | 839/1210 [00:38<00:16, 22.07it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  69%|██████▉   | 840/1210 [00:38<00:16, 22.03it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  70%|██████▉   | 841/1210 [00:38<00:16, 22.05it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  70%|██████▉   | 842/1210 [00:38<00:16, 22.04it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  70%|██████▉   | 843/1210 [00:38<00:16, 22.06it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  70%|██████▉   | 844/1210 [00:38<00:16, 22.04it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  70%|██████▉   | 845/1210 [00:38<00:16, 22.06it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  70%|██████▉   | 846/1210 [00:38<00:16, 22.05it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  70%|███████   | 847/1210 [00:38<00:16, 22.07it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  70%|███████   | 848/1210 [00:38<00:16, 22.06it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  70%|███████   | 849/1210 [00:38<00:16, 22.08it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  70%|███████   | 850/1210 [00:38<00:16, 22.06it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  70%|███████   | 851/1210 [00:38<00:16, 22.08it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  70%|███████   | 852/1210 [00:38<00:16, 22.07it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  70%|███████   | 853/1210 [00:38<00:16, 22.09it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  71%|███████   | 854/1210 [00:38<00:16, 22.08it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  71%|███████   | 855/1210 [00:38<00:16, 22.10it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  71%|███████   | 856/1210 [00:38<00:16, 22.08it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  71%|███████   | 857/1210 [00:38<00:15, 22.10it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  71%|███████   | 858/1210 [00:38<00:15, 22.09it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  71%|███████   | 859/1210 [00:38<00:15, 22.11it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  71%|███████   | 860/1210 [00:38<00:15, 22.08it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  71%|███████   | 861/1210 [00:38<00:15, 22.09it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  71%|███████   | 862/1210 [00:39<00:15, 22.08it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  71%|███████▏  | 863/1210 [00:39<00:15, 22.10it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  71%|███████▏  | 864/1210 [00:39<00:15, 22.09it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  71%|███████▏  | 865/1210 [00:39<00:15, 22.11it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  72%|███████▏  | 866/1210 [00:39<00:15, 22.09it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  72%|███████▏  | 867/1210 [00:39<00:15, 22.11it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  72%|███████▏  | 868/1210 [00:39<00:15, 22.10it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  72%|███████▏  | 869/1210 [00:39<00:15, 22.12it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  72%|███████▏  | 870/1210 [00:39<00:15, 22.11it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  72%|███████▏  | 871/1210 [00:39<00:15, 22.13it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  72%|███████▏  | 872/1210 [00:39<00:15, 22.12it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  72%|███████▏  | 873/1210 [00:39<00:15, 22.14it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  72%|███████▏  | 874/1210 [00:39<00:15, 22.12it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  72%|███████▏  | 875/1210 [00:39<00:15, 22.14it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  72%|███████▏  | 876/1210 [00:39<00:15, 22.13it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  72%|███████▏  | 877/1210 [00:39<00:15, 22.15it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  73%|███████▎  | 878/1210 [00:39<00:15, 22.13it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  73%|███████▎  | 879/1210 [00:39<00:14, 22.14it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  73%|███████▎  | 880/1210 [00:39<00:14, 22.13it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  73%|███████▎  | 881/1210 [00:39<00:14, 22.14it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  73%|███████▎  | 882/1210 [00:39<00:14, 22.13it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  73%|███████▎  | 883/1210 [00:39<00:14, 22.14it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  73%|███████▎  | 884/1210 [00:39<00:14, 22.13it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  73%|███████▎  | 885/1210 [00:39<00:14, 22.15it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  73%|███████▎  | 886/1210 [00:40<00:14, 22.14it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  73%|███████▎  | 887/1210 [00:40<00:14, 22.16it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  73%|███████▎  | 888/1210 [00:40<00:14, 22.15it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  73%|███████▎  | 889/1210 [00:40<00:14, 22.16it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  74%|███████▎  | 890/1210 [00:40<00:14, 22.15it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  74%|███████▎  | 891/1210 [00:40<00:14, 22.17it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  74%|███████▎  | 892/1210 [00:40<00:14, 22.17it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  74%|███████▍  | 893/1210 [00:40<00:14, 22.18it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  74%|███████▍  | 894/1210 [00:40<00:14, 22.17it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  74%|███████▍  | 895/1210 [00:40<00:14, 22.19it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  74%|███████▍  | 896/1210 [00:40<00:14, 22.18it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  74%|███████▍  | 897/1210 [00:40<00:14, 22.20it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  74%|███████▍  | 898/1210 [00:40<00:14, 22.18it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  74%|███████▍  | 899/1210 [00:40<00:14, 22.20it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  74%|███████▍  | 900/1210 [00:40<00:13, 22.19it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  74%|███████▍  | 901/1210 [00:40<00:13, 22.21it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  75%|███████▍  | 902/1210 [00:40<00:13, 22.18it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  75%|███████▍  | 903/1210 [00:40<00:13, 22.19it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  75%|███████▍  | 904/1210 [00:40<00:13, 22.16it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  75%|███████▍  | 905/1210 [00:40<00:13, 22.18it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  75%|███████▍  | 906/1210 [00:40<00:13, 22.17it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  75%|███████▍  | 907/1210 [00:40<00:13, 22.19it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  75%|███████▌  | 908/1210 [00:40<00:13, 22.17it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  75%|███████▌  | 909/1210 [00:40<00:13, 22.19it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  75%|███████▌  | 910/1210 [00:41<00:13, 22.17it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  75%|███████▌  | 911/1210 [00:41<00:13, 22.19it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  75%|███████▌  | 912/1210 [00:41<00:13, 22.18it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  75%|███████▌  | 913/1210 [00:41<00:13, 22.20it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  76%|███████▌  | 914/1210 [00:41<00:13, 22.18it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  76%|███████▌  | 915/1210 [00:41<00:13, 22.20it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  76%|███████▌  | 916/1210 [00:41<00:13, 22.18it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  76%|███████▌  | 917/1210 [00:41<00:13, 22.20it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  76%|███████▌  | 918/1210 [00:41<00:13, 22.18it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  76%|███████▌  | 919/1210 [00:41<00:13, 22.19it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  76%|███████▌  | 920/1210 [00:41<00:13, 22.16it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  76%|███████▌  | 921/1210 [00:41<00:13, 22.18it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  76%|███████▌  | 922/1210 [00:41<00:12, 22.17it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  76%|███████▋  | 923/1210 [00:41<00:12, 22.19it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  76%|███████▋  | 924/1210 [00:41<00:12, 22.18it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  76%|███████▋  | 925/1210 [00:41<00:12, 22.19it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  77%|███████▋  | 926/1210 [00:41<00:12, 22.19it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  77%|███████▋  | 927/1210 [00:41<00:12, 22.21it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  77%|███████▋  | 928/1210 [00:41<00:12, 22.18it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  77%|███████▋  | 929/1210 [00:41<00:12, 22.20it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  77%|███████▋  | 930/1210 [00:41<00:12, 22.20it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  77%|███████▋  | 931/1210 [00:41<00:12, 22.21it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  77%|███████▋  | 932/1210 [00:41<00:12, 22.21it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  77%|███████▋  | 933/1210 [00:42<00:12, 22.20it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  77%|███████▋  | 934/1210 [00:42<00:12, 22.20it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  77%|███████▋  | 935/1210 [00:42<00:12, 22.21it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  77%|███████▋  | 936/1210 [00:42<00:12, 22.22it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  77%|███████▋  | 937/1210 [00:42<00:12, 22.23it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  78%|███████▊  | 938/1210 [00:42<00:12, 22.22it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  78%|███████▊  | 939/1210 [00:42<00:12, 22.23it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  78%|███████▊  | 940/1210 [00:42<00:12, 22.23it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  78%|███████▊  | 941/1210 [00:42<00:12, 22.24it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  78%|███████▊  | 942/1210 [00:42<00:12, 22.23it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  78%|███████▊  | 943/1210 [00:42<00:12, 22.23it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  78%|███████▊  | 944/1210 [00:42<00:11, 22.23it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  78%|███████▊  | 945/1210 [00:42<00:11, 22.24it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  78%|███████▊  | 946/1210 [00:42<00:11, 22.22it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  78%|███████▊  | 947/1210 [00:42<00:11, 22.23it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  78%|███████▊  | 948/1210 [00:42<00:11, 22.23it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  78%|███████▊  | 949/1210 [00:42<00:11, 22.23it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  79%|███████▊  | 950/1210 [00:42<00:11, 22.23it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  79%|███████▊  | 951/1210 [00:42<00:11, 22.24it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  79%|███████▊  | 952/1210 [00:42<00:11, 22.24it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  79%|███████▉  | 953/1210 [00:42<00:11, 22.25it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  79%|███████▉  | 954/1210 [00:42<00:11, 22.24it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  79%|███████▉  | 955/1210 [00:42<00:11, 22.25it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  79%|███████▉  | 956/1210 [00:42<00:11, 22.25it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  79%|███████▉  | 957/1210 [00:43<00:11, 22.25it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  79%|███████▉  | 958/1210 [00:43<00:11, 22.25it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  79%|███████▉  | 959/1210 [00:43<00:11, 22.26it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  79%|███████▉  | 960/1210 [00:43<00:11, 22.26it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  79%|███████▉  | 961/1210 [00:43<00:11, 22.26it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  80%|███████▉  | 962/1210 [00:43<00:11, 22.26it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  80%|███████▉  | 963/1210 [00:43<00:11, 22.27it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  80%|███████▉  | 964/1210 [00:43<00:11, 22.27it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  80%|███████▉  | 965/1210 [00:43<00:11, 22.27it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  80%|███████▉  | 966/1210 [00:43<00:10, 22.26it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  80%|███████▉  | 967/1210 [00:43<00:10, 22.27it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  80%|████████  | 968/1210 [00:43<00:10, 22.27it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  80%|████████  | 969/1210 [00:43<00:10, 22.27it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  80%|████████  | 970/1210 [00:43<00:10, 22.25it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  80%|████████  | 971/1210 [00:43<00:10, 22.25it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  80%|████████  | 972/1210 [00:43<00:10, 22.25it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  80%|████████  | 973/1210 [00:43<00:10, 22.26it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  80%|████████  | 974/1210 [00:43<00:10, 22.27it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  81%|████████  | 975/1210 [00:43<00:10, 22.27it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  81%|████████  | 976/1210 [00:43<00:10, 22.27it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  81%|████████  | 977/1210 [00:43<00:10, 22.27it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  81%|████████  | 978/1210 [00:43<00:10, 22.27it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  81%|████████  | 979/1210 [00:43<00:10, 22.28it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  81%|████████  | 980/1210 [00:43<00:10, 22.28it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  81%|████████  | 981/1210 [00:44<00:10, 22.29it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  81%|████████  | 982/1210 [00:44<00:10, 22.28it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  81%|████████  | 983/1210 [00:44<00:10, 22.29it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  81%|████████▏ | 984/1210 [00:44<00:10, 22.28it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  81%|████████▏ | 985/1210 [00:44<00:10, 22.29it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  81%|████████▏ | 986/1210 [00:44<00:10, 22.28it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  82%|████████▏ | 987/1210 [00:44<00:10, 22.29it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  82%|████████▏ | 988/1210 [00:44<00:09, 22.28it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  82%|████████▏ | 989/1210 [00:44<00:09, 22.30it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  82%|████████▏ | 990/1210 [00:44<00:09, 22.27it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  82%|████████▏ | 991/1210 [00:44<00:09, 22.29it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  82%|████████▏ | 992/1210 [00:44<00:09, 22.28it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  82%|████████▏ | 993/1210 [00:44<00:09, 22.30it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  82%|████████▏ | 994/1210 [00:44<00:09, 22.27it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  82%|████████▏ | 995/1210 [00:44<00:09, 22.29it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  82%|████████▏ | 996/1210 [00:44<00:09, 22.28it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  82%|████████▏ | 997/1210 [00:44<00:09, 22.30it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  82%|████████▏ | 998/1210 [00:44<00:09, 22.29it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  83%|████████▎ | 999/1210 [00:44<00:09, 22.30it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  83%|████████▎ | 1000/1210 [00:44<00:09, 22.29it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  83%|████████▎ | 1001/1210 [00:44<00:09, 22.31it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  83%|████████▎ | 1002/1210 [00:44<00:09, 22.29it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  83%|████████▎ | 1003/1210 [00:44<00:09, 22.31it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  83%|████████▎ | 1004/1210 [00:45<00:09, 22.30it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  83%|████████▎ | 1005/1210 [00:45<00:09, 22.32it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  83%|████████▎ | 1006/1210 [00:45<00:09, 22.31it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  83%|████████▎ | 1007/1210 [00:45<00:09, 22.33it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  83%|████████▎ | 1008/1210 [00:45<00:09, 22.31it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  83%|████████▎ | 1009/1210 [00:45<00:09, 22.33it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  83%|████████▎ | 1010/1210 [00:45<00:08, 22.31it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  84%|████████▎ | 1011/1210 [00:45<00:08, 22.32it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  84%|████████▎ | 1012/1210 [00:45<00:08, 22.31it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  84%|████████▎ | 1013/1210 [00:45<00:08, 22.32it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  84%|████████▍ | 1014/1210 [00:45<00:08, 22.31it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  84%|████████▍ | 1015/1210 [00:45<00:08, 22.32it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  84%|████████▍ | 1016/1210 [00:45<00:08, 22.31it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  84%|████████▍ | 1017/1210 [00:45<00:08, 22.33it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  84%|████████▍ | 1018/1210 [00:45<00:08, 22.31it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  84%|████████▍ | 1019/1210 [00:45<00:08, 22.33it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  84%|████████▍ | 1020/1210 [00:45<00:08, 22.32it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  84%|████████▍ | 1021/1210 [00:45<00:08, 22.34it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  84%|████████▍ | 1022/1210 [00:45<00:08, 22.32it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  85%|████████▍ | 1023/1210 [00:45<00:08, 22.34it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  85%|████████▍ | 1024/1210 [00:45<00:08, 22.32it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  85%|████████▍ | 1025/1210 [00:45<00:08, 22.34it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  85%|████████▍ | 1026/1210 [00:45<00:08, 22.33it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  85%|████████▍ | 1027/1210 [00:45<00:08, 22.35it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  85%|████████▍ | 1028/1210 [00:46<00:08, 22.33it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  85%|████████▌ | 1029/1210 [00:46<00:08, 22.35it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  85%|████████▌ | 1030/1210 [00:46<00:08, 22.33it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  85%|████████▌ | 1031/1210 [00:46<00:08, 22.34it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  85%|████████▌ | 1032/1210 [00:46<00:07, 22.32it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  85%|████████▌ | 1033/1210 [00:46<00:07, 22.34it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  85%|████████▌ | 1034/1210 [00:46<00:07, 22.33it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  86%|████████▌ | 1035/1210 [00:46<00:07, 22.34it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  86%|████████▌ | 1036/1210 [00:46<00:07, 22.33it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  86%|████████▌ | 1037/1210 [00:46<00:07, 22.35it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  86%|████████▌ | 1038/1210 [00:46<00:07, 22.33it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  86%|████████▌ | 1039/1210 [00:46<00:07, 22.35it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  86%|████████▌ | 1040/1210 [00:46<00:07, 22.33it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  86%|████████▌ | 1041/1210 [00:46<00:07, 22.35it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  86%|████████▌ | 1042/1210 [00:46<00:07, 22.34it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  86%|████████▌ | 1043/1210 [00:46<00:07, 22.36it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  86%|████████▋ | 1044/1210 [00:46<00:07, 22.34it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  86%|████████▋ | 1045/1210 [00:46<00:07, 22.35it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  86%|████████▋ | 1046/1210 [00:46<00:07, 22.34it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  87%|████████▋ | 1047/1210 [00:46<00:07, 22.35it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  87%|████████▋ | 1048/1210 [00:46<00:07, 22.33it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  87%|████████▋ | 1049/1210 [00:46<00:07, 22.34it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  87%|████████▋ | 1050/1210 [00:47<00:07, 22.32it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  87%|████████▋ | 1051/1210 [00:47<00:07, 22.34it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  87%|████████▋ | 1052/1210 [00:47<00:07, 22.32it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  87%|████████▋ | 1053/1210 [00:47<00:07, 22.34it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  87%|████████▋ | 1054/1210 [00:47<00:06, 22.32it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  87%|████████▋ | 1055/1210 [00:47<00:06, 22.34it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  87%|████████▋ | 1056/1210 [00:47<00:06, 22.33it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  87%|████████▋ | 1057/1210 [00:47<00:06, 22.34it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  87%|████████▋ | 1058/1210 [00:47<00:06, 22.33it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  88%|████████▊ | 1059/1210 [00:47<00:06, 22.35it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  88%|████████▊ | 1060/1210 [00:47<00:06, 22.34it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  88%|████████▊ | 1061/1210 [00:47<00:06, 22.35it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  88%|████████▊ | 1062/1210 [00:47<00:06, 22.34it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  88%|████████▊ | 1063/1210 [00:47<00:06, 22.36it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  88%|████████▊ | 1064/1210 [00:47<00:06, 22.34it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  88%|████████▊ | 1065/1210 [00:47<00:06, 22.36it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  88%|████████▊ | 1066/1210 [00:47<00:06, 22.35it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  88%|████████▊ | 1067/1210 [00:47<00:06, 22.36it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  88%|████████▊ | 1068/1210 [00:47<00:06, 22.34it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  88%|████████▊ | 1069/1210 [00:47<00:06, 22.35it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  88%|████████▊ | 1070/1210 [00:47<00:06, 22.35it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  89%|████████▊ | 1071/1210 [00:47<00:06, 22.36it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  89%|████████▊ | 1072/1210 [00:47<00:06, 22.36it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  89%|████████▊ | 1073/1210 [00:47<00:06, 22.37it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  89%|████████▉ | 1074/1210 [00:48<00:06, 22.36it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  89%|████████▉ | 1075/1210 [00:48<00:06, 22.38it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  89%|████████▉ | 1076/1210 [00:48<00:05, 22.37it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  89%|████████▉ | 1077/1210 [00:48<00:05, 22.39it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  89%|████████▉ | 1078/1210 [00:48<00:05, 22.38it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  89%|████████▉ | 1079/1210 [00:48<00:05, 22.39it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  89%|████████▉ | 1080/1210 [00:48<00:05, 22.38it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  89%|████████▉ | 1081/1210 [00:48<00:05, 22.39it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  89%|████████▉ | 1082/1210 [00:48<00:05, 22.39it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  90%|████████▉ | 1083/1210 [00:48<00:05, 22.39it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  90%|████████▉ | 1084/1210 [00:48<00:05, 22.39it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  90%|████████▉ | 1085/1210 [00:48<00:05, 22.40it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  90%|████████▉ | 1086/1210 [00:48<00:05, 22.40it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  90%|████████▉ | 1087/1210 [00:48<00:05, 22.40it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  90%|████████▉ | 1088/1210 [00:48<00:05, 22.39it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  90%|█████████ | 1089/1210 [00:48<00:05, 22.39it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  90%|█████████ | 1090/1210 [00:48<00:05, 22.39it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  90%|█████████ | 1091/1210 [00:48<00:05, 22.40it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  90%|█████████ | 1092/1210 [00:48<00:05, 22.39it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  90%|█████████ | 1093/1210 [00:48<00:05, 22.40it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  90%|█████████ | 1094/1210 [00:48<00:05, 22.40it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  90%|█████████ | 1095/1210 [00:48<00:05, 22.41it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  91%|█████████ | 1096/1210 [00:48<00:05, 22.40it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  91%|█████████ | 1097/1210 [00:48<00:05, 22.41it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  91%|█████████ | 1098/1210 [00:49<00:04, 22.41it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  91%|█████████ | 1099/1210 [00:49<00:04, 22.41it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  91%|█████████ | 1100/1210 [00:49<00:04, 22.41it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  91%|█████████ | 1101/1210 [00:49<00:04, 22.42it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  91%|█████████ | 1102/1210 [00:49<00:04, 22.41it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  91%|█████████ | 1103/1210 [00:49<00:04, 22.43it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  91%|█████████ | 1104/1210 [00:49<00:04, 22.43it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  91%|█████████▏| 1105/1210 [00:49<00:04, 22.43it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  91%|█████████▏| 1106/1210 [00:49<00:04, 22.43it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  91%|█████████▏| 1107/1210 [00:49<00:04, 22.44it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  92%|█████████▏| 1108/1210 [00:49<00:04, 22.43it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  92%|█████████▏| 1109/1210 [00:49<00:04, 22.44it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  92%|█████████▏| 1110/1210 [00:49<00:04, 22.42it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  92%|█████████▏| 1111/1210 [00:49<00:04, 22.43it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  92%|█████████▏| 1112/1210 [00:49<00:04, 22.42it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  92%|█████████▏| 1113/1210 [00:49<00:04, 22.44it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  92%|█████████▏| 1114/1210 [00:49<00:04, 22.42it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  92%|█████████▏| 1115/1210 [00:49<00:04, 22.44it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  92%|█████████▏| 1116/1210 [00:49<00:04, 22.43it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  92%|█████████▏| 1117/1210 [00:49<00:04, 22.44it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  92%|█████████▏| 1118/1210 [00:49<00:04, 22.43it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  92%|█████████▏| 1119/1210 [00:49<00:04, 22.44it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  93%|█████████▎| 1120/1210 [00:49<00:04, 22.44it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  93%|█████████▎| 1121/1210 [00:49<00:03, 22.45it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  93%|█████████▎| 1122/1210 [00:49<00:03, 22.44it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  93%|█████████▎| 1123/1210 [00:50<00:03, 22.46it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  93%|█████████▎| 1124/1210 [00:50<00:03, 22.45it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  93%|█████████▎| 1125/1210 [00:50<00:03, 22.46it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  93%|█████████▎| 1126/1210 [00:50<00:03, 22.45it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  93%|█████████▎| 1127/1210 [00:50<00:03, 22.46it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  93%|█████████▎| 1128/1210 [00:50<00:03, 22.45it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  93%|█████████▎| 1129/1210 [00:50<00:03, 22.47it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  93%|█████████▎| 1130/1210 [00:50<00:03, 22.46it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  93%|█████████▎| 1131/1210 [00:50<00:03, 22.47it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  94%|█████████▎| 1132/1210 [00:50<00:03, 22.46it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  94%|█████████▎| 1133/1210 [00:50<00:03, 22.47it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  94%|█████████▎| 1134/1210 [00:50<00:03, 22.46it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  94%|█████████▍| 1135/1210 [00:50<00:03, 22.48it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  94%|█████████▍| 1136/1210 [00:50<00:03, 22.46it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  94%|█████████▍| 1137/1210 [00:50<00:03, 22.48it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  94%|█████████▍| 1138/1210 [00:50<00:03, 22.47it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  94%|█████████▍| 1139/1210 [00:50<00:03, 22.48it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  94%|█████████▍| 1140/1210 [00:50<00:03, 22.46it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  94%|█████████▍| 1141/1210 [00:50<00:03, 22.48it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  94%|█████████▍| 1142/1210 [00:50<00:03, 22.47it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  94%|█████████▍| 1143/1210 [00:50<00:02, 22.48it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  95%|█████████▍| 1144/1210 [00:50<00:02, 22.47it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  95%|█████████▍| 1145/1210 [00:50<00:02, 22.48it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  95%|█████████▍| 1146/1210 [00:50<00:02, 22.48it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  95%|█████████▍| 1147/1210 [00:51<00:02, 22.49it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  95%|█████████▍| 1148/1210 [00:51<00:02, 22.48it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  95%|█████████▍| 1149/1210 [00:51<00:02, 22.49it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  95%|█████████▌| 1150/1210 [00:51<00:02, 22.48it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  95%|█████████▌| 1151/1210 [00:51<00:02, 22.49it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  95%|█████████▌| 1152/1210 [00:51<00:02, 22.47it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  95%|█████████▌| 1153/1210 [00:51<00:02, 22.49it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  95%|█████████▌| 1154/1210 [00:51<00:02, 22.47it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  95%|█████████▌| 1155/1210 [00:51<00:02, 22.49it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  96%|█████████▌| 1156/1210 [00:51<00:02, 22.48it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  96%|█████████▌| 1157/1210 [00:51<00:02, 22.49it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  96%|█████████▌| 1158/1210 [00:51<00:02, 22.46it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  96%|█████████▌| 1159/1210 [00:51<00:02, 22.48it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  96%|█████████▌| 1160/1210 [00:51<00:02, 22.47it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  96%|█████████▌| 1161/1210 [00:51<00:02, 22.49it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  96%|█████████▌| 1162/1210 [00:51<00:02, 22.48it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  96%|█████████▌| 1163/1210 [00:51<00:02, 22.49it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  96%|█████████▌| 1164/1210 [00:51<00:02, 22.48it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  96%|█████████▋| 1165/1210 [00:51<00:02, 22.50it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  96%|█████████▋| 1166/1210 [00:51<00:01, 22.48it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  96%|█████████▋| 1167/1210 [00:51<00:01, 22.49it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  97%|█████████▋| 1168/1210 [00:51<00:01, 22.49it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  97%|█████████▋| 1169/1210 [00:51<00:01, 22.50it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  97%|█████████▋| 1170/1210 [00:52<00:01, 22.50it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  97%|█████████▋| 1171/1210 [00:52<00:01, 22.51it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  97%|█████████▋| 1172/1210 [00:52<00:01, 22.50it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  97%|█████████▋| 1173/1210 [00:52<00:01, 22.51it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  97%|█████████▋| 1174/1210 [00:52<00:01, 22.50it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  97%|█████████▋| 1175/1210 [00:52<00:01, 22.51it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  97%|█████████▋| 1176/1210 [00:52<00:01, 22.50it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  97%|█████████▋| 1177/1210 [00:52<00:01, 22.51it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  97%|█████████▋| 1178/1210 [00:52<00:01, 22.50it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  97%|█████████▋| 1179/1210 [00:52<00:01, 22.52it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  98%|█████████▊| 1180/1210 [00:52<00:01, 22.51it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  98%|█████████▊| 1181/1210 [00:52<00:01, 22.53it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  98%|█████████▊| 1182/1210 [00:52<00:01, 22.51it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  98%|█████████▊| 1183/1210 [00:52<00:01, 22.53it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  98%|█████████▊| 1184/1210 [00:52<00:01, 22.51it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  98%|█████████▊| 1185/1210 [00:52<00:01, 22.52it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  98%|█████████▊| 1186/1210 [00:52<00:01, 22.51it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  98%|█████████▊| 1187/1210 [00:52<00:01, 22.53it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  98%|█████████▊| 1188/1210 [00:52<00:00, 22.52it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  98%|█████████▊| 1189/1210 [00:52<00:00, 22.53it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  98%|█████████▊| 1190/1210 [00:52<00:00, 22.51it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  98%|█████████▊| 1191/1210 [00:52<00:00, 22.53it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  99%|█████████▊| 1192/1210 [00:52<00:00, 22.52it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  99%|█████████▊| 1193/1210 [00:52<00:00, 22.53it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  99%|█████████▊| 1194/1210 [00:53<00:00, 22.53it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  99%|█████████▉| 1195/1210 [00:53<00:00, 22.54it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  99%|█████████▉| 1196/1210 [00:53<00:00, 22.53it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  99%|█████████▉| 1197/1210 [00:53<00:00, 22.55it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  99%|█████████▉| 1198/1210 [00:53<00:00, 22.54it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  99%|█████████▉| 1199/1210 [00:53<00:00, 22.55it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  99%|█████████▉| 1200/1210 [00:53<00:00, 22.54it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  99%|█████████▉| 1201/1210 [00:53<00:00, 22.55it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  99%|█████████▉| 1202/1210 [00:53<00:00, 22.55it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2:  99%|█████████▉| 1203/1210 [00:53<00:00, 22.56it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2: 100%|█████████▉| 1204/1210 [00:53<00:00, 22.55it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2: 100%|█████████▉| 1205/1210 [00:53<00:00, 22.56it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2: 100%|█████████▉| 1206/1210 [00:53<00:00, 22.55it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2: 100%|█████████▉| 1207/1210 [00:53<00:00, 22.55it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2: 100%|█████████▉| 1208/1210 [00:53<00:00, 22.55it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2: 100%|█████████▉| 1209/1210 [00:53<00:00, 22.56it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.0049, train_loss_epoch=0.0106]\n",
      "Epoch 2: 100%|██████████| 1210/1210 [00:53<00:00, 22.56it/s, loss=0.00225, v_num=1, train_loss_step=0.00219, val_loss=0.00235, train_loss_epoch=0.0106]\n",
      "Epoch 3:  15%|█▌        | 185/1210 [00:09<00:51, 19.86it/s, loss=0.002, v_num=1, train_loss_step=0.00162, val_loss=0.00235, train_loss_epoch=0.00206]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=2000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  15%|█▌        | 185/1210 [00:09<00:51, 19.82it/s, loss=0.002, v_num=1, train_loss_step=0.00162, val_loss=0.00235, train_loss_epoch=0.00206]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger('tf_logs', name=\"FCwC_model\")\n",
    "\n",
    "trainer = Trainer(gpus=1, max_np_epoch=10, logger=logger)\n",
    "trainer.fit(model, train_dataloaders=dataloader, val_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xm65lJFvnnLs",
    "outputId": "ffd88daa-bbd8-4b3a-c1f8-e693f3ffece0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yield', 'bin0', 'bin1', 'bin2', 'bin3', 'bin4', 'bin5', 'bin6', 'bin7', 'bin8', 'bin9', 'bin10', 'bin11', 'bin12', 'bin13', 'bin14', 'bin15', 'bin16', 'bin17', 'bin18', 'bin19', 'bin20', 'bin21', 'bin22', 'bin23', 'bin24', 'bin25', 'bin26', 'bin27', 'bin28', 'bin29', 'bin30', 'bin31', 'bin32', 'bin33', 'bin34', 'bin35', 'bin36', 'bin37', 'bin38', 'bin39', 'bin40', 'bin41', 'bin42', 'bin43', 'bin44', 'bin45', 'bin46', 'bin47', 'bin48', 'bin49', 'bin50', 'bin51', 'bin52', 'bin53', 'bin54', 'bin55', 'bin56', 'bin57', 'bin58', 'bin59', 'bin60', 'bin61', 'bin62', 'bin63', 'bin64', 'bin65', 'bin66', 'bin67', 'bin68', 'bin69', 'bin70', 'bin71', 'bin72', 'bin73', 'bin74', 'bin75', 'bin76', 'bin77', 'bin78', 'bin79', 'bin80', 'bin81', 'bin82', 'bin83', 'bin84', 'bin85', 'bin86', 'bin87', 'bin88', 'bin89', 'bin90', 'bin91', 'bin92', 'bin93', 'bin94', 'bin95', 'bin96', 'bin97', 'bin98', 'bin99', 'bin100', 'bin101', 'bin102', 'bin103', 'bin104', 'bin105', 'bin106', 'bin107', 'bin108', 'bin109', 'bin110', 'bin111', 'bin112', 'bin113', 'bin114', 'bin115', 'bin116', 'bin117', 'bin118', 'bin119', 'bin120', 'bin121', 'bin122', 'bin123', 'bin124', 'bin125', 'bin126', 'bin127', 'bin128', 'bin129', 'bin130', 'bin131', 'bin132', 'bin133', 'bin134', 'bin135', 'bin136', 'bin137', 'bin138', 'bin139', 'bin140', 'bin141', 'bin142', 'bin143', 'bin144', 'bin145', 'bin146', 'bin147', 'bin148', 'bin149', 'bin150', 'bin151', 'bin152', 'bin153', 'bin154', 'bin155', 'bin156', 'bin157', 'bin158', 'bin159', 'bin160', 'bin161', 'bin162', 'bin163', 'bin164', 'bin165', 'bin166', 'bin167', 'bin168', 'bin169', 'bin170', 'bin171', 'bin172', 'bin173', 'bin174', 'bin175', 'bin176', 'bin177', 'bin178', 'bin179', 'bin180', 'bin181', 'bin182', 'bin183', 'bin184', 'bin185', 'bin186', 'bin187', 'bin188', 'bin189', 'bin190', 'bin191', 'bin192', 'bin193', 'bin194', 'bin195', 'bin196', 'bin197', 'bin198', 'bin199', 'bin200', 'bin201', 'bin202', 'bin203', 'bin204', 'bin205', 'bin206', 'bin207', 'bin208', 'bin209', 'bin210', 'bin211', 'bin212', 'bin213', 'bin214', 'bin215', 'bin216', 'bin217', 'bin218', 'bin219', 'bin220', 'bin221', 'bin222', 'bin223', 'bin224', 'bin225', 'bin226', 'bin227', 'bin228', 'bin229', 'bin230', 'bin231', 'bin232', 'bin233', 'bin234', 'bin235', 'bin236', 'bin237', 'bin238', 'bin239', 'bin240', 'bin241', 'bin242', 'bin243', 'bin244', 'bin245', 'bin246', 'bin247', 'bin248', 'bin249', 'bin250', 'bin251', 'bin252', 'bin253', 'bin254', 'bin255', 'bin256', 'bin257', 'bin258', 'bin259', 'bin260', 'bin261', 'bin262', 'bin263', 'bin264', 'bin265', 'bin266', 'bin267', 'bin268', 'bin269', 'bin270', 'bin271', 'bin272', 'bin273', 'bin274', 'bin275', 'bin276', 'bin277', 'bin278', 'bin279', 'bin280', 'bin281', 'bin282', 'bin283', 'bin284', 'bin285', 'bin286', 'bin287', 'bin288', 'bin289', 'bin290', 'bin291', 'bin292', 'bin293', 'bin294', 'bin295', 'bin296', 'bin297', 'bin298', 'bin299', 'bin300', 'bin301', 'bin302', 'bin303', 'bin304', 'bin305', 'bin306', 'bin307', 'bin308', 'bin309', 'bin310', 'bin311', 'bin312', 'bin313', 'bin314', 'bin315', 'bin316', 'bin317', 'bin318', 'bin319', 'bin320', 'bin321', 'bin322', 'bin323', 'bin324', 'bin325', 'bin326', 'bin327', 'bin328', 'bin329', 'bin330', 'bin331', 'bin332', 'bin333', 'bin334', 'bin335', 'bin336', 'bin337', 'bin338', 'bin339', 'bin340', 'bin341', 'bin342', 'bin343', 'bin344', 'bin345', 'bin346', 'bin347', 'bin348', 'bin349', 'bin350', 'bin351', 'bin352', 'bin353', 'bin354', 'bin355', 'bin356', 'bin357', 'bin358', 'bin359', 'bin360', 'bin361', 'bin362', 'bin363', 'bin364', 'bin365', 'bin366', 'bin367', 'bin368', 'bin369', 'bin370', 'bin371', 'bin372', 'bin373', 'bin374', 'bin375', 'bin376', 'bin377', 'bin378', 'bin379', 'bin380', 'bin381', 'bin382', 'bin383', 'bin384', 'bin385', 'bin386', 'bin387', 'bin388', 'bin389', 'bin390', 'bin391', 'bin392', 'bin393', 'bin394', 'bin395', 'bin396', 'bin397', 'bin398', 'bin399', 'bin400', 'bin401', 'bin402', 'bin403', 'bin404', 'bin405', 'bin406', 'bin407', 'bin408', 'bin409', 'bin410', 'bin411', 'bin412', 'bin413', 'bin414', 'bin415', 'bin416', 'bin417', 'bin418', 'bin419', 'bin420', 'bin421', 'bin422', 'bin423', 'bin424', 'bin425', 'bin426', 'bin427', 'bin428', 'bin429', 'bin430', 'bin431', 'bin432', 'bin433', 'bin434', 'bin435', 'bin436', 'bin437', 'bin438', 'bin439', 'bin440', 'bin441', 'bin442', 'bin443', 'bin444', 'bin445', 'bin446', 'bin447', 'bin448', 'bin449', 'bin450', 'bin451', 'bin452', 'bin453', 'bin454', 'bin455', 'bin456', 'bin457', 'bin458', 'bin459', 'bin460', 'bin461', 'bin462', 'bin463', 'bin464', 'bin465', 'bin466', 'bin467', 'bin468', 'bin469', 'bin470', 'bin471', 'bin472', 'bin473', 'bin474', 'bin475', 'bin476', 'bin477', 'bin478', 'bin479', 'bin480', 'bin481', 'bin482', 'bin483', 'bin484', 'bin485', 'bin486', 'bin487', 'bin488', 'bin489', 'bin490', 'bin491', 'bin492', 'bin493', 'bin494', 'bin495', 'bin496', 'bin497', 'bin498', 'bin499', 'bin500', 'bin501', 'bin502', 'bin503', 'bin504', 'bin505', 'bin506', 'bin507', 'bin508', 'bin509', 'bin510', 'bin511']\n"
     ]
    }
   ],
   "source": [
    "max_prediction_length = 3\n",
    "max_encoder_length = 8\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "# print(training_cutoff)\n",
    "# x = data[lambda x: x.time_idx <= training_cutoff]\n",
    "# print()\n",
    "\n",
    "bins_name = list([\"yield\"])\n",
    "for bin in range(0, 512):\n",
    "  bins_name.append(f'bin{bin}')\n",
    "\n",
    "print(bins_name)\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx = \"time_idx\",\n",
    "    target = \"yield\",\n",
    "    group_ids = [\"county\", \"bands\", \"time\"],\n",
    "    min_encoder_length = max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length = max_encoder_length,\n",
    "    min_prediction_length = 1,\n",
    "    max_prediction_length = max_prediction_length,\n",
    "    # static_categoricals = [\"county\"],\n",
    "    # static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],\n",
    "    # time_varying_known_categoricals=[\"special_days\", \"month\"],\n",
    "    # variable_groups={\"special_days\": special_days},  # group of categorical variables can be treated as one variable\n",
    "    # time_varying_known_reals=[\"time_idx\", \"price_regular\", \"discount_in_percent\"],\n",
    "    # time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals = bins_name,\n",
    "    allow_missing_timesteps = True,\n",
    "    # target_normalizer=GroupNormalizer(\n",
    "    #     groups=[\"agency\", \"sku\"], transformation=\"softplus\"\n",
    "    # ),  # use softplus and normalize by group\n",
    "    # add_relative_time_idx = True,\n",
    "    # add_target_scales = True,\n",
    "    # add_encoder_length = True,\n",
    ")\n",
    "\n",
    "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "# for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 1  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train =  True, batch_size = batch_size, num_workers = 0)\n",
    "val_dataloader = validation.to_dataloader(train = False, batch_size = batch_size, num_workers = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gThPXEaensCI",
    "outputId": "45c75a22-e2c8-4538-f11e-eaf1aea74050"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03679545223712921"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
    "baseline_predictions = Baseline().predict(val_dataloader)\n",
    "(actuals - baseline_predictions).abs().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BR2PH1m8opGv"
   },
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "# pl.seed_everything(42)\n",
    "# trainer = pl.Trainer(\n",
    "#     gpus=0,\n",
    "#     # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "#     # of the gradient for recurrent neural networks\n",
    "#     gradient_clip_val=0.1,\n",
    "#     auto_lr_find = True,\n",
    "# )\n",
    "\n",
    "\n",
    "# tft = TemporalFusionTransformer.from_dataset(\n",
    "#     training,\n",
    "#     # not meaningful for finding the learning rate but otherwise very important\n",
    "#     learning_rate=0.03,\n",
    "#     hidden_size=16,  # most important hyperparameter apart from learning rate\n",
    "#     # number of attention heads. Set to up to 4 for large datasets\n",
    "#     attention_head_size=1,\n",
    "#     dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "#     hidden_continuous_size=8,  # set to <= hidden_size\n",
    "#     output_size=1,  # 7 quantiles by default\n",
    "#     loss=QuantileLoss(),\n",
    "#     # reduce learning rate if no improvement in validation loss after x epochs\n",
    "#     reduce_on_plateau_patience=4,\n",
    "# )\n",
    "# print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQexRoPJopqa"
   },
   "outputs": [],
   "source": [
    "# # find optimal learning rate\n",
    "# res = trainer.tuner.lr_find(\n",
    "#     tft,\n",
    "#     train_dataloaders=train_dataloader,\n",
    "#     val_dataloaders=val_dataloader,\n",
    "#     max_lr=10.0,\n",
    "#     min_lr=1e-6,\n",
    "# )\n",
    "\n",
    "# # res = trainer.tuner.lr_find(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader,)\n",
    "\n",
    "# print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "# fig = res.plot(show=True, suggest=True)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x8ohrZoqosSX",
    "outputId": "4f54da2d-924c-4403-93ee-6ce56f78cc4c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus=0)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=0)` instead.\n",
      "  rank_zero_deprecation(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 371.8k\n"
     ]
    }
   ],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=300,\n",
    "    gpus=0,\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=1,  # 7 quantiles by default\n",
    "    loss=MAPE(),  #QuantileLoss(), #MAPE(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zcFzxHVrVQ-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorboard as tb \n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 939,
     "referenced_widgets": [
      "14d07476dd484e5e8a7bca7bf59ceae5",
      "c56fd12d72db4e4caf6d5ef0ade8b9f8",
      "fa10f2402f224290815d92ee89213162",
      "daf65e7e84e34266929b680250bc7a46",
      "ba34c1e6d0cb4b03870cb4aef1b6c32c",
      "cdd9d4ddccdc4e06a76aafddce2701f0",
      "ae9c80f166694da09e56af90999a7c6b",
      "5eeae71bc8384c98921f6ead0351b047",
      "74f31ab6b038471ca73833d340ba5606",
      "88d278124f704f90a67d37da11072342",
      "325556a59ba44f10bea30d875b032bcd"
     ]
    },
    "id": "L3O1F_Joo0YG",
    "outputId": "a2851582-3695-4376-9460-bdf095cbcd12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | MAPE                            | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 0     \n",
      "3  | prescalers                         | ModuleDict                      | 8.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 528   \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 357 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 0     \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "20 | output_layer                       | Linear                          | 17    \n",
      "----------------------------------------------------------------------------------------\n",
      "371 K     Trainable params\n",
      "0         Non-trainable params\n",
      "371 K     Total params\n",
      "1.487     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d07476dd484e5e8a7bca7bf59ceae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0b3f9c96239d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m trainer.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"`Trainer.fit()` requires a `LightningModule`, got: {model.__class__.__qualname__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lightning_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    604\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         )\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}: trainer tearing down\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0misolate_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0;31m# enable train mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1260\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1262\u001b[0;31m                 \u001b[0mval_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_sanity_check_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_run_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\u001b[0m in \u001b[0;36mon_run_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# free memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\u001b[0m in \u001b[0;36m_evaluation_epoch_end\u001b[0;34m(self, outputs)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# call the model epoch end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mhook_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test_epoch_end\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"validation_epoch_end\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_lightning_module_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_or_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_on_evaluation_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[LightningModule]{pl_module.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1342\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m         \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_forecasting/models/base_model.py\u001b[0m in \u001b[0;36mvalidation_epoch_end\u001b[0;34m(self, outputs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py\u001b[0m in \u001b[0;36mepoch_end\u001b[0;34m(self, outputs)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \"\"\"\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_interval\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_interpretation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     def interpret_output(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py\u001b[0m in \u001b[0;36mlog_interpretation\u001b[0;34m(self, outputs)\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0;31m# use padded_stack because decoder length histogram can be of different length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpadded_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"interpretation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"right\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"interpretation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m         }\n\u001b[1;32m    798\u001b[0m         \u001b[0;31m# normalize attention with length histogram squared to account for: 1. zeros in attention and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyhBzIoi5cqP"
   },
   "outputs": [],
   "source": [
    "# load the best model according to the validation loss\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtZA-Mom5gaW"
   },
   "outputs": [],
   "source": [
    "# calcualte mean absolute error on validation set\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "predictions = best_tft.predict(val_dataloader)\n",
    "(actuals - predictions).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXtVdTBs5jZP"
   },
   "outputs": [],
   "source": [
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoYevMoT5wGA"
   },
   "outputs": [],
   "source": [
    "for idx in range(10):  # plot 10 examples\n",
    "    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQT6Kxwl53gu"
   },
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cufHwzV8pLiR"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "# create study\n",
    "study = optimize_hyperparameters(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=200,\n",
    "    max_epochs=50,\n",
    "    gradient_clip_val_range=(0.01, 1.0),\n",
    "    hidden_size_range=(8, 128),\n",
    "    hidden_continuous_size_range=(8, 128),\n",
    "    attention_head_size_range=(1, 4),\n",
    "    learning_rate_range=(0.001, 0.1),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=30),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08YeXDvqo3ZE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "14d07476dd484e5e8a7bca7bf59ceae5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c56fd12d72db4e4caf6d5ef0ade8b9f8",
       "IPY_MODEL_fa10f2402f224290815d92ee89213162",
       "IPY_MODEL_daf65e7e84e34266929b680250bc7a46"
      ],
      "layout": "IPY_MODEL_ba34c1e6d0cb4b03870cb4aef1b6c32c"
     }
    },
    "325556a59ba44f10bea30d875b032bcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5eeae71bc8384c98921f6ead0351b047": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74f31ab6b038471ca73833d340ba5606": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "88d278124f704f90a67d37da11072342": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae9c80f166694da09e56af90999a7c6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba34c1e6d0cb4b03870cb4aef1b6c32c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "c56fd12d72db4e4caf6d5ef0ade8b9f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdd9d4ddccdc4e06a76aafddce2701f0",
      "placeholder": "​",
      "style": "IPY_MODEL_ae9c80f166694da09e56af90999a7c6b",
      "value": "Sanity Checking DataLoader 0:   0%"
     }
    },
    "cdd9d4ddccdc4e06a76aafddce2701f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "daf65e7e84e34266929b680250bc7a46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88d278124f704f90a67d37da11072342",
      "placeholder": "​",
      "style": "IPY_MODEL_325556a59ba44f10bea30d875b032bcd",
      "value": " 0/2 [00:00&lt;?, ?it/s]"
     }
    },
    "fa10f2402f224290815d92ee89213162": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5eeae71bc8384c98921f6ead0351b047",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_74f31ab6b038471ca73833d340ba5606",
      "value": 0
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
