{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvpNHOs_nTQK",
    "outputId": "58e5e73c-d790-40a6-a1dc-1f6638a75f55"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive/')\n",
    "\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pDXvdYFuLYSI",
    "outputId": "db9c1dcc-d2d2-450c-bab9-29f775f571d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hy-tmp\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "home_dir = '/content/gdrive/My Drive/AChina' \n",
    "home_dir = '/hy-tmp'\n",
    "os.chdir(home_dir)\n",
    "!pwd\n",
    "\n",
    "!pip install tqdm\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WPwRIbsMnaq8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "\n",
    "# os.chdir(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYbhRWaxneDl",
    "outputId": "c79dccdb-c661-4f1c-ca13-a14d7bd2c1c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (1.8.1)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scipy) (1.22.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.12.0+cu113)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.8/dist-packages (1.8.5.post0)\n",
      "Requirement already satisfied: pytorch_forecasting in /usr/local/lib/python3.8/dist-packages (0.10.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.22.3)\n",
      "Requirement already satisfied: tensorboardX>=2.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2.5.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.64.1)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2022.11.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: lightning-utilities!=0.4.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (0.4.2)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (0.11.0)\n",
      "Requirement already satisfied: scikit-learn<1.2,>=0.24 in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (1.1.1)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (0.13.5)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (3.5.2)\n",
      "Requirement already satisfied: optuna<3.0.0,>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (2.10.1)\n",
      "Requirement already satisfied: scipy<2.0,>=1.8 in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (1.8.1)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (1.4.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.28.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.3)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch_forecasting) (1.4.45)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.8/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch_forecasting) (6.7.0)\n",
      "Requirement already satisfied: cliff in /usr/local/lib/python3.8/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch_forecasting) (4.1.0)\n",
      "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch_forecasting) (0.9.0)\n",
      "Requirement already satisfied: alembic in /usr/local/lib/python3.8/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch_forecasting) (1.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas<2.0.0,>=1.3.0->pytorch_forecasting) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas<2.0.0,>=1.3.0->pytorch_forecasting) (2022.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<1.2,>=0.24->pytorch_forecasting) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<1.2,>=0.24->pytorch_forecasting) (3.1.0)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=2.2->pytorch-lightning) (3.19.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pytorch_forecasting) (1.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pytorch_forecasting) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pytorch_forecasting) (4.34.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pytorch_forecasting) (9.2.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.8/dist-packages (from statsmodels->pytorch_forecasting) (0.5.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from patsy>=0.5.2->statsmodels->pytorch_forecasting) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy>=1.1.0->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (2.0.1)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (5.8.0)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.8/dist-packages (from alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (1.2.4)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (4.12.0)\n",
      "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (0.5.1)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (2.4.2)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (3.5.0)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (4.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.3)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (0.2.5)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (1.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (3.8.1)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from stevedore>=2.0.1->cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (5.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from Mako->alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (2.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.8/dist-packages (12.6.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from rich) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich) (2.12.0)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from rich) (4.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install torch torchvision\n",
    "# !pip install pytorch_lightning\n",
    "# !pip install pytorch_forecasting\n",
    "\n",
    "# !pip install torch -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install pytorch-forecasting\n",
    "\n",
    "!pip install scipy\n",
    "!pip install torch pytorch-lightning pytorch_forecasting\n",
    "!pip install rich\n",
    "\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAPE, SMAPE, PoissonLoss, QuantileLoss\n",
    "# from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "65_fJ6MIbncZ",
    "outputId": "06e9c0ab-8d58-480b-86d1-4ca275159e4a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>years</th>\n",
       "      <th>yield</th>\n",
       "      <th>sownareas</th>\n",
       "      <th>yieldvals</th>\n",
       "      <th>county</th>\n",
       "      <th>bands</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>bin0</th>\n",
       "      <th>bin1</th>\n",
       "      <th>...</th>\n",
       "      <th>bin502</th>\n",
       "      <th>bin503</th>\n",
       "      <th>bin504</th>\n",
       "      <th>bin505</th>\n",
       "      <th>bin506</th>\n",
       "      <th>bin507</th>\n",
       "      <th>bin508</th>\n",
       "      <th>bin509</th>\n",
       "      <th>bin510</th>\n",
       "      <th>bin511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>band_0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>band_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>band_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>band_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>band_4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>band_5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>band_6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>band_7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>band_8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>band_0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>band_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>band_2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>band_3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>band_4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>band_5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 520 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  years    yield  sownareas  yieldvals  county   bands  \\\n",
       "0            0   2003  0.61295      75.21       46.1       0  band_0   \n",
       "1            1   2003  0.61295      75.21       46.1       0  band_1   \n",
       "2            2   2003  0.61295      75.21       46.1       0  band_2   \n",
       "3            3   2003  0.61295      75.21       46.1       0  band_3   \n",
       "4            4   2003  0.61295      75.21       46.1       0  band_4   \n",
       "5            5   2003  0.61295      75.21       46.1       0  band_5   \n",
       "6            6   2003  0.61295      75.21       46.1       0  band_6   \n",
       "7            7   2003  0.61295      75.21       46.1       0  band_7   \n",
       "8            8   2003  0.61295      75.21       46.1       0  band_8   \n",
       "9            9   2003  0.61295      75.21       46.1       0  band_0   \n",
       "10          10   2003  0.61295      75.21       46.1       0  band_1   \n",
       "11          11   2003  0.61295      75.21       46.1       0  band_2   \n",
       "12          12   2003  0.61295      75.21       46.1       0  band_3   \n",
       "13          13   2003  0.61295      75.21       46.1       0  band_4   \n",
       "14          14   2003  0.61295      75.21       46.1       0  band_5   \n",
       "\n",
       "    time_idx  bin0  bin1  ...    bin502    bin503    bin504    bin505  \\\n",
       "0          0   0.0   0.0  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1          0   0.0   0.0  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "2          0   0.0   0.0  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "3          0   0.0   0.0  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "4          0   0.0   0.0  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "5          0   0.0   0.0  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "6          0   0.0   0.0  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "7          0   0.0   0.0  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "8          0   0.0   0.0  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "9          1   0.0   0.0  ...  0.001077  0.000833  0.001126  0.000098   \n",
       "10         1   0.0   0.0  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "11         1   0.0   0.0  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "12         1   0.0   0.0  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "13         1   0.0   0.0  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "14         1   0.0   0.0  ...  0.000097  0.000484  0.000242  0.000339   \n",
       "\n",
       "      bin506    bin507    bin508    bin509    bin510    bin511  \n",
       "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "8   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "9   0.000784  0.000881  0.000979  0.000441  0.000294  0.000196  \n",
       "10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "12  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "13  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "14  0.000048  0.000000  0.000145  0.000194  0.000000  0.000000  \n",
       "\n",
       "[15 rows x 520 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('corn_china_pandas.csv')  # encoding= 'unicode_escape')\n",
    "\n",
    "data[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TOncsJYonfF_"
   },
   "outputs": [],
   "source": [
    "# years = [x for x in range(2003, 2019)]\n",
    "\n",
    "# data.rename(columns={'time_idx' : 'time'}, inplace=True)  \n",
    "\n",
    "# # data[5:15]  \n",
    "# data.insert(1, \"time_idx\", data['years'])  \n",
    "# # df = data.assign(time_dx = data.time * 10)\n",
    "\n",
    "# time_idx = 0\n",
    "# for year in years:\n",
    "#     data['time_idx'] = data['time_idx'].replace([year], time_idx)\n",
    "#     time_idx = time_idx + 1\n",
    "    \n",
    "# data['years'] = data['years'].astype(str)\n",
    "# data['county'] = data['county'].astype(str)\n",
    "# data['time'] = data['time'].astype(str)\n",
    "\n",
    "# dff = data[ data['years'] == '2018' ]\n",
    "# dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "F-uXD_srqwLW",
    "outputId": "8a8a0424-2e67-4dd2-be62-b6b0bf46bfc3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>years</th>\n",
       "      <th>yield</th>\n",
       "      <th>sownareas</th>\n",
       "      <th>yieldvals</th>\n",
       "      <th>county</th>\n",
       "      <th>bands</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>bin0</th>\n",
       "      <th>bin1</th>\n",
       "      <th>...</th>\n",
       "      <th>bin502</th>\n",
       "      <th>bin503</th>\n",
       "      <th>bin504</th>\n",
       "      <th>bin505</th>\n",
       "      <th>bin506</th>\n",
       "      <th>bin507</th>\n",
       "      <th>bin508</th>\n",
       "      <th>bin509</th>\n",
       "      <th>bin510</th>\n",
       "      <th>bin511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>band_0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>band_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>band_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>band_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>band_4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 520 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 years    yield  sownareas  yieldvals county   bands  time_idx  \\\n",
       "0           0  2003  0.61295      75.21       46.1      0  band_0         0   \n",
       "1           1  2003  0.61295      75.21       46.1      0  band_1         0   \n",
       "2           2  2003  0.61295      75.21       46.1      0  band_2         0   \n",
       "3           3  2003  0.61295      75.21       46.1      0  band_3         0   \n",
       "4           4  2003  0.61295      75.21       46.1      0  band_4         0   \n",
       "\n",
       "   bin0  bin1  ...  bin502  bin503  bin504  bin505  bin506  bin507  bin508  \\\n",
       "0   0.0   0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1   0.0   0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2   0.0   0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3   0.0   0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4   0.0   0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   bin509  bin510  bin511  \n",
       "0     0.0     0.0     0.0  \n",
       "1     0.0     0.0     0.0  \n",
       "2     0.0     0.0     0.0  \n",
       "3     0.0     0.0     0.0  \n",
       "4     0.0     0.0     0.0  \n",
       "\n",
       "[5 rows x 520 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['years'] = data['years'].astype(str)\n",
    "data['county'] = data['county'].astype(str)\n",
    "data['time_idx'] = data['time_idx'].astype(int)\n",
    "data.head()\n",
    "# print(type(data['bin500'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "LL4gooLRnkdv",
    "outputId": "cb1d012b-c336-4683-d2d9-f454d61c1cb1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>yield</th>\n",
       "      <th>sownareas</th>\n",
       "      <th>yieldvals</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>bin0</th>\n",
       "      <th>bin1</th>\n",
       "      <th>bin2</th>\n",
       "      <th>bin3</th>\n",
       "      <th>bin4</th>\n",
       "      <th>...</th>\n",
       "      <th>bin502</th>\n",
       "      <th>bin503</th>\n",
       "      <th>bin504</th>\n",
       "      <th>bin505</th>\n",
       "      <th>bin506</th>\n",
       "      <th>bin507</th>\n",
       "      <th>bin508</th>\n",
       "      <th>bin509</th>\n",
       "      <th>bin510</th>\n",
       "      <th>bin511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4175.500000</td>\n",
       "      <td>0.517953</td>\n",
       "      <td>1054.189060</td>\n",
       "      <td>564.198213</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2411.027829</td>\n",
       "      <td>0.106652</td>\n",
       "      <td>1126.086187</td>\n",
       "      <td>659.807771</td>\n",
       "      <td>9.233143</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.003375</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276526</td>\n",
       "      <td>1.810000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2087.750000</td>\n",
       "      <td>0.438383</td>\n",
       "      <td>192.340000</td>\n",
       "      <td>116.940000</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4175.500000</td>\n",
       "      <td>0.518895</td>\n",
       "      <td>577.010000</td>\n",
       "      <td>263.520000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6263.250000</td>\n",
       "      <td>0.574336</td>\n",
       "      <td>1559.340000</td>\n",
       "      <td>736.100000</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8351.000000</td>\n",
       "      <td>0.855448</td>\n",
       "      <td>4210.460000</td>\n",
       "      <td>2662.150000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.025479</td>\n",
       "      <td>0.027569</td>\n",
       "      <td>0.024157</td>\n",
       "      <td>0.028934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026096</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.024242</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.021858</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.032663</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 517 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0         yield     sownareas     yieldvals      time_idx  \\\n",
       "count  91872.000000  91872.000000  91872.000000  91872.000000  91872.000000   \n",
       "mean    4175.500000      0.517953   1054.189060    564.198213     15.500000   \n",
       "std     2411.027829      0.106652   1126.086187    659.807771      9.233143   \n",
       "min        0.000000      0.276526      1.810000      1.250000      0.000000   \n",
       "25%     2087.750000      0.438383    192.340000    116.940000      7.750000   \n",
       "50%     4175.500000      0.518895    577.010000    263.520000     15.500000   \n",
       "75%     6263.250000      0.574336   1559.340000    736.100000     23.250000   \n",
       "max     8351.000000      0.855448   4210.460000   2662.150000     31.000000   \n",
       "\n",
       "               bin0          bin1          bin2          bin3          bin4  \\\n",
       "count  91872.000000  91872.000000  91872.000000  91872.000000  91872.000000   \n",
       "mean       0.000733      0.000152      0.000182      0.000171      0.000206   \n",
       "std        0.003606      0.000685      0.000838      0.000762      0.000930   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000059      0.000020      0.000023      0.000023      0.000026   \n",
       "max        0.111111      0.025479      0.027569      0.024157      0.028934   \n",
       "\n",
       "       ...        bin502        bin503        bin504        bin505  \\\n",
       "count  ...  91872.000000  91872.000000  91872.000000  91872.000000   \n",
       "mean   ...      0.000247      0.000256      0.000257      0.000240   \n",
       "std    ...      0.000711      0.000745      0.000783      0.000701   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      0.000148      0.000156      0.000151      0.000142   \n",
       "max    ...      0.026096      0.040541      0.054054      0.024242   \n",
       "\n",
       "             bin506        bin507        bin508        bin509        bin510  \\\n",
       "count  91872.000000  91872.000000  91872.000000  91872.000000  91872.000000   \n",
       "mean       0.000252      0.000254      0.000228      0.000244      0.000231   \n",
       "std        0.003375      0.001853      0.000673      0.000899      0.000685   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000141      0.000144      0.000127      0.000135      0.000129   \n",
       "max        1.000000      0.500000      0.021858      0.157895      0.032663   \n",
       "\n",
       "             bin511  \n",
       "count  91872.000000  \n",
       "mean       0.000254  \n",
       "std        0.000792  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000141  \n",
       "max        0.066667  \n",
       "\n",
       "[8 rows x 517 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class FullyConnectedModule(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_hidden_layers: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # input layer\n",
    "        module_list = [nn.Linear(input_size, hidden_size), nn.ReLU()]\n",
    "        # hidden layers\n",
    "        for _ in range(n_hidden_layers):\n",
    "            module_list.extend([nn.Linear(hidden_size, hidden_size), nn.ReLU()])\n",
    "        # output layer\n",
    "        module_list.append(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "        self.sequential = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x of shape: batch_size x n_timesteps_in\n",
    "        # output of shape batch_size x n_timesteps_out\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "# test that network works as intended\n",
    "network = FullyConnectedModule(input_size=5, output_size=2, hidden_size=10, n_hidden_layers=2)\n",
    "x = torch.rand(20, 5)\n",
    "network(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "from pytorch_forecasting.models.nn import MultiEmbedding\n",
    "from pytorch_forecasting.models import BaseModelWithCovariates\n",
    "\n",
    "class FullyConnectedModelWithCovariates(BaseModelWithCovariates):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        hidden_size: int,\n",
    "        n_hidden_layers: int,\n",
    "        x_reals: List[str],\n",
    "        x_categoricals: List[str],\n",
    "        embedding_sizes: Dict[str, Tuple[int, int]],\n",
    "        embedding_labels: Dict[str, List[str]],\n",
    "        static_categoricals: List[str],\n",
    "        static_reals: List[str],\n",
    "        time_varying_categoricals_encoder: List[str],\n",
    "        time_varying_categoricals_decoder: List[str],\n",
    "        time_varying_reals_encoder: List[str],\n",
    "        time_varying_reals_decoder: List[str],\n",
    "        embedding_paddings: List[str],\n",
    "        categorical_groups: Dict[str, List[str]],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n",
    "        self.save_hyperparameters()\n",
    "        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # create embedder - can be fed with x[\"encoder_cat\"] or x[\"decoder_cat\"] and will return\n",
    "        # dictionary of category names mapped to embeddings\n",
    "        self.input_embeddings = MultiEmbedding(\n",
    "            embedding_sizes=self.hparams.embedding_sizes,\n",
    "            categorical_groups=self.hparams.categorical_groups,\n",
    "            embedding_paddings=self.hparams.embedding_paddings,\n",
    "            x_categoricals=self.hparams.x_categoricals,\n",
    "            max_embedding_size=self.hparams.hidden_size,\n",
    "        )\n",
    "        \n",
    "        # calculate the size of all concatenated embeddings + continous variables\n",
    "        n_features = sum(\n",
    "            embedding_size for classes_size, embedding_size in self.hparams.embedding_sizes.values()\n",
    "        ) + len(self.reals)\n",
    "\n",
    "        # create network that will be fed with continious variables and embeddings\n",
    "        self.network = FullyConnectedModule(\n",
    "            input_size=self.hparams.input_size * n_features,\n",
    "            output_size=self.hparams.output_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            n_hidden_layers=self.hparams.n_hidden_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # x is a batch generated based on the TimeSeriesDataset\n",
    "        batch_size = x[\"encoder_lengths\"].size(0)\n",
    "        embeddings = self.input_embeddings(x[\"encoder_cat\"])  # returns dictionary with embedding tensors\n",
    "        network_input = torch.cat(\n",
    "            [x[\"encoder_cont\"]]\n",
    "            + [\n",
    "                emb\n",
    "                for name, emb in embeddings.items()\n",
    "                if name in self.encoder_variables or name in self.static_variables\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        prediction = self.network(network_input.view(batch_size, -1))\n",
    "\n",
    "        # rescale predictions into target space\n",
    "        prediction = self.transform_output(prediction, target_scale=x[\"target_scale\"])\n",
    "\n",
    "        # We need to return a dictionary that at least contains the prediction.\n",
    "        # The parameter can be directly forwarded from the input.\n",
    "        # The conversion to a named tuple can be directly achieved with the `to_network_output` function.\n",
    "        return self.to_network_output(prediction=prediction)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataset(cls, dataset: TimeSeriesDataSet, **kwargs):\n",
    "        new_kwargs = {\n",
    "            \"output_size\": dataset.max_prediction_length,\n",
    "            \"input_size\": dataset.max_encoder_length,\n",
    "        }\n",
    "        new_kwargs.update(kwargs)  # use to pass real hyperparameters and override defaults set by dataset\n",
    "        # example for dataset validation\n",
    "        assert dataset.max_prediction_length == dataset.min_prediction_length, \"Decoder only supports a fixed length\"\n",
    "        assert dataset.min_encoder_length == dataset.max_encoder_length, \"Encoder only supports a fixed length\"\n",
    "\n",
    "        return super().from_dataset(dataset, **new_kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"categorical_groups\":                {}\n",
       "\"embedding_labels\":                  {'county': {'0': 0, '1': 1, '11': 2, '12': 3, '13': 4, '14': 5, '15': 6, '16': 7, '17': 8, '19': 9, '2': 10, '21': 11, '23': 12, '24': 13, '26': 14, '29': 15, '3': 16, '5': 17, '8': 18, '9': 19}, 'bands': {'band_0': 0, 'band_1': 1, 'band_2': 2, 'band_3': 3, 'band_4': 4, 'band_5': 5, 'band_6': 6, 'band_7': 7, 'band_8': 8}}\n",
       "\"embedding_paddings\":                []\n",
       "\"embedding_sizes\":                   {'county': (20, 9), 'bands': (9, 5)}\n",
       "\"hidden_size\":                       10\n",
       "\"input_size\":                        16\n",
       "\"learning_rate\":                     0.001\n",
       "\"log_gradient_flow\":                 False\n",
       "\"log_interval\":                      -1\n",
       "\"log_val_interval\":                  -1\n",
       "\"logging_metrics\":                   ModuleList()\n",
       "\"loss\":                              SMAPE()\n",
       "\"monotone_constaints\":               {}\n",
       "\"n_hidden_layers\":                   2\n",
       "\"optimizer\":                         ranger\n",
       "\"optimizer_params\":                  None\n",
       "\"output_size\":                       1\n",
       "\"output_transformer\":                GroupNormalizer(\n",
       "\tmethod='standard',\n",
       "\tgroups=[],\n",
       "\tcenter=True,\n",
       "\tscale_by_group=False,\n",
       "\ttransformation='relu',\n",
       "\tmethod_kwargs={}\n",
       ")\n",
       "\"reduce_on_plateau_min_lr\":          1e-05\n",
       "\"reduce_on_plateau_patience\":        1000\n",
       "\"reduce_on_plateau_reduction\":       2.0\n",
       "\"static_categoricals\":               []\n",
       "\"static_reals\":                      []\n",
       "\"time_varying_categoricals_decoder\": []\n",
       "\"time_varying_categoricals_encoder\": ['county', 'bands']\n",
       "\"time_varying_reals_decoder\":        []\n",
       "\"time_varying_reals_encoder\":        ['bin0', 'bin1', 'bin2', 'bin3', 'bin4', 'bin5', 'bin6', 'bin7', 'bin8', 'bin9', 'bin10', 'bin11', 'bin12', 'bin13', 'bin14', 'bin15', 'bin16', 'bin17', 'bin18', 'bin19', 'bin20', 'bin21', 'bin22', 'bin23', 'bin24', 'bin25', 'bin26', 'bin27', 'bin28', 'bin29', 'bin30', 'bin31', 'bin32', 'bin33', 'bin34', 'bin35', 'bin36', 'bin37', 'bin38', 'bin39', 'bin40', 'bin41', 'bin42', 'bin43', 'bin44', 'bin45', 'bin46', 'bin47', 'bin48', 'bin49', 'bin50', 'bin51', 'bin52', 'bin53', 'bin54', 'bin55', 'bin56', 'bin57', 'bin58', 'bin59', 'bin60', 'bin61', 'bin62', 'bin63', 'bin64', 'bin65', 'bin66', 'bin67', 'bin68', 'bin69', 'bin70', 'bin71', 'bin72', 'bin73', 'bin74', 'bin75', 'bin76', 'bin77', 'bin78', 'bin79', 'bin80', 'bin81', 'bin82', 'bin83', 'bin84', 'bin85', 'bin86', 'bin87', 'bin88', 'bin89', 'bin90', 'bin91', 'bin92', 'bin93', 'bin94', 'bin95', 'bin96', 'bin97', 'bin98', 'bin99', 'bin100', 'bin101', 'bin102', 'bin103', 'bin104', 'bin105', 'bin106', 'bin107', 'bin108', 'bin109', 'bin110', 'bin111', 'bin112', 'bin113', 'bin114', 'bin115', 'bin116', 'bin117', 'bin118', 'bin119', 'bin120', 'bin121', 'bin122', 'bin123', 'bin124', 'bin125', 'bin126', 'bin127', 'bin128', 'bin129', 'bin130', 'bin131', 'bin132', 'bin133', 'bin134', 'bin135', 'bin136', 'bin137', 'bin138', 'bin139', 'bin140', 'bin141', 'bin142', 'bin143', 'bin144', 'bin145', 'bin146', 'bin147', 'bin148', 'bin149', 'bin150', 'bin151', 'bin152', 'bin153', 'bin154', 'bin155', 'bin156', 'bin157', 'bin158', 'bin159', 'bin160', 'bin161', 'bin162', 'bin163', 'bin164', 'bin165', 'bin166', 'bin167', 'bin168', 'bin169', 'bin170', 'bin171', 'bin172', 'bin173', 'bin174', 'bin175', 'bin176', 'bin177', 'bin178', 'bin179', 'bin180', 'bin181', 'bin182', 'bin183', 'bin184', 'bin185', 'bin186', 'bin187', 'bin188', 'bin189', 'bin190', 'bin191', 'bin192', 'bin193', 'bin194', 'bin195', 'bin196', 'bin197', 'bin198', 'bin199', 'bin200', 'bin201', 'bin202', 'bin203', 'bin204', 'bin205', 'bin206', 'bin207', 'bin208', 'bin209', 'bin210', 'bin211', 'bin212', 'bin213', 'bin214', 'bin215', 'bin216', 'bin217', 'bin218', 'bin219', 'bin220', 'bin221', 'bin222', 'bin223', 'bin224', 'bin225', 'bin226', 'bin227', 'bin228', 'bin229', 'bin230', 'bin231', 'bin232', 'bin233', 'bin234', 'bin235', 'bin236', 'bin237', 'bin238', 'bin239', 'bin240', 'bin241', 'bin242', 'bin243', 'bin244', 'bin245', 'bin246', 'bin247', 'bin248', 'bin249', 'bin250', 'bin251', 'bin252', 'bin253', 'bin254', 'bin255', 'bin256', 'bin257', 'bin258', 'bin259', 'bin260', 'bin261', 'bin262', 'bin263', 'bin264', 'bin265', 'bin266', 'bin267', 'bin268', 'bin269', 'bin270', 'bin271', 'bin272', 'bin273', 'bin274', 'bin275', 'bin276', 'bin277', 'bin278', 'bin279', 'bin280', 'bin281', 'bin282', 'bin283', 'bin284', 'bin285', 'bin286', 'bin287', 'bin288', 'bin289', 'bin290', 'bin291', 'bin292', 'bin293', 'bin294', 'bin295', 'bin296', 'bin297', 'bin298', 'bin299', 'bin300', 'bin301', 'bin302', 'bin303', 'bin304', 'bin305', 'bin306', 'bin307', 'bin308', 'bin309', 'bin310', 'bin311', 'bin312', 'bin313', 'bin314', 'bin315', 'bin316', 'bin317', 'bin318', 'bin319', 'bin320', 'bin321', 'bin322', 'bin323', 'bin324', 'bin325', 'bin326', 'bin327', 'bin328', 'bin329', 'bin330', 'bin331', 'bin332', 'bin333', 'bin334', 'bin335', 'bin336', 'bin337', 'bin338', 'bin339', 'bin340', 'bin341', 'bin342', 'bin343', 'bin344', 'bin345', 'bin346', 'bin347', 'bin348', 'bin349', 'bin350', 'bin351', 'bin352', 'bin353', 'bin354', 'bin355', 'bin356', 'bin357', 'bin358', 'bin359', 'bin360', 'bin361', 'bin362', 'bin363', 'bin364', 'bin365', 'bin366', 'bin367', 'bin368', 'bin369', 'bin370', 'bin371', 'bin372', 'bin373', 'bin374', 'bin375', 'bin376', 'bin377', 'bin378', 'bin379', 'bin380', 'bin381', 'bin382', 'bin383', 'bin384', 'bin385', 'bin386', 'bin387', 'bin388', 'bin389', 'bin390', 'bin391', 'bin392', 'bin393', 'bin394', 'bin395', 'bin396', 'bin397', 'bin398', 'bin399', 'bin400', 'bin401', 'bin402', 'bin403', 'bin404', 'bin405', 'bin406', 'bin407', 'bin408', 'bin409', 'bin410', 'bin411', 'bin412', 'bin413', 'bin414', 'bin415', 'bin416', 'bin417', 'bin418', 'bin419', 'bin420', 'bin421', 'bin422', 'bin423', 'bin424', 'bin425', 'bin426', 'bin427', 'bin428', 'bin429', 'bin430', 'bin431', 'bin432', 'bin433', 'bin434', 'bin435', 'bin436', 'bin437', 'bin438', 'bin439', 'bin440', 'bin441', 'bin442', 'bin443', 'bin444', 'bin445', 'bin446', 'bin447', 'bin448', 'bin449', 'bin450', 'bin451', 'bin452', 'bin453', 'bin454', 'bin455', 'bin456', 'bin457', 'bin458', 'bin459', 'bin460', 'bin461', 'bin462', 'bin463', 'bin464', 'bin465', 'bin466', 'bin467', 'bin468', 'bin469', 'bin470', 'bin471', 'bin472', 'bin473', 'bin474', 'bin475', 'bin476', 'bin477', 'bin478', 'bin479', 'bin480', 'bin481', 'bin482', 'bin483', 'bin484', 'bin485', 'bin486', 'bin487', 'bin488', 'bin489', 'bin490', 'bin491', 'bin492', 'bin493', 'bin494', 'bin495', 'bin496', 'bin497', 'bin498', 'bin499', 'bin500', 'bin501', 'bin502', 'bin503', 'bin504', 'bin505', 'bin506', 'bin507', 'bin508', 'bin509', 'bin510', 'bin511']\n",
       "\"weight_decay\":                      0.0\n",
       "\"x_categoricals\":                    ['county', 'bands']\n",
       "\"x_reals\":                           ['bin0', 'bin1', 'bin2', 'bin3', 'bin4', 'bin5', 'bin6', 'bin7', 'bin8', 'bin9', 'bin10', 'bin11', 'bin12', 'bin13', 'bin14', 'bin15', 'bin16', 'bin17', 'bin18', 'bin19', 'bin20', 'bin21', 'bin22', 'bin23', 'bin24', 'bin25', 'bin26', 'bin27', 'bin28', 'bin29', 'bin30', 'bin31', 'bin32', 'bin33', 'bin34', 'bin35', 'bin36', 'bin37', 'bin38', 'bin39', 'bin40', 'bin41', 'bin42', 'bin43', 'bin44', 'bin45', 'bin46', 'bin47', 'bin48', 'bin49', 'bin50', 'bin51', 'bin52', 'bin53', 'bin54', 'bin55', 'bin56', 'bin57', 'bin58', 'bin59', 'bin60', 'bin61', 'bin62', 'bin63', 'bin64', 'bin65', 'bin66', 'bin67', 'bin68', 'bin69', 'bin70', 'bin71', 'bin72', 'bin73', 'bin74', 'bin75', 'bin76', 'bin77', 'bin78', 'bin79', 'bin80', 'bin81', 'bin82', 'bin83', 'bin84', 'bin85', 'bin86', 'bin87', 'bin88', 'bin89', 'bin90', 'bin91', 'bin92', 'bin93', 'bin94', 'bin95', 'bin96', 'bin97', 'bin98', 'bin99', 'bin100', 'bin101', 'bin102', 'bin103', 'bin104', 'bin105', 'bin106', 'bin107', 'bin108', 'bin109', 'bin110', 'bin111', 'bin112', 'bin113', 'bin114', 'bin115', 'bin116', 'bin117', 'bin118', 'bin119', 'bin120', 'bin121', 'bin122', 'bin123', 'bin124', 'bin125', 'bin126', 'bin127', 'bin128', 'bin129', 'bin130', 'bin131', 'bin132', 'bin133', 'bin134', 'bin135', 'bin136', 'bin137', 'bin138', 'bin139', 'bin140', 'bin141', 'bin142', 'bin143', 'bin144', 'bin145', 'bin146', 'bin147', 'bin148', 'bin149', 'bin150', 'bin151', 'bin152', 'bin153', 'bin154', 'bin155', 'bin156', 'bin157', 'bin158', 'bin159', 'bin160', 'bin161', 'bin162', 'bin163', 'bin164', 'bin165', 'bin166', 'bin167', 'bin168', 'bin169', 'bin170', 'bin171', 'bin172', 'bin173', 'bin174', 'bin175', 'bin176', 'bin177', 'bin178', 'bin179', 'bin180', 'bin181', 'bin182', 'bin183', 'bin184', 'bin185', 'bin186', 'bin187', 'bin188', 'bin189', 'bin190', 'bin191', 'bin192', 'bin193', 'bin194', 'bin195', 'bin196', 'bin197', 'bin198', 'bin199', 'bin200', 'bin201', 'bin202', 'bin203', 'bin204', 'bin205', 'bin206', 'bin207', 'bin208', 'bin209', 'bin210', 'bin211', 'bin212', 'bin213', 'bin214', 'bin215', 'bin216', 'bin217', 'bin218', 'bin219', 'bin220', 'bin221', 'bin222', 'bin223', 'bin224', 'bin225', 'bin226', 'bin227', 'bin228', 'bin229', 'bin230', 'bin231', 'bin232', 'bin233', 'bin234', 'bin235', 'bin236', 'bin237', 'bin238', 'bin239', 'bin240', 'bin241', 'bin242', 'bin243', 'bin244', 'bin245', 'bin246', 'bin247', 'bin248', 'bin249', 'bin250', 'bin251', 'bin252', 'bin253', 'bin254', 'bin255', 'bin256', 'bin257', 'bin258', 'bin259', 'bin260', 'bin261', 'bin262', 'bin263', 'bin264', 'bin265', 'bin266', 'bin267', 'bin268', 'bin269', 'bin270', 'bin271', 'bin272', 'bin273', 'bin274', 'bin275', 'bin276', 'bin277', 'bin278', 'bin279', 'bin280', 'bin281', 'bin282', 'bin283', 'bin284', 'bin285', 'bin286', 'bin287', 'bin288', 'bin289', 'bin290', 'bin291', 'bin292', 'bin293', 'bin294', 'bin295', 'bin296', 'bin297', 'bin298', 'bin299', 'bin300', 'bin301', 'bin302', 'bin303', 'bin304', 'bin305', 'bin306', 'bin307', 'bin308', 'bin309', 'bin310', 'bin311', 'bin312', 'bin313', 'bin314', 'bin315', 'bin316', 'bin317', 'bin318', 'bin319', 'bin320', 'bin321', 'bin322', 'bin323', 'bin324', 'bin325', 'bin326', 'bin327', 'bin328', 'bin329', 'bin330', 'bin331', 'bin332', 'bin333', 'bin334', 'bin335', 'bin336', 'bin337', 'bin338', 'bin339', 'bin340', 'bin341', 'bin342', 'bin343', 'bin344', 'bin345', 'bin346', 'bin347', 'bin348', 'bin349', 'bin350', 'bin351', 'bin352', 'bin353', 'bin354', 'bin355', 'bin356', 'bin357', 'bin358', 'bin359', 'bin360', 'bin361', 'bin362', 'bin363', 'bin364', 'bin365', 'bin366', 'bin367', 'bin368', 'bin369', 'bin370', 'bin371', 'bin372', 'bin373', 'bin374', 'bin375', 'bin376', 'bin377', 'bin378', 'bin379', 'bin380', 'bin381', 'bin382', 'bin383', 'bin384', 'bin385', 'bin386', 'bin387', 'bin388', 'bin389', 'bin390', 'bin391', 'bin392', 'bin393', 'bin394', 'bin395', 'bin396', 'bin397', 'bin398', 'bin399', 'bin400', 'bin401', 'bin402', 'bin403', 'bin404', 'bin405', 'bin406', 'bin407', 'bin408', 'bin409', 'bin410', 'bin411', 'bin412', 'bin413', 'bin414', 'bin415', 'bin416', 'bin417', 'bin418', 'bin419', 'bin420', 'bin421', 'bin422', 'bin423', 'bin424', 'bin425', 'bin426', 'bin427', 'bin428', 'bin429', 'bin430', 'bin431', 'bin432', 'bin433', 'bin434', 'bin435', 'bin436', 'bin437', 'bin438', 'bin439', 'bin440', 'bin441', 'bin442', 'bin443', 'bin444', 'bin445', 'bin446', 'bin447', 'bin448', 'bin449', 'bin450', 'bin451', 'bin452', 'bin453', 'bin454', 'bin455', 'bin456', 'bin457', 'bin458', 'bin459', 'bin460', 'bin461', 'bin462', 'bin463', 'bin464', 'bin465', 'bin466', 'bin467', 'bin468', 'bin469', 'bin470', 'bin471', 'bin472', 'bin473', 'bin474', 'bin475', 'bin476', 'bin477', 'bin478', 'bin479', 'bin480', 'bin481', 'bin482', 'bin483', 'bin484', 'bin485', 'bin486', 'bin487', 'bin488', 'bin489', 'bin490', 'bin491', 'bin492', 'bin493', 'bin494', 'bin495', 'bin496', 'bin497', 'bin498', 'bin499', 'bin500', 'bin501', 'bin502', 'bin503', 'bin504', 'bin505', 'bin506', 'bin507', 'bin508', 'bin509', 'bin510', 'bin511']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.utilities.model_summary import summarize\n",
    "# create the dataset from the pandas dataframe\n",
    "train_data = data[ data[\"years\"] != \"2018\" ]\n",
    "valid_data = data[ data[\"years\"] == \"2018\" ]\n",
    "\n",
    "bins_name = list()   #list([\"yield\"])\n",
    "for bin in range(0, 512):\n",
    "    bins_name.append(f'bin{bin}')\n",
    "\n",
    "# print(bins_name)\n",
    "\n",
    "train_dataset_with_covariates = TimeSeriesDataSet(\n",
    "    train_data,\n",
    "    group_ids=[\"years\", \"county\", \"bands\"],\n",
    "    target=\"yield\",\n",
    "    time_idx=\"time_idx\",\n",
    "    min_encoder_length=16,\n",
    "    max_encoder_length=16,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=1,\n",
    "    time_varying_unknown_reals=bins_name,  #[\"yield\"],\n",
    "    time_varying_unknown_categoricals=[\"county\", \"bands\"],\n",
    "    # time_varying_known_reals=[\"real_covariate\"],\n",
    "    # time_varying_known_categoricals=[\"years\", \"county\"],\n",
    "    # static_categoricals=[\"years\", \"county\"],\n",
    ")\n",
    "\n",
    "valid_dataset_with_covariates = TimeSeriesDataSet(\n",
    "    valid_data,\n",
    "    group_ids=[\"years\", \"county\", \"bands\"],\n",
    "    target=\"yield\",\n",
    "    time_idx=\"time_idx\",\n",
    "    min_encoder_length=16,\n",
    "    max_encoder_length=16,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=1,\n",
    "    time_varying_unknown_reals=bins_name,   #[\"yield\"],\n",
    "    time_varying_unknown_categoricals=[\"county\", \"bands\"],\n",
    "    # time_varying_known_reals=[\"real_covariate\"],\n",
    "    # time_varying_known_categoricals=[\"years\", \"county\"],\n",
    "    # static_categoricals=[\"years\", \"county\"],\n",
    ")\n",
    "\n",
    "model = FullyConnectedModelWithCovariates.from_dataset(train_dataset_with_covariates, hidden_size=10, n_hidden_layers=2)\n",
    "summarize(model,max_depth=-1)  # print model summary\n",
    "model.hparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.metrics import MAE\n",
    "\n",
    "# model = FullyConnectedModelWithCovariates.from_dataset(train_dataset_with_covariates, \n",
    "#                                                        hidden_size=50, n_hidden_layers=10, loss=MAE())\n",
    "\n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 32\n",
    "train_dataloader = train_dataset_with_covariates.to_dataloader(train=True,  batch_size=batch_size, num_workers=2)\n",
    "valid_dataloader = valid_dataset_with_covariates.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tencoder_cat = torch.Size([32, 16, 2])\n",
      "\tencoder_cont = torch.Size([32, 16, 512])\n",
      "\tencoder_target = torch.Size([32, 16])\n",
      "\tencoder_lengths = torch.Size([32])\n",
      "\tdecoder_cat = torch.Size([32, 1, 2])\n",
      "\tdecoder_cont = torch.Size([32, 1, 512])\n",
      "\tdecoder_target = torch.Size([32, 1])\n",
      "\tdecoder_lengths = torch.Size([32])\n",
      "\tdecoder_time_idx = torch.Size([32, 1])\n",
      "\tgroups = torch.Size([32, 3])\n",
      "\ttarget_scale = torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "# convert the dataset to a dataloader\n",
    "# dataloader = dataset.to_dataloader(batch_size=4)\n",
    "\n",
    "# and load the first batch\n",
    "x, y = next(iter(valid_dataloader))\n",
    "# print(\"x =\", x)\n",
    "# print(\"\\ny =\", y)\n",
    "# print(\"\\nsizes of x =\")\n",
    "for key, value in x.items():\n",
    "    print(f\"\\t{key} = {value.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /tf_logs/FCNN_3: batch_size=32 dataset experiments\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                 | Params\n",
      "----------------------------------------------------------\n",
      "0 | loss             | SMAPE                | 0     \n",
      "1 | logging_metrics  | ModuleList           | 0     \n",
      "2 | input_embeddings | MultiEmbedding       | 225   \n",
      "3 | network          | FullyConnectedModule | 84.4 K\n",
      "----------------------------------------------------------\n",
      "84.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "84.6 K    Total params\n",
      "0.339     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  94%|█████████▍| 1350/1436 [02:26<00:09,  9.24it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 1351/1436 [02:26<00:09,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  94%|█████████▍| 1352/1436 [02:26<00:09,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  94%|█████████▍| 1353/1436 [02:26<00:09,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  94%|█████████▍| 1354/1436 [02:26<00:08,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  94%|█████████▍| 1355/1436 [02:27<00:08,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  94%|█████████▍| 1356/1436 [02:27<00:08,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  94%|█████████▍| 1357/1436 [02:27<00:08,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  95%|█████████▍| 1358/1436 [02:27<00:08,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  95%|█████████▍| 1359/1436 [02:27<00:08,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  95%|█████████▍| 1360/1436 [02:27<00:08,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  95%|█████████▍| 1361/1436 [02:27<00:08,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  95%|█████████▍| 1362/1436 [02:27<00:08,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  95%|█████████▍| 1363/1436 [02:28<00:07,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  95%|█████████▍| 1364/1436 [02:28<00:07,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  95%|█████████▌| 1365/1436 [02:28<00:07,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  95%|█████████▌| 1366/1436 [02:28<00:07,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  95%|█████████▌| 1367/1436 [02:28<00:07,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  95%|█████████▌| 1368/1436 [02:28<00:07,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  95%|█████████▌| 1369/1436 [02:28<00:07,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  95%|█████████▌| 1370/1436 [02:28<00:07,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  95%|█████████▌| 1371/1436 [02:28<00:07,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  96%|█████████▌| 1372/1436 [02:28<00:06,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  96%|█████████▌| 1373/1436 [02:29<00:06,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  96%|█████████▌| 1374/1436 [02:29<00:06,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  96%|█████████▌| 1375/1436 [02:29<00:06,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  96%|█████████▌| 1376/1436 [02:29<00:06,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  96%|█████████▌| 1377/1436 [02:29<00:06,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  96%|█████████▌| 1378/1436 [02:29<00:06,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  96%|█████████▌| 1379/1436 [02:29<00:06,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  96%|█████████▌| 1380/1436 [02:29<00:06,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  96%|█████████▌| 1381/1436 [02:30<00:05,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  96%|█████████▌| 1382/1436 [02:30<00:05,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  96%|█████████▋| 1383/1436 [02:30<00:05,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  96%|█████████▋| 1384/1436 [02:30<00:05,  9.20it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  96%|█████████▋| 1385/1436 [02:30<00:05,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  97%|█████████▋| 1386/1436 [02:30<00:05,  9.20it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  97%|█████████▋| 1387/1436 [02:30<00:05,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  97%|█████████▋| 1388/1436 [02:30<00:05,  9.20it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  97%|█████████▋| 1389/1436 [02:30<00:05,  9.21it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  97%|█████████▋| 1390/1436 [02:31<00:04,  9.20it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  97%|█████████▋| 1391/1436 [02:31<00:04,  9.20it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  97%|█████████▋| 1392/1436 [02:31<00:04,  9.20it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  97%|█████████▋| 1393/1436 [02:31<00:04,  9.20it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  97%|█████████▋| 1394/1436 [02:31<00:04,  9.20it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  97%|█████████▋| 1395/1436 [02:31<00:04,  9.20it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  97%|█████████▋| 1396/1436 [02:31<00:04,  9.19it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  97%|█████████▋| 1397/1436 [02:31<00:04,  9.20it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  97%|█████████▋| 1398/1436 [02:32<00:04,  9.20it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  97%|█████████▋| 1399/1436 [02:32<00:04,  9.20it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  97%|█████████▋| 1400/1436 [02:32<00:03,  9.19it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  98%|█████████▊| 1401/1436 [02:32<00:03,  9.20it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  98%|█████████▊| 1402/1436 [02:32<00:03,  9.19it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  98%|█████████▊| 1403/1436 [02:32<00:03,  9.20it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  98%|█████████▊| 1404/1436 [02:32<00:03,  9.19it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  98%|█████████▊| 1405/1436 [02:32<00:03,  9.19it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  98%|█████████▊| 1406/1436 [02:32<00:03,  9.19it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  98%|█████████▊| 1407/1436 [02:33<00:03,  9.20it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  98%|█████████▊| 1408/1436 [02:33<00:03,  9.19it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  98%|█████████▊| 1409/1436 [02:33<00:02,  9.19it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  98%|█████████▊| 1410/1436 [02:33<00:02,  9.18it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  98%|█████████▊| 1411/1436 [02:33<00:02,  9.19it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  98%|█████████▊| 1412/1436 [02:33<00:02,  9.18it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  98%|█████████▊| 1413/1436 [02:33<00:02,  9.19it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  98%|█████████▊| 1414/1436 [02:34<00:02,  9.18it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  99%|█████████▊| 1415/1436 [02:34<00:02,  9.19it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  99%|█████████▊| 1416/1436 [02:34<00:02,  9.18it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  99%|█████████▊| 1417/1436 [02:34<00:02,  9.19it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  99%|█████████▊| 1418/1436 [02:34<00:01,  9.18it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  99%|█████████▉| 1419/1436 [02:34<00:01,  9.18it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  99%|█████████▉| 1420/1436 [02:34<00:01,  9.18it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  99%|█████████▉| 1421/1436 [02:34<00:01,  9.18it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  99%|█████████▉| 1422/1436 [02:35<00:01,  9.17it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  99%|█████████▉| 1423/1436 [02:35<00:01,  9.18it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  99%|█████████▉| 1424/1436 [02:35<00:01,  9.17it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  99%|█████████▉| 1425/1436 [02:35<00:01,  9.18it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  99%|█████████▉| 1426/1436 [02:35<00:01,  9.18it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  99%|█████████▉| 1427/1436 [02:35<00:00,  9.18it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0:  99%|█████████▉| 1428/1436 [02:35<00:00,  9.17it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0: 100%|█████████▉| 1429/1436 [02:35<00:00,  9.18it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0: 100%|█████████▉| 1430/1436 [02:35<00:00,  9.17it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0: 100%|█████████▉| 1431/1436 [02:35<00:00,  9.17it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0: 100%|█████████▉| 1432/1436 [02:36<00:00,  9.17it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0: 100%|█████████▉| 1433/1436 [02:36<00:00,  9.17it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0: 100%|█████████▉| 1434/1436 [02:36<00:00,  9.17it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0: 100%|█████████▉| 1435/1436 [02:36<00:00,  9.18it/s, loss=0.0841, v_num=0, train_loss_step=0.0574]\n",
      "Epoch 0: 100%|██████████| 1436/1436 [02:36<00:00,  9.18it/s, loss=0.0841, v_num=0, train_loss_step=0.0574, val_loss=0.0933]\n",
      "Epoch 1:  94%|█████████▍| 1350/1436 [02:36<00:09,  8.62it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  94%|█████████▍| 1351/1436 [02:37<00:09,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  94%|█████████▍| 1352/1436 [02:37<00:09,  8.59it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  94%|█████████▍| 1353/1436 [02:37<00:09,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  94%|█████████▍| 1354/1436 [02:37<00:09,  8.59it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  94%|█████████▍| 1355/1436 [02:37<00:09,  8.59it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  94%|█████████▍| 1356/1436 [02:37<00:09,  8.59it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  94%|█████████▍| 1357/1436 [02:38<00:09,  8.59it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  95%|█████████▍| 1358/1436 [02:38<00:09,  8.59it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  95%|█████████▍| 1359/1436 [02:38<00:08,  8.59it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  95%|█████████▍| 1360/1436 [02:38<00:08,  8.59it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  95%|█████████▍| 1361/1436 [02:38<00:08,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  95%|█████████▍| 1362/1436 [02:38<00:08,  8.59it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  95%|█████████▍| 1363/1436 [02:38<00:08,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  95%|█████████▍| 1364/1436 [02:38<00:08,  8.59it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  95%|█████████▌| 1365/1436 [02:39<00:08,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  95%|█████████▌| 1366/1436 [02:39<00:08,  8.59it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  95%|█████████▌| 1367/1436 [02:39<00:08,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  95%|█████████▌| 1368/1436 [02:39<00:07,  8.59it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  95%|█████████▌| 1369/1436 [02:39<00:07,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  95%|█████████▌| 1370/1436 [02:39<00:07,  8.59it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  95%|█████████▌| 1371/1436 [02:39<00:07,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  96%|█████████▌| 1372/1436 [02:39<00:07,  8.59it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  96%|█████████▌| 1373/1436 [02:40<00:07,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  96%|█████████▌| 1374/1436 [02:40<00:07,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  96%|█████████▌| 1375/1436 [02:40<00:07,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  96%|█████████▌| 1376/1436 [02:40<00:06,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  96%|█████████▌| 1377/1436 [02:40<00:06,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  96%|█████████▌| 1378/1436 [02:40<00:06,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  96%|█████████▌| 1379/1436 [02:40<00:06,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  96%|█████████▌| 1380/1436 [02:40<00:06,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  96%|█████████▌| 1381/1436 [02:40<00:06,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  96%|█████████▌| 1382/1436 [02:41<00:06,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  96%|█████████▋| 1383/1436 [02:41<00:06,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  96%|█████████▋| 1384/1436 [02:41<00:06,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  96%|█████████▋| 1385/1436 [02:41<00:05,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  97%|█████████▋| 1386/1436 [02:41<00:05,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  97%|█████████▋| 1387/1436 [02:41<00:05,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  97%|█████████▋| 1388/1436 [02:41<00:05,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  97%|█████████▋| 1389/1436 [02:41<00:05,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  97%|█████████▋| 1390/1436 [02:42<00:05,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  97%|█████████▋| 1391/1436 [02:42<00:05,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  97%|█████████▋| 1392/1436 [02:42<00:05,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  97%|█████████▋| 1393/1436 [02:42<00:05,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  97%|█████████▋| 1394/1436 [02:42<00:04,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  97%|█████████▋| 1395/1436 [02:42<00:04,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  97%|█████████▋| 1396/1436 [02:42<00:04,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  97%|█████████▋| 1397/1436 [02:42<00:04,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  97%|█████████▋| 1398/1436 [02:42<00:04,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  97%|█████████▋| 1399/1436 [02:43<00:04,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  97%|█████████▋| 1400/1436 [02:43<00:04,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  98%|█████████▊| 1401/1436 [02:43<00:04,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  98%|█████████▊| 1402/1436 [02:43<00:03,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  98%|█████████▊| 1403/1436 [02:43<00:03,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  98%|█████████▊| 1404/1436 [02:43<00:03,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  98%|█████████▊| 1405/1436 [02:43<00:03,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  98%|█████████▊| 1406/1436 [02:43<00:03,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  98%|█████████▊| 1407/1436 [02:44<00:03,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  98%|█████████▊| 1408/1436 [02:44<00:03,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  98%|█████████▊| 1409/1436 [02:44<00:03,  8.57it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  98%|█████████▊| 1410/1436 [02:44<00:03,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  98%|█████████▊| 1411/1436 [02:44<00:02,  8.57it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  98%|█████████▊| 1412/1436 [02:44<00:02,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  98%|█████████▊| 1413/1436 [02:44<00:02,  8.57it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  98%|█████████▊| 1414/1436 [02:44<00:02,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  99%|█████████▊| 1415/1436 [02:45<00:02,  8.57it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  99%|█████████▊| 1416/1436 [02:45<00:02,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  99%|█████████▊| 1417/1436 [02:45<00:02,  8.57it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  99%|█████████▊| 1418/1436 [02:45<00:02,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  99%|█████████▉| 1419/1436 [02:45<00:01,  8.57it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  99%|█████████▉| 1420/1436 [02:45<00:01,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  99%|█████████▉| 1421/1436 [02:45<00:01,  8.57it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  99%|█████████▉| 1422/1436 [02:45<00:01,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  99%|█████████▉| 1423/1436 [02:46<00:01,  8.57it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  99%|█████████▉| 1424/1436 [02:46<00:01,  8.57it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  99%|█████████▉| 1425/1436 [02:46<00:01,  8.57it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  99%|█████████▉| 1426/1436 [02:46<00:01,  8.57it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  99%|█████████▉| 1427/1436 [02:46<00:01,  8.57it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1:  99%|█████████▉| 1428/1436 [02:46<00:00,  8.57it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1: 100%|█████████▉| 1429/1436 [02:46<00:00,  8.57it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1: 100%|█████████▉| 1430/1436 [02:46<00:00,  8.57it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1: 100%|█████████▉| 1431/1436 [02:46<00:00,  8.57it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1: 100%|█████████▉| 1432/1436 [02:47<00:00,  8.57it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1: 100%|█████████▉| 1433/1436 [02:47<00:00,  8.57it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1: 100%|█████████▉| 1434/1436 [02:47<00:00,  8.57it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1: 100%|█████████▉| 1435/1436 [02:47<00:00,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0933, train_loss_epoch=0.110]\n",
      "Epoch 1: 100%|██████████| 1436/1436 [02:47<00:00,  8.58it/s, loss=0.0858, v_num=0, train_loss_step=0.0612, val_loss=0.0905, train_loss_epoch=0.110]\n",
      "Epoch 2:  94%|█████████▍| 1350/1436 [02:35<00:09,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  94%|█████████▍| 1351/1436 [02:35<00:09,  8.67it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  94%|█████████▍| 1352/1436 [02:35<00:09,  8.67it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  94%|█████████▍| 1353/1436 [02:36<00:09,  8.67it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  94%|█████████▍| 1354/1436 [02:36<00:09,  8.67it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  94%|█████████▍| 1355/1436 [02:36<00:09,  8.67it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  94%|█████████▍| 1356/1436 [02:36<00:09,  8.67it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  94%|█████████▍| 1357/1436 [02:36<00:09,  8.67it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  95%|█████████▍| 1358/1436 [02:36<00:08,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  95%|█████████▍| 1359/1436 [02:36<00:08,  8.67it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  95%|█████████▍| 1360/1436 [02:36<00:08,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  95%|█████████▍| 1361/1436 [02:36<00:08,  8.67it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  95%|█████████▍| 1362/1436 [02:36<00:08,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  95%|█████████▍| 1363/1436 [02:37<00:08,  8.67it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  95%|█████████▍| 1364/1436 [02:37<00:08,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  95%|█████████▌| 1365/1436 [02:37<00:08,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  95%|█████████▌| 1366/1436 [02:37<00:08,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  95%|█████████▌| 1367/1436 [02:37<00:07,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  95%|█████████▌| 1368/1436 [02:37<00:07,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  95%|█████████▌| 1369/1436 [02:37<00:07,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  95%|█████████▌| 1370/1436 [02:37<00:07,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  95%|█████████▌| 1371/1436 [02:37<00:07,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  96%|█████████▌| 1372/1436 [02:38<00:07,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  96%|█████████▌| 1373/1436 [02:38<00:07,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  96%|█████████▌| 1374/1436 [02:38<00:07,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  96%|█████████▌| 1375/1436 [02:38<00:07,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  96%|█████████▌| 1376/1436 [02:38<00:06,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  96%|█████████▌| 1377/1436 [02:38<00:06,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  96%|█████████▌| 1378/1436 [02:38<00:06,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  96%|█████████▌| 1379/1436 [02:38<00:06,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  96%|█████████▌| 1380/1436 [02:38<00:06,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  96%|█████████▌| 1381/1436 [02:39<00:06,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  96%|█████████▌| 1382/1436 [02:39<00:06,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  96%|█████████▋| 1383/1436 [02:39<00:06,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  96%|█████████▋| 1384/1436 [02:39<00:05,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  96%|█████████▋| 1385/1436 [02:39<00:05,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  97%|█████████▋| 1386/1436 [02:39<00:05,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  97%|█████████▋| 1387/1436 [02:39<00:05,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  97%|█████████▋| 1388/1436 [02:39<00:05,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  97%|█████████▋| 1389/1436 [02:39<00:05,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  97%|█████████▋| 1390/1436 [02:40<00:05,  8.68it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  97%|█████████▋| 1391/1436 [02:40<00:05,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  97%|█████████▋| 1392/1436 [02:40<00:05,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  97%|█████████▋| 1393/1436 [02:40<00:04,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  97%|█████████▋| 1394/1436 [02:40<00:04,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  97%|█████████▋| 1395/1436 [02:40<00:04,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  97%|█████████▋| 1396/1436 [02:40<00:04,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  97%|█████████▋| 1397/1436 [02:40<00:04,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  97%|█████████▋| 1398/1436 [02:40<00:04,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  97%|█████████▋| 1399/1436 [02:41<00:04,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  97%|█████████▋| 1400/1436 [02:41<00:04,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  98%|█████████▊| 1401/1436 [02:41<00:04,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  98%|█████████▊| 1402/1436 [02:41<00:03,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  98%|█████████▊| 1403/1436 [02:41<00:03,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  98%|█████████▊| 1404/1436 [02:41<00:03,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  98%|█████████▊| 1405/1436 [02:41<00:03,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  98%|█████████▊| 1406/1436 [02:41<00:03,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  98%|█████████▊| 1407/1436 [02:41<00:03,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  98%|█████████▊| 1408/1436 [02:42<00:03,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  98%|█████████▊| 1409/1436 [02:42<00:03,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  98%|█████████▊| 1410/1436 [02:42<00:02,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  98%|█████████▊| 1411/1436 [02:42<00:02,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  98%|█████████▊| 1412/1436 [02:42<00:02,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  98%|█████████▊| 1413/1436 [02:42<00:02,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  98%|█████████▊| 1414/1436 [02:42<00:02,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  99%|█████████▊| 1415/1436 [02:42<00:02,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  99%|█████████▊| 1416/1436 [02:42<00:02,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  99%|█████████▊| 1417/1436 [02:42<00:02,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  99%|█████████▊| 1418/1436 [02:43<00:02,  8.70it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  99%|█████████▉| 1419/1436 [02:43<00:01,  8.69it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  99%|█████████▉| 1420/1436 [02:43<00:01,  8.70it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  99%|█████████▉| 1421/1436 [02:43<00:01,  8.70it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  99%|█████████▉| 1422/1436 [02:43<00:01,  8.70it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  99%|█████████▉| 1423/1436 [02:43<00:01,  8.70it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  99%|█████████▉| 1424/1436 [02:43<00:01,  8.70it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  99%|█████████▉| 1425/1436 [02:43<00:01,  8.70it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  99%|█████████▉| 1426/1436 [02:43<00:01,  8.70it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  99%|█████████▉| 1427/1436 [02:44<00:01,  8.70it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2:  99%|█████████▉| 1428/1436 [02:44<00:00,  8.70it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2: 100%|█████████▉| 1429/1436 [02:44<00:00,  8.70it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2: 100%|█████████▉| 1430/1436 [02:44<00:00,  8.70it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2: 100%|█████████▉| 1431/1436 [02:44<00:00,  8.70it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2: 100%|█████████▉| 1432/1436 [02:44<00:00,  8.70it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2: 100%|█████████▉| 1433/1436 [02:44<00:00,  8.70it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2: 100%|█████████▉| 1434/1436 [02:44<00:00,  8.70it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2: 100%|█████████▉| 1435/1436 [02:44<00:00,  8.71it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0905, train_loss_epoch=0.0811]\n",
      "Epoch 2: 100%|██████████| 1436/1436 [02:44<00:00,  8.71it/s, loss=0.0704, v_num=0, train_loss_step=0.0716, val_loss=0.0851, train_loss_epoch=0.0811]\n",
      "Epoch 3:  94%|█████████▍| 1350/1436 [02:22<00:09,  9.46it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  94%|█████████▍| 1351/1436 [02:23<00:09,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  94%|█████████▍| 1352/1436 [02:23<00:08,  9.43it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  94%|█████████▍| 1353/1436 [02:23<00:08,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  94%|█████████▍| 1354/1436 [02:23<00:08,  9.43it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  94%|█████████▍| 1355/1436 [02:23<00:08,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  94%|█████████▍| 1356/1436 [02:23<00:08,  9.43it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  94%|█████████▍| 1357/1436 [02:24<00:08,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  95%|█████████▍| 1358/1436 [02:24<00:08,  9.43it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  95%|█████████▍| 1359/1436 [02:24<00:08,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  95%|█████████▍| 1360/1436 [02:24<00:08,  9.43it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  95%|█████████▍| 1361/1436 [02:24<00:07,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  95%|█████████▍| 1362/1436 [02:24<00:07,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  95%|█████████▍| 1363/1436 [02:24<00:07,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  95%|█████████▍| 1364/1436 [02:24<00:07,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  95%|█████████▌| 1365/1436 [02:24<00:07,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  95%|█████████▌| 1366/1436 [02:24<00:07,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  95%|█████████▌| 1367/1436 [02:25<00:07,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  95%|█████████▌| 1368/1436 [02:25<00:07,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  95%|█████████▌| 1369/1436 [02:25<00:07,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  95%|█████████▌| 1370/1436 [02:25<00:07,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  95%|█████████▌| 1371/1436 [02:25<00:06,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  96%|█████████▌| 1372/1436 [02:25<00:06,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  96%|█████████▌| 1373/1436 [02:25<00:06,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  96%|█████████▌| 1374/1436 [02:25<00:06,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  96%|█████████▌| 1375/1436 [02:26<00:06,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  96%|█████████▌| 1376/1436 [02:26<00:06,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  96%|█████████▌| 1377/1436 [02:26<00:06,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  96%|█████████▌| 1378/1436 [02:26<00:06,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  96%|█████████▌| 1379/1436 [02:26<00:06,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  96%|█████████▌| 1380/1436 [02:26<00:05,  9.42it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  96%|█████████▌| 1381/1436 [02:26<00:05,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  96%|█████████▌| 1382/1436 [02:26<00:05,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  96%|█████████▋| 1383/1436 [02:26<00:05,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  96%|█████████▋| 1384/1436 [02:27<00:05,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  96%|█████████▋| 1385/1436 [02:27<00:05,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  97%|█████████▋| 1386/1436 [02:27<00:05,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  97%|█████████▋| 1387/1436 [02:27<00:05,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  97%|█████████▋| 1388/1436 [02:27<00:05,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  97%|█████████▋| 1389/1436 [02:27<00:04,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  97%|█████████▋| 1390/1436 [02:27<00:04,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  97%|█████████▋| 1391/1436 [02:27<00:04,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  97%|█████████▋| 1392/1436 [02:27<00:04,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  97%|█████████▋| 1393/1436 [02:28<00:04,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  97%|█████████▋| 1394/1436 [02:28<00:04,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  97%|█████████▋| 1395/1436 [02:28<00:04,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  97%|█████████▋| 1396/1436 [02:28<00:04,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  97%|█████████▋| 1397/1436 [02:28<00:04,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  97%|█████████▋| 1398/1436 [02:28<00:04,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  97%|█████████▋| 1399/1436 [02:28<00:03,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  97%|█████████▋| 1400/1436 [02:28<00:03,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  98%|█████████▊| 1401/1436 [02:29<00:03,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  98%|█████████▊| 1402/1436 [02:29<00:03,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  98%|█████████▊| 1403/1436 [02:29<00:03,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  98%|█████████▊| 1404/1436 [02:29<00:03,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  98%|█████████▊| 1405/1436 [02:29<00:03,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  98%|█████████▊| 1406/1436 [02:29<00:03,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  98%|█████████▊| 1407/1436 [02:29<00:03,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  98%|█████████▊| 1408/1436 [02:29<00:02,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  98%|█████████▊| 1409/1436 [02:29<00:02,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  98%|█████████▊| 1410/1436 [02:29<00:02,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  98%|█████████▊| 1411/1436 [02:30<00:02,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  98%|█████████▊| 1412/1436 [02:30<00:02,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  98%|█████████▊| 1413/1436 [02:30<00:02,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  98%|█████████▊| 1414/1436 [02:30<00:02,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  99%|█████████▊| 1415/1436 [02:30<00:02,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  99%|█████████▊| 1416/1436 [02:30<00:02,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  99%|█████████▊| 1417/1436 [02:30<00:02,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  99%|█████████▊| 1418/1436 [02:30<00:01,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  99%|█████████▉| 1419/1436 [02:31<00:01,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  99%|█████████▉| 1420/1436 [02:31<00:01,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  99%|█████████▉| 1421/1436 [02:31<00:01,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  99%|█████████▉| 1422/1436 [02:31<00:01,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  99%|█████████▉| 1423/1436 [02:31<00:01,  9.39it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  99%|█████████▉| 1424/1436 [02:31<00:01,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  99%|█████████▉| 1425/1436 [02:31<00:01,  9.39it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  99%|█████████▉| 1426/1436 [02:31<00:01,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  99%|█████████▉| 1427/1436 [02:31<00:00,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3:  99%|█████████▉| 1428/1436 [02:31<00:00,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3: 100%|█████████▉| 1429/1436 [02:32<00:00,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3: 100%|█████████▉| 1430/1436 [02:32<00:00,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3: 100%|█████████▉| 1431/1436 [02:32<00:00,  9.39it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3: 100%|█████████▉| 1432/1436 [02:32<00:00,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3: 100%|█████████▉| 1433/1436 [02:32<00:00,  9.39it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3: 100%|█████████▉| 1434/1436 [02:32<00:00,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3: 100%|█████████▉| 1435/1436 [02:32<00:00,  9.40it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0851, train_loss_epoch=0.077]\n",
      "Epoch 3: 100%|██████████| 1436/1436 [02:32<00:00,  9.41it/s, loss=0.0702, v_num=0, train_loss_step=0.0622, val_loss=0.0838, train_loss_epoch=0.077]\n",
      "Epoch 4:  94%|█████████▍| 1350/1436 [02:21<00:09,  9.54it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  94%|█████████▍| 1351/1436 [02:22<00:08,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  94%|█████████▍| 1352/1436 [02:22<00:08,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  94%|█████████▍| 1353/1436 [02:22<00:08,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  94%|█████████▍| 1354/1436 [02:22<00:08,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  94%|█████████▍| 1355/1436 [02:22<00:08,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  94%|█████████▍| 1356/1436 [02:22<00:08,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  94%|█████████▍| 1357/1436 [02:22<00:08,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  95%|█████████▍| 1358/1436 [02:22<00:08,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  95%|█████████▍| 1359/1436 [02:22<00:08,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  95%|█████████▍| 1360/1436 [02:22<00:07,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  95%|█████████▍| 1361/1436 [02:23<00:07,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  95%|█████████▍| 1362/1436 [02:23<00:07,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  95%|█████████▍| 1363/1436 [02:23<00:07,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  95%|█████████▍| 1364/1436 [02:23<00:07,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  95%|█████████▌| 1365/1436 [02:23<00:07,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  95%|█████████▌| 1366/1436 [02:23<00:07,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  95%|█████████▌| 1367/1436 [02:23<00:07,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  95%|█████████▌| 1368/1436 [02:23<00:07,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  95%|█████████▌| 1369/1436 [02:23<00:07,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  95%|█████████▌| 1370/1436 [02:24<00:06,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  95%|█████████▌| 1371/1436 [02:24<00:06,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  96%|█████████▌| 1372/1436 [02:24<00:06,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  96%|█████████▌| 1373/1436 [02:24<00:06,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  96%|█████████▌| 1374/1436 [02:24<00:06,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  96%|█████████▌| 1375/1436 [02:24<00:06,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  96%|█████████▌| 1376/1436 [02:24<00:06,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  96%|█████████▌| 1377/1436 [02:24<00:06,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  96%|█████████▌| 1378/1436 [02:24<00:06,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  96%|█████████▌| 1379/1436 [02:25<00:05,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  96%|█████████▌| 1380/1436 [02:25<00:05,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  96%|█████████▌| 1381/1436 [02:25<00:05,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  96%|█████████▌| 1382/1436 [02:25<00:05,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  96%|█████████▋| 1383/1436 [02:25<00:05,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  96%|█████████▋| 1384/1436 [02:25<00:05,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  96%|█████████▋| 1385/1436 [02:25<00:05,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  97%|█████████▋| 1386/1436 [02:25<00:05,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  97%|█████████▋| 1387/1436 [02:25<00:05,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  97%|█████████▋| 1388/1436 [02:25<00:05,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  97%|█████████▋| 1389/1436 [02:26<00:04,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  97%|█████████▋| 1390/1436 [02:26<00:04,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  97%|█████████▋| 1391/1436 [02:26<00:04,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  97%|█████████▋| 1392/1436 [02:26<00:04,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  97%|█████████▋| 1393/1436 [02:26<00:04,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  97%|█████████▋| 1394/1436 [02:26<00:04,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  97%|█████████▋| 1395/1436 [02:26<00:04,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  97%|█████████▋| 1396/1436 [02:26<00:04,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  97%|█████████▋| 1397/1436 [02:26<00:04,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  97%|█████████▋| 1398/1436 [02:26<00:03,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  97%|█████████▋| 1399/1436 [02:27<00:03,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  97%|█████████▋| 1400/1436 [02:27<00:03,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  98%|█████████▊| 1401/1436 [02:27<00:03,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  98%|█████████▊| 1402/1436 [02:27<00:03,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  98%|█████████▊| 1403/1436 [02:27<00:03,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  98%|█████████▊| 1404/1436 [02:27<00:03,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  98%|█████████▊| 1405/1436 [02:27<00:03,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  98%|█████████▊| 1406/1436 [02:27<00:03,  9.52it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  98%|█████████▊| 1407/1436 [02:27<00:03,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  98%|█████████▊| 1408/1436 [02:27<00:02,  9.52it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  98%|█████████▊| 1409/1436 [02:28<00:02,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  98%|█████████▊| 1410/1436 [02:28<00:02,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  98%|█████████▊| 1411/1436 [02:28<00:02,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  98%|█████████▊| 1412/1436 [02:28<00:02,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  98%|█████████▊| 1413/1436 [02:28<00:02,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  98%|█████████▊| 1414/1436 [02:28<00:02,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  99%|█████████▊| 1415/1436 [02:28<00:02,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  99%|█████████▊| 1416/1436 [02:28<00:02,  9.52it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  99%|█████████▊| 1417/1436 [02:28<00:01,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  99%|█████████▊| 1418/1436 [02:29<00:01,  9.52it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  99%|█████████▉| 1419/1436 [02:29<00:01,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  99%|█████████▉| 1420/1436 [02:29<00:01,  9.52it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  99%|█████████▉| 1421/1436 [02:29<00:01,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  99%|█████████▉| 1422/1436 [02:29<00:01,  9.52it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  99%|█████████▉| 1423/1436 [02:29<00:01,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  99%|█████████▉| 1424/1436 [02:29<00:01,  9.52it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  99%|█████████▉| 1425/1436 [02:29<00:01,  9.52it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  99%|█████████▉| 1426/1436 [02:29<00:01,  9.52it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  99%|█████████▉| 1427/1436 [02:29<00:00,  9.51it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4:  99%|█████████▉| 1428/1436 [02:30<00:00,  9.52it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4: 100%|█████████▉| 1429/1436 [02:30<00:00,  9.52it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4: 100%|█████████▉| 1430/1436 [02:30<00:00,  9.52it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4: 100%|█████████▉| 1431/1436 [02:30<00:00,  9.52it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4: 100%|█████████▉| 1432/1436 [02:30<00:00,  9.52it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4: 100%|█████████▉| 1433/1436 [02:30<00:00,  9.52it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4: 100%|█████████▉| 1434/1436 [02:30<00:00,  9.52it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4: 100%|█████████▉| 1435/1436 [02:30<00:00,  9.53it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0838, train_loss_epoch=0.075]\n",
      "Epoch 4: 100%|██████████| 1436/1436 [02:30<00:00,  9.53it/s, loss=0.0721, v_num=0, train_loss_step=0.0666, val_loss=0.0854, train_loss_epoch=0.075]\n",
      "Epoch 5:  94%|█████████▍| 1350/1436 [02:23<00:09,  9.40it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  94%|█████████▍| 1351/1436 [02:24<00:09,  9.37it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  94%|█████████▍| 1352/1436 [02:24<00:08,  9.37it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  94%|█████████▍| 1353/1436 [02:24<00:08,  9.37it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  94%|█████████▍| 1354/1436 [02:24<00:08,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  94%|█████████▍| 1355/1436 [02:24<00:08,  9.37it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  94%|█████████▍| 1356/1436 [02:24<00:08,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  94%|█████████▍| 1357/1436 [02:24<00:08,  9.37it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  95%|█████████▍| 1358/1436 [02:24<00:08,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  95%|█████████▍| 1359/1436 [02:25<00:08,  9.37it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  95%|█████████▍| 1360/1436 [02:25<00:08,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  95%|█████████▍| 1361/1436 [02:25<00:08,  9.37it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  95%|█████████▍| 1362/1436 [02:25<00:07,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  95%|█████████▍| 1363/1436 [02:25<00:07,  9.37it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  95%|█████████▍| 1364/1436 [02:25<00:07,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  95%|█████████▌| 1365/1436 [02:25<00:07,  9.37it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  95%|█████████▌| 1366/1436 [02:25<00:07,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  95%|█████████▌| 1367/1436 [02:25<00:07,  9.37it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  95%|█████████▌| 1368/1436 [02:25<00:07,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  95%|█████████▌| 1369/1436 [02:26<00:07,  9.37it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  95%|█████████▌| 1370/1436 [02:26<00:07,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  95%|█████████▌| 1371/1436 [02:26<00:06,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  96%|█████████▌| 1372/1436 [02:26<00:06,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  96%|█████████▌| 1373/1436 [02:26<00:06,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  96%|█████████▌| 1374/1436 [02:26<00:06,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  96%|█████████▌| 1375/1436 [02:26<00:06,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  96%|█████████▌| 1376/1436 [02:26<00:06,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  96%|█████████▌| 1377/1436 [02:26<00:06,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  96%|█████████▌| 1378/1436 [02:26<00:06,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  96%|█████████▌| 1379/1436 [02:27<00:06,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  96%|█████████▌| 1380/1436 [02:27<00:05,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  96%|█████████▌| 1381/1436 [02:27<00:05,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  96%|█████████▌| 1382/1436 [02:27<00:05,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  96%|█████████▋| 1383/1436 [02:27<00:05,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  96%|█████████▋| 1384/1436 [02:27<00:05,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  96%|█████████▋| 1385/1436 [02:27<00:05,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  97%|█████████▋| 1386/1436 [02:27<00:05,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  97%|█████████▋| 1387/1436 [02:27<00:05,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  97%|█████████▋| 1388/1436 [02:27<00:05,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  97%|█████████▋| 1389/1436 [02:28<00:05,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  97%|█████████▋| 1390/1436 [02:28<00:04,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  97%|█████████▋| 1391/1436 [02:28<00:04,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  97%|█████████▋| 1392/1436 [02:28<00:04,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  97%|█████████▋| 1393/1436 [02:28<00:04,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  97%|█████████▋| 1394/1436 [02:28<00:04,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  97%|█████████▋| 1395/1436 [02:28<00:04,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  97%|█████████▋| 1396/1436 [02:28<00:04,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  97%|█████████▋| 1397/1436 [02:28<00:04,  9.38it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  97%|█████████▋| 1398/1436 [02:28<00:04,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  97%|█████████▋| 1399/1436 [02:29<00:03,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  97%|█████████▋| 1400/1436 [02:29<00:03,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  98%|█████████▊| 1401/1436 [02:29<00:03,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  98%|█████████▊| 1402/1436 [02:29<00:03,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  98%|█████████▊| 1403/1436 [02:29<00:03,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  98%|█████████▊| 1404/1436 [02:29<00:03,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  98%|█████████▊| 1405/1436 [02:29<00:03,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  98%|█████████▊| 1406/1436 [02:29<00:03,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  98%|█████████▊| 1407/1436 [02:29<00:03,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  98%|█████████▊| 1408/1436 [02:29<00:02,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  98%|█████████▊| 1409/1436 [02:30<00:02,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  98%|█████████▊| 1410/1436 [02:30<00:02,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  98%|█████████▊| 1411/1436 [02:30<00:02,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  98%|█████████▊| 1412/1436 [02:30<00:02,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  98%|█████████▊| 1413/1436 [02:30<00:02,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  98%|█████████▊| 1414/1436 [02:30<00:02,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  99%|█████████▊| 1415/1436 [02:30<00:02,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  99%|█████████▊| 1416/1436 [02:30<00:02,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  99%|█████████▊| 1417/1436 [02:30<00:02,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  99%|█████████▊| 1418/1436 [02:30<00:01,  9.40it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  99%|█████████▉| 1419/1436 [02:31<00:01,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  99%|█████████▉| 1420/1436 [02:31<00:01,  9.40it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  99%|█████████▉| 1421/1436 [02:31<00:01,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  99%|█████████▉| 1422/1436 [02:31<00:01,  9.40it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  99%|█████████▉| 1423/1436 [02:31<00:01,  9.39it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  99%|█████████▉| 1424/1436 [02:31<00:01,  9.40it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  99%|█████████▉| 1425/1436 [02:31<00:01,  9.40it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  99%|█████████▉| 1426/1436 [02:31<00:01,  9.40it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  99%|█████████▉| 1427/1436 [02:31<00:00,  9.40it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5:  99%|█████████▉| 1428/1436 [02:31<00:00,  9.40it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5: 100%|█████████▉| 1429/1436 [02:32<00:00,  9.40it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5: 100%|█████████▉| 1430/1436 [02:32<00:00,  9.40it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5: 100%|█████████▉| 1431/1436 [02:32<00:00,  9.40it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5: 100%|█████████▉| 1432/1436 [02:32<00:00,  9.40it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5: 100%|█████████▉| 1433/1436 [02:32<00:00,  9.40it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5: 100%|█████████▉| 1434/1436 [02:32<00:00,  9.40it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5: 100%|█████████▉| 1435/1436 [02:32<00:00,  9.41it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0854, train_loss_epoch=0.0739]\n",
      "Epoch 5: 100%|██████████| 1436/1436 [02:32<00:00,  9.41it/s, loss=0.0748, v_num=0, train_loss_step=0.0833, val_loss=0.0855, train_loss_epoch=0.0739]\n",
      "Epoch 6:  94%|█████████▍| 1350/1436 [02:19<00:08,  9.67it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  94%|█████████▍| 1351/1436 [02:20<00:08,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  94%|█████████▍| 1352/1436 [02:20<00:08,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  94%|█████████▍| 1353/1436 [02:20<00:08,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  94%|█████████▍| 1354/1436 [02:20<00:08,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  94%|█████████▍| 1355/1436 [02:20<00:08,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  94%|█████████▍| 1356/1436 [02:20<00:08,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  94%|█████████▍| 1357/1436 [02:20<00:08,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  95%|█████████▍| 1358/1436 [02:20<00:08,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  95%|█████████▍| 1359/1436 [02:20<00:07,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  95%|█████████▍| 1360/1436 [02:20<00:07,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  95%|█████████▍| 1361/1436 [02:21<00:07,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  95%|█████████▍| 1362/1436 [02:21<00:07,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  95%|█████████▍| 1363/1436 [02:21<00:07,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  95%|█████████▍| 1364/1436 [02:21<00:07,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  95%|█████████▌| 1365/1436 [02:21<00:07,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  95%|█████████▌| 1366/1436 [02:21<00:07,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  95%|█████████▌| 1367/1436 [02:21<00:07,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  95%|█████████▌| 1368/1436 [02:21<00:07,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  95%|█████████▌| 1369/1436 [02:21<00:06,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  95%|█████████▌| 1370/1436 [02:22<00:06,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  95%|█████████▌| 1371/1436 [02:22<00:06,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  96%|█████████▌| 1372/1436 [02:22<00:06,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  96%|█████████▌| 1373/1436 [02:22<00:06,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  96%|█████████▌| 1374/1436 [02:22<00:06,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  96%|█████████▌| 1375/1436 [02:22<00:06,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  96%|█████████▌| 1376/1436 [02:22<00:06,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  96%|█████████▌| 1377/1436 [02:22<00:06,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  96%|█████████▌| 1378/1436 [02:22<00:06,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  96%|█████████▌| 1379/1436 [02:23<00:05,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  96%|█████████▌| 1380/1436 [02:23<00:05,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  96%|█████████▌| 1381/1436 [02:23<00:05,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  96%|█████████▌| 1382/1436 [02:23<00:05,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  96%|█████████▋| 1383/1436 [02:23<00:05,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  96%|█████████▋| 1384/1436 [02:23<00:05,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  96%|█████████▋| 1385/1436 [02:23<00:05,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  97%|█████████▋| 1386/1436 [02:23<00:05,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  97%|█████████▋| 1387/1436 [02:23<00:05,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  97%|█████████▋| 1388/1436 [02:23<00:04,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  97%|█████████▋| 1389/1436 [02:24<00:04,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  97%|█████████▋| 1390/1436 [02:24<00:04,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  97%|█████████▋| 1391/1436 [02:24<00:04,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  97%|█████████▋| 1392/1436 [02:24<00:04,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  97%|█████████▋| 1393/1436 [02:24<00:04,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  97%|█████████▋| 1394/1436 [02:24<00:04,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  97%|█████████▋| 1395/1436 [02:24<00:04,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  97%|█████████▋| 1396/1436 [02:24<00:04,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  97%|█████████▋| 1397/1436 [02:24<00:04,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  97%|█████████▋| 1398/1436 [02:24<00:03,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  97%|█████████▋| 1399/1436 [02:25<00:03,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  97%|█████████▋| 1400/1436 [02:25<00:03,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  98%|█████████▊| 1401/1436 [02:25<00:03,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  98%|█████████▊| 1402/1436 [02:25<00:03,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  98%|█████████▊| 1403/1436 [02:25<00:03,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  98%|█████████▊| 1404/1436 [02:25<00:03,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  98%|█████████▊| 1405/1436 [02:25<00:03,  9.64it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  98%|█████████▊| 1406/1436 [02:25<00:03,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  98%|█████████▊| 1407/1436 [02:25<00:03,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  98%|█████████▊| 1408/1436 [02:25<00:02,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  98%|█████████▊| 1409/1436 [02:26<00:02,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  98%|█████████▊| 1410/1436 [02:26<00:02,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  98%|█████████▊| 1411/1436 [02:26<00:02,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  98%|█████████▊| 1412/1436 [02:26<00:02,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  98%|█████████▊| 1413/1436 [02:26<00:02,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  98%|█████████▊| 1414/1436 [02:26<00:02,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  99%|█████████▊| 1415/1436 [02:26<00:02,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  99%|█████████▊| 1416/1436 [02:26<00:02,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  99%|█████████▊| 1417/1436 [02:26<00:01,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  99%|█████████▊| 1418/1436 [02:26<00:01,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  99%|█████████▉| 1419/1436 [02:27<00:01,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  99%|█████████▉| 1420/1436 [02:27<00:01,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  99%|█████████▉| 1421/1436 [02:27<00:01,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  99%|█████████▉| 1422/1436 [02:27<00:01,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  99%|█████████▉| 1423/1436 [02:27<00:01,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  99%|█████████▉| 1424/1436 [02:27<00:01,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  99%|█████████▉| 1425/1436 [02:27<00:01,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  99%|█████████▉| 1426/1436 [02:27<00:01,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  99%|█████████▉| 1427/1436 [02:27<00:00,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6:  99%|█████████▉| 1428/1436 [02:27<00:00,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6: 100%|█████████▉| 1429/1436 [02:28<00:00,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6: 100%|█████████▉| 1430/1436 [02:28<00:00,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6: 100%|█████████▉| 1431/1436 [02:28<00:00,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6: 100%|█████████▉| 1432/1436 [02:28<00:00,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6: 100%|█████████▉| 1433/1436 [02:28<00:00,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6: 100%|█████████▉| 1434/1436 [02:28<00:00,  9.65it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6: 100%|█████████▉| 1435/1436 [02:28<00:00,  9.66it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0855, train_loss_epoch=0.0731]\n",
      "Epoch 6: 100%|██████████| 1436/1436 [02:28<00:00,  9.66it/s, loss=0.0765, v_num=0, train_loss_step=0.0716, val_loss=0.0859, train_loss_epoch=0.0731]\n",
      "Epoch 7:  94%|█████████▍| 1350/1436 [02:29<00:09,  9.03it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  94%|█████████▍| 1351/1436 [02:30<00:09,  9.00it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  94%|█████████▍| 1352/1436 [02:30<00:09,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  94%|█████████▍| 1353/1436 [02:30<00:09,  9.00it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  94%|█████████▍| 1354/1436 [02:30<00:09,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  94%|█████████▍| 1355/1436 [02:30<00:08,  9.00it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  94%|█████████▍| 1356/1436 [02:30<00:08,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  94%|█████████▍| 1357/1436 [02:30<00:08,  9.00it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  95%|█████████▍| 1358/1436 [02:30<00:08,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  95%|█████████▍| 1359/1436 [02:30<00:08,  9.00it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  95%|█████████▍| 1360/1436 [02:30<00:08,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  95%|█████████▍| 1361/1436 [02:31<00:08,  9.00it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  95%|█████████▍| 1362/1436 [02:31<00:08,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  95%|█████████▍| 1363/1436 [02:31<00:08,  9.00it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  95%|█████████▍| 1364/1436 [02:31<00:07,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  95%|█████████▌| 1365/1436 [02:31<00:07,  9.00it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  95%|█████████▌| 1366/1436 [02:31<00:07,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  95%|█████████▌| 1367/1436 [02:31<00:07,  9.00it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  95%|█████████▌| 1368/1436 [02:31<00:07,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  95%|█████████▌| 1369/1436 [02:31<00:07,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  95%|█████████▌| 1370/1436 [02:32<00:07,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  95%|█████████▌| 1371/1436 [02:32<00:07,  9.00it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  96%|█████████▌| 1372/1436 [02:32<00:07,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  96%|█████████▌| 1373/1436 [02:32<00:06,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  96%|█████████▌| 1374/1436 [02:32<00:06,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  96%|█████████▌| 1375/1436 [02:32<00:06,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  96%|█████████▌| 1376/1436 [02:32<00:06,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  96%|█████████▌| 1377/1436 [02:32<00:06,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  96%|█████████▌| 1378/1436 [02:32<00:06,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  96%|█████████▌| 1379/1436 [02:33<00:06,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  96%|█████████▌| 1380/1436 [02:33<00:06,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  96%|█████████▌| 1381/1436 [02:33<00:06,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  96%|█████████▌| 1382/1436 [02:33<00:05,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  96%|█████████▋| 1383/1436 [02:33<00:05,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  96%|█████████▋| 1384/1436 [02:33<00:05,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  96%|█████████▋| 1385/1436 [02:33<00:05,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  97%|█████████▋| 1386/1436 [02:33<00:05,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  97%|█████████▋| 1387/1436 [02:33<00:05,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  97%|█████████▋| 1388/1436 [02:33<00:05,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  97%|█████████▋| 1389/1436 [02:34<00:05,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  97%|█████████▋| 1390/1436 [02:34<00:05,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  97%|█████████▋| 1391/1436 [02:34<00:04,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  97%|█████████▋| 1392/1436 [02:34<00:04,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  97%|█████████▋| 1393/1436 [02:34<00:04,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  97%|█████████▋| 1394/1436 [02:34<00:04,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  97%|█████████▋| 1395/1436 [02:34<00:04,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  97%|█████████▋| 1396/1436 [02:34<00:04,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  97%|█████████▋| 1397/1436 [02:35<00:04,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  97%|█████████▋| 1398/1436 [02:35<00:04,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  97%|█████████▋| 1399/1436 [02:35<00:04,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  97%|█████████▋| 1400/1436 [02:35<00:03,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  98%|█████████▊| 1401/1436 [02:35<00:03,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  98%|█████████▊| 1402/1436 [02:35<00:03,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  98%|█████████▊| 1403/1436 [02:35<00:03,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  98%|█████████▊| 1404/1436 [02:35<00:03,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  98%|█████████▊| 1405/1436 [02:35<00:03,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  98%|█████████▊| 1406/1436 [02:35<00:03,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  98%|█████████▊| 1407/1436 [02:36<00:03,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  98%|█████████▊| 1408/1436 [02:36<00:03,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  98%|█████████▊| 1409/1436 [02:36<00:02,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  98%|█████████▊| 1410/1436 [02:36<00:02,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  98%|█████████▊| 1411/1436 [02:36<00:02,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  98%|█████████▊| 1412/1436 [02:36<00:02,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  98%|█████████▊| 1413/1436 [02:36<00:02,  9.01it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  98%|█████████▊| 1414/1436 [02:36<00:02,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  99%|█████████▊| 1415/1436 [02:36<00:02,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  99%|█████████▊| 1416/1436 [02:36<00:02,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  99%|█████████▊| 1417/1436 [02:37<00:02,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  99%|█████████▊| 1418/1436 [02:37<00:01,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  99%|█████████▉| 1419/1436 [02:37<00:01,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  99%|█████████▉| 1420/1436 [02:37<00:01,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  99%|█████████▉| 1421/1436 [02:37<00:01,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  99%|█████████▉| 1422/1436 [02:37<00:01,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  99%|█████████▉| 1423/1436 [02:37<00:01,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  99%|█████████▉| 1424/1436 [02:37<00:01,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  99%|█████████▉| 1425/1436 [02:38<00:01,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  99%|█████████▉| 1426/1436 [02:38<00:01,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  99%|█████████▉| 1427/1436 [02:38<00:00,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7:  99%|█████████▉| 1428/1436 [02:38<00:00,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7: 100%|█████████▉| 1429/1436 [02:38<00:00,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7: 100%|█████████▉| 1430/1436 [02:38<00:00,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7: 100%|█████████▉| 1431/1436 [02:38<00:00,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7: 100%|█████████▉| 1432/1436 [02:38<00:00,  9.03it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7: 100%|█████████▉| 1433/1436 [02:38<00:00,  9.02it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7: 100%|█████████▉| 1434/1436 [02:38<00:00,  9.03it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7: 100%|█████████▉| 1435/1436 [02:38<00:00,  9.03it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0859, train_loss_epoch=0.0726]\n",
      "Epoch 7: 100%|██████████| 1436/1436 [02:38<00:00,  9.04it/s, loss=0.0702, v_num=0, train_loss_step=0.0472, val_loss=0.0847, train_loss_epoch=0.0726]\n",
      "Epoch 8:  94%|█████████▍| 1350/1436 [02:23<00:09,  9.44it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  94%|█████████▍| 1351/1436 [02:23<00:09,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  94%|█████████▍| 1352/1436 [02:23<00:08,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  94%|█████████▍| 1353/1436 [02:23<00:08,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  94%|█████████▍| 1354/1436 [02:23<00:08,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  94%|█████████▍| 1355/1436 [02:24<00:08,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  94%|█████████▍| 1356/1436 [02:24<00:08,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  94%|█████████▍| 1357/1436 [02:24<00:08,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  95%|█████████▍| 1358/1436 [02:24<00:08,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  95%|█████████▍| 1359/1436 [02:24<00:08,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  95%|█████████▍| 1360/1436 [02:24<00:08,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  95%|█████████▍| 1361/1436 [02:24<00:07,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  95%|█████████▍| 1362/1436 [02:24<00:07,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  95%|█████████▍| 1363/1436 [02:24<00:07,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  95%|█████████▍| 1364/1436 [02:24<00:07,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  95%|█████████▌| 1365/1436 [02:25<00:07,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  95%|█████████▌| 1366/1436 [02:25<00:07,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  95%|█████████▌| 1367/1436 [02:25<00:07,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  95%|█████████▌| 1368/1436 [02:25<00:07,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  95%|█████████▌| 1369/1436 [02:25<00:07,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  95%|█████████▌| 1370/1436 [02:25<00:07,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  95%|█████████▌| 1371/1436 [02:25<00:06,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  96%|█████████▌| 1372/1436 [02:25<00:06,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  96%|█████████▌| 1373/1436 [02:25<00:06,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  96%|█████████▌| 1374/1436 [02:25<00:06,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  96%|█████████▌| 1375/1436 [02:26<00:06,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  96%|█████████▌| 1376/1436 [02:26<00:06,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  96%|█████████▌| 1377/1436 [02:26<00:06,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  96%|█████████▌| 1378/1436 [02:26<00:06,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  96%|█████████▌| 1379/1436 [02:26<00:06,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  96%|█████████▌| 1380/1436 [02:26<00:05,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  96%|█████████▌| 1381/1436 [02:26<00:05,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  96%|█████████▌| 1382/1436 [02:26<00:05,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  96%|█████████▋| 1383/1436 [02:26<00:05,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  96%|█████████▋| 1384/1436 [02:26<00:05,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  96%|█████████▋| 1385/1436 [02:27<00:05,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  97%|█████████▋| 1386/1436 [02:27<00:05,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  97%|█████████▋| 1387/1436 [02:27<00:05,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  97%|█████████▋| 1388/1436 [02:27<00:05,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  97%|█████████▋| 1389/1436 [02:27<00:04,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  97%|█████████▋| 1390/1436 [02:27<00:04,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  97%|█████████▋| 1391/1436 [02:27<00:04,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  97%|█████████▋| 1392/1436 [02:27<00:04,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  97%|█████████▋| 1393/1436 [02:27<00:04,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  97%|█████████▋| 1394/1436 [02:28<00:04,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  97%|█████████▋| 1395/1436 [02:28<00:04,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  97%|█████████▋| 1396/1436 [02:28<00:04,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  97%|█████████▋| 1397/1436 [02:28<00:04,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  97%|█████████▋| 1398/1436 [02:28<00:04,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  97%|█████████▋| 1399/1436 [02:28<00:03,  9.41it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  97%|█████████▋| 1400/1436 [02:28<00:03,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  98%|█████████▊| 1401/1436 [02:28<00:03,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  98%|█████████▊| 1402/1436 [02:28<00:03,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  98%|█████████▊| 1403/1436 [02:29<00:03,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  98%|█████████▊| 1404/1436 [02:29<00:03,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  98%|█████████▊| 1405/1436 [02:29<00:03,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  98%|█████████▊| 1406/1436 [02:29<00:03,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  98%|█████████▊| 1407/1436 [02:29<00:03,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  98%|█████████▊| 1408/1436 [02:29<00:02,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  98%|█████████▊| 1409/1436 [02:29<00:02,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  98%|█████████▊| 1410/1436 [02:29<00:02,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  98%|█████████▊| 1411/1436 [02:29<00:02,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  98%|█████████▊| 1412/1436 [02:29<00:02,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  98%|█████████▊| 1413/1436 [02:30<00:02,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  98%|█████████▊| 1414/1436 [02:30<00:02,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  99%|█████████▊| 1415/1436 [02:30<00:02,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  99%|█████████▊| 1416/1436 [02:30<00:02,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  99%|█████████▊| 1417/1436 [02:30<00:02,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  99%|█████████▊| 1418/1436 [02:30<00:01,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  99%|█████████▉| 1419/1436 [02:30<00:01,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  99%|█████████▉| 1420/1436 [02:30<00:01,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  99%|█████████▉| 1421/1436 [02:30<00:01,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  99%|█████████▉| 1422/1436 [02:30<00:01,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  99%|█████████▉| 1423/1436 [02:31<00:01,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  99%|█████████▉| 1424/1436 [02:31<00:01,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  99%|█████████▉| 1425/1436 [02:31<00:01,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  99%|█████████▉| 1426/1436 [02:31<00:01,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  99%|█████████▉| 1427/1436 [02:31<00:00,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8:  99%|█████████▉| 1428/1436 [02:31<00:00,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8: 100%|█████████▉| 1429/1436 [02:31<00:00,  9.42it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8: 100%|█████████▉| 1430/1436 [02:31<00:00,  9.43it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8: 100%|█████████▉| 1431/1436 [02:31<00:00,  9.43it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8: 100%|█████████▉| 1432/1436 [02:31<00:00,  9.43it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8: 100%|█████████▉| 1433/1436 [02:32<00:00,  9.43it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8: 100%|█████████▉| 1434/1436 [02:32<00:00,  9.43it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8: 100%|█████████▉| 1435/1436 [02:32<00:00,  9.44it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0847, train_loss_epoch=0.0719]\n",
      "Epoch 8: 100%|██████████| 1436/1436 [02:32<00:00,  9.44it/s, loss=0.0705, v_num=0, train_loss_step=0.0662, val_loss=0.0836, train_loss_epoch=0.0719]\n",
      "Epoch 9:  94%|█████████▍| 1350/1436 [02:22<00:09,  9.46it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  94%|█████████▍| 1351/1436 [02:23<00:09,  9.42it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  94%|█████████▍| 1352/1436 [02:23<00:08,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  94%|█████████▍| 1353/1436 [02:23<00:08,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  94%|█████████▍| 1354/1436 [02:23<00:08,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  94%|█████████▍| 1355/1436 [02:23<00:08,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  94%|█████████▍| 1356/1436 [02:23<00:08,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  94%|█████████▍| 1357/1436 [02:23<00:08,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  95%|█████████▍| 1358/1436 [02:23<00:08,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  95%|█████████▍| 1359/1436 [02:24<00:08,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  95%|█████████▍| 1360/1436 [02:24<00:08,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  95%|█████████▍| 1361/1436 [02:24<00:07,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  95%|█████████▍| 1362/1436 [02:24<00:07,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  95%|█████████▍| 1363/1436 [02:24<00:07,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  95%|█████████▍| 1364/1436 [02:24<00:07,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  95%|█████████▌| 1365/1436 [02:24<00:07,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  95%|█████████▌| 1366/1436 [02:24<00:07,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  95%|█████████▌| 1367/1436 [02:24<00:07,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  95%|█████████▌| 1368/1436 [02:25<00:07,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  95%|█████████▌| 1369/1436 [02:25<00:07,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  95%|█████████▌| 1370/1436 [02:25<00:06,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  95%|█████████▌| 1371/1436 [02:25<00:06,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  96%|█████████▌| 1372/1436 [02:25<00:06,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  96%|█████████▌| 1373/1436 [02:25<00:06,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  96%|█████████▌| 1374/1436 [02:25<00:06,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  96%|█████████▌| 1375/1436 [02:25<00:06,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  96%|█████████▌| 1376/1436 [02:25<00:06,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  96%|█████████▌| 1377/1436 [02:26<00:06,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  96%|█████████▌| 1378/1436 [02:26<00:06,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  96%|█████████▌| 1379/1436 [02:26<00:06,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  96%|█████████▌| 1380/1436 [02:26<00:05,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  96%|█████████▌| 1381/1436 [02:26<00:05,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  96%|█████████▌| 1382/1436 [02:26<00:05,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  96%|█████████▋| 1383/1436 [02:26<00:05,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  96%|█████████▋| 1384/1436 [02:26<00:05,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  96%|█████████▋| 1385/1436 [02:26<00:05,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  97%|█████████▋| 1386/1436 [02:26<00:05,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  97%|█████████▋| 1387/1436 [02:27<00:05,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  97%|█████████▋| 1388/1436 [02:27<00:05,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  97%|█████████▋| 1389/1436 [02:27<00:04,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  97%|█████████▋| 1390/1436 [02:27<00:04,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  97%|█████████▋| 1391/1436 [02:27<00:04,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  97%|█████████▋| 1392/1436 [02:27<00:04,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  97%|█████████▋| 1393/1436 [02:27<00:04,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  97%|█████████▋| 1394/1436 [02:27<00:04,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  97%|█████████▋| 1395/1436 [02:27<00:04,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  97%|█████████▋| 1396/1436 [02:27<00:04,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  97%|█████████▋| 1397/1436 [02:28<00:04,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  97%|█████████▋| 1398/1436 [02:28<00:04,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  97%|█████████▋| 1399/1436 [02:28<00:03,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  97%|█████████▋| 1400/1436 [02:28<00:03,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  98%|█████████▊| 1401/1436 [02:28<00:03,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  98%|█████████▊| 1402/1436 [02:28<00:03,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  98%|█████████▊| 1403/1436 [02:28<00:03,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  98%|█████████▊| 1404/1436 [02:28<00:03,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  98%|█████████▊| 1405/1436 [02:28<00:03,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  98%|█████████▊| 1406/1436 [02:28<00:03,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  98%|█████████▊| 1407/1436 [02:29<00:03,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  98%|█████████▊| 1408/1436 [02:29<00:02,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  98%|█████████▊| 1409/1436 [02:29<00:02,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  98%|█████████▊| 1410/1436 [02:29<00:02,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  98%|█████████▊| 1411/1436 [02:29<00:02,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  98%|█████████▊| 1412/1436 [02:29<00:02,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  98%|█████████▊| 1413/1436 [02:29<00:02,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  98%|█████████▊| 1414/1436 [02:29<00:02,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  99%|█████████▊| 1415/1436 [02:29<00:02,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  99%|█████████▊| 1416/1436 [02:30<00:02,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  99%|█████████▊| 1417/1436 [02:30<00:02,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  99%|█████████▊| 1418/1436 [02:30<00:01,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  99%|█████████▉| 1419/1436 [02:30<00:01,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  99%|█████████▉| 1420/1436 [02:30<00:01,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  99%|█████████▉| 1421/1436 [02:30<00:01,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  99%|█████████▉| 1422/1436 [02:30<00:01,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  99%|█████████▉| 1423/1436 [02:30<00:01,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  99%|█████████▉| 1424/1436 [02:30<00:01,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  99%|█████████▉| 1425/1436 [02:31<00:01,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  99%|█████████▉| 1426/1436 [02:31<00:01,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  99%|█████████▉| 1427/1436 [02:31<00:00,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9:  99%|█████████▉| 1428/1436 [02:31<00:00,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9: 100%|█████████▉| 1429/1436 [02:31<00:00,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9: 100%|█████████▉| 1430/1436 [02:31<00:00,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9: 100%|█████████▉| 1431/1436 [02:31<00:00,  9.43it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9: 100%|█████████▉| 1432/1436 [02:31<00:00,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9: 100%|█████████▉| 1433/1436 [02:31<00:00,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9: 100%|█████████▉| 1434/1436 [02:31<00:00,  9.44it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9: 100%|█████████▉| 1435/1436 [02:31<00:00,  9.45it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0836, train_loss_epoch=0.0715]\n",
      "Epoch 9: 100%|██████████| 1436/1436 [02:31<00:00,  9.45it/s, loss=0.0713, v_num=0, train_loss_step=0.0564, val_loss=0.0835, train_loss_epoch=0.0715]\n",
      "Epoch 10:  94%|█████████▍| 1350/1436 [02:21<00:09,  9.54it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  94%|█████████▍| 1351/1436 [02:22<00:08,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  94%|█████████▍| 1352/1436 [02:22<00:08,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  94%|█████████▍| 1353/1436 [02:22<00:08,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  94%|█████████▍| 1354/1436 [02:22<00:08,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  94%|█████████▍| 1355/1436 [02:22<00:08,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  94%|█████████▍| 1356/1436 [02:22<00:08,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  94%|█████████▍| 1357/1436 [02:22<00:08,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  95%|█████████▍| 1358/1436 [02:22<00:08,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  95%|█████████▍| 1359/1436 [02:22<00:08,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  95%|█████████▍| 1360/1436 [02:22<00:07,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  95%|█████████▍| 1361/1436 [02:23<00:07,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  95%|█████████▍| 1362/1436 [02:23<00:07,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  95%|█████████▍| 1363/1436 [02:23<00:07,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  95%|█████████▍| 1364/1436 [02:23<00:07,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  95%|█████████▌| 1365/1436 [02:23<00:07,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  95%|█████████▌| 1366/1436 [02:23<00:07,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  95%|█████████▌| 1367/1436 [02:23<00:07,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  95%|█████████▌| 1368/1436 [02:23<00:07,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  95%|█████████▌| 1369/1436 [02:23<00:07,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  95%|█████████▌| 1370/1436 [02:24<00:06,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  95%|█████████▌| 1371/1436 [02:24<00:06,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  96%|█████████▌| 1372/1436 [02:24<00:06,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  96%|█████████▌| 1373/1436 [02:24<00:06,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  96%|█████████▌| 1374/1436 [02:24<00:06,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  96%|█████████▌| 1375/1436 [02:24<00:06,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  96%|█████████▌| 1376/1436 [02:24<00:06,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  96%|█████████▌| 1377/1436 [02:24<00:06,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  96%|█████████▌| 1378/1436 [02:24<00:06,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  96%|█████████▌| 1379/1436 [02:25<00:05,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  96%|█████████▌| 1380/1436 [02:25<00:05,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  96%|█████████▌| 1381/1436 [02:25<00:05,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  96%|█████████▌| 1382/1436 [02:25<00:05,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  96%|█████████▋| 1383/1436 [02:25<00:05,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  96%|█████████▋| 1384/1436 [02:25<00:05,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  96%|█████████▋| 1385/1436 [02:25<00:05,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  97%|█████████▋| 1386/1436 [02:25<00:05,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  97%|█████████▋| 1387/1436 [02:25<00:05,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  97%|█████████▋| 1388/1436 [02:25<00:05,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  97%|█████████▋| 1389/1436 [02:26<00:04,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  97%|█████████▋| 1390/1436 [02:26<00:04,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  97%|█████████▋| 1391/1436 [02:26<00:04,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  97%|█████████▋| 1392/1436 [02:26<00:04,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  97%|█████████▋| 1393/1436 [02:26<00:04,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  97%|█████████▋| 1394/1436 [02:26<00:04,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  97%|█████████▋| 1395/1436 [02:26<00:04,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  97%|█████████▋| 1396/1436 [02:26<00:04,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  97%|█████████▋| 1397/1436 [02:26<00:04,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  97%|█████████▋| 1398/1436 [02:27<00:03,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  97%|█████████▋| 1399/1436 [02:27<00:03,  9.50it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  97%|█████████▋| 1400/1436 [02:27<00:03,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  98%|█████████▊| 1401/1436 [02:27<00:03,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  98%|█████████▊| 1402/1436 [02:27<00:03,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  98%|█████████▊| 1403/1436 [02:27<00:03,  9.50it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  98%|█████████▊| 1404/1436 [02:27<00:03,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  98%|█████████▊| 1405/1436 [02:27<00:03,  9.50it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  98%|█████████▊| 1406/1436 [02:27<00:03,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  98%|█████████▊| 1407/1436 [02:27<00:03,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  98%|█████████▊| 1408/1436 [02:28<00:02,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  98%|█████████▊| 1409/1436 [02:28<00:02,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  98%|█████████▊| 1410/1436 [02:28<00:02,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  98%|█████████▊| 1411/1436 [02:28<00:02,  9.50it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  98%|█████████▊| 1412/1436 [02:28<00:02,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  98%|█████████▊| 1413/1436 [02:28<00:02,  9.50it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  98%|█████████▊| 1414/1436 [02:28<00:02,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  99%|█████████▊| 1415/1436 [02:28<00:02,  9.50it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  99%|█████████▊| 1416/1436 [02:28<00:02,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  99%|█████████▊| 1417/1436 [02:29<00:01,  9.50it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  99%|█████████▊| 1418/1436 [02:29<00:01,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  99%|█████████▉| 1419/1436 [02:29<00:01,  9.50it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  99%|█████████▉| 1420/1436 [02:29<00:01,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  99%|█████████▉| 1421/1436 [02:29<00:01,  9.50it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  99%|█████████▉| 1422/1436 [02:29<00:01,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  99%|█████████▉| 1423/1436 [02:29<00:01,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  99%|█████████▉| 1424/1436 [02:29<00:01,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  99%|█████████▉| 1425/1436 [02:29<00:01,  9.50it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  99%|█████████▉| 1426/1436 [02:29<00:01,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  99%|█████████▉| 1427/1436 [02:30<00:00,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10:  99%|█████████▉| 1428/1436 [02:30<00:00,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10: 100%|█████████▉| 1429/1436 [02:30<00:00,  9.50it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10: 100%|█████████▉| 1430/1436 [02:30<00:00,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10: 100%|█████████▉| 1431/1436 [02:30<00:00,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10: 100%|█████████▉| 1432/1436 [02:30<00:00,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10: 100%|█████████▉| 1433/1436 [02:30<00:00,  9.50it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10: 100%|█████████▉| 1434/1436 [02:30<00:00,  9.51it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10: 100%|█████████▉| 1435/1436 [02:30<00:00,  9.52it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0835, train_loss_epoch=0.071]\n",
      "Epoch 10: 100%|██████████| 1436/1436 [02:30<00:00,  9.52it/s, loss=0.0729, v_num=0, train_loss_step=0.0603, val_loss=0.0859, train_loss_epoch=0.071]\n",
      "Epoch 11:  94%|█████████▍| 1350/1436 [02:24<00:09,  9.37it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  94%|█████████▍| 1351/1436 [02:24<00:09,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  94%|█████████▍| 1352/1436 [02:24<00:08,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  94%|█████████▍| 1353/1436 [02:24<00:08,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  94%|█████████▍| 1354/1436 [02:24<00:08,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  94%|█████████▍| 1355/1436 [02:25<00:08,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  94%|█████████▍| 1356/1436 [02:25<00:08,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  94%|█████████▍| 1357/1436 [02:25<00:08,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  95%|█████████▍| 1358/1436 [02:25<00:08,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  95%|█████████▍| 1359/1436 [02:25<00:08,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  95%|█████████▍| 1360/1436 [02:25<00:08,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  95%|█████████▍| 1361/1436 [02:25<00:08,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  95%|█████████▍| 1362/1436 [02:25<00:07,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  95%|█████████▍| 1363/1436 [02:25<00:07,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  95%|█████████▍| 1364/1436 [02:26<00:07,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  95%|█████████▌| 1365/1436 [02:26<00:07,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  95%|█████████▌| 1366/1436 [02:26<00:07,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  95%|█████████▌| 1367/1436 [02:26<00:07,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  95%|█████████▌| 1368/1436 [02:26<00:07,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  95%|█████████▌| 1369/1436 [02:26<00:07,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  95%|█████████▌| 1370/1436 [02:26<00:07,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  95%|█████████▌| 1371/1436 [02:26<00:06,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  96%|█████████▌| 1372/1436 [02:26<00:06,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  96%|█████████▌| 1373/1436 [02:27<00:06,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  96%|█████████▌| 1374/1436 [02:27<00:06,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  96%|█████████▌| 1375/1436 [02:27<00:06,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  96%|█████████▌| 1376/1436 [02:27<00:06,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  96%|█████████▌| 1377/1436 [02:27<00:06,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  96%|█████████▌| 1378/1436 [02:27<00:06,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  96%|█████████▌| 1379/1436 [02:27<00:06,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  96%|█████████▌| 1380/1436 [02:27<00:05,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  96%|█████████▌| 1381/1436 [02:27<00:05,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  96%|█████████▌| 1382/1436 [02:27<00:05,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  96%|█████████▋| 1383/1436 [02:28<00:05,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  96%|█████████▋| 1384/1436 [02:28<00:05,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  96%|█████████▋| 1385/1436 [02:28<00:05,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  97%|█████████▋| 1386/1436 [02:28<00:05,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  97%|█████████▋| 1387/1436 [02:28<00:05,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  97%|█████████▋| 1388/1436 [02:28<00:05,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  97%|█████████▋| 1389/1436 [02:28<00:05,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  97%|█████████▋| 1390/1436 [02:28<00:04,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  97%|█████████▋| 1391/1436 [02:28<00:04,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  97%|█████████▋| 1392/1436 [02:29<00:04,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  97%|█████████▋| 1393/1436 [02:29<00:04,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  97%|█████████▋| 1394/1436 [02:29<00:04,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  97%|█████████▋| 1395/1436 [02:29<00:04,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  97%|█████████▋| 1396/1436 [02:29<00:04,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  97%|█████████▋| 1397/1436 [02:29<00:04,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  97%|█████████▋| 1398/1436 [02:29<00:04,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  97%|█████████▋| 1399/1436 [02:29<00:03,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  97%|█████████▋| 1400/1436 [02:29<00:03,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  98%|█████████▊| 1401/1436 [02:29<00:03,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  98%|█████████▊| 1402/1436 [02:30<00:03,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  98%|█████████▊| 1403/1436 [02:30<00:03,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  98%|█████████▊| 1404/1436 [02:30<00:03,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  98%|█████████▊| 1405/1436 [02:30<00:03,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  98%|█████████▊| 1406/1436 [02:30<00:03,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  98%|█████████▊| 1407/1436 [02:30<00:03,  9.34it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  98%|█████████▊| 1408/1436 [02:30<00:02,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  98%|█████████▊| 1409/1436 [02:30<00:02,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  98%|█████████▊| 1410/1436 [02:30<00:02,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  98%|█████████▊| 1411/1436 [02:30<00:02,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  98%|█████████▊| 1412/1436 [02:31<00:02,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  98%|█████████▊| 1413/1436 [02:31<00:02,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  98%|█████████▊| 1414/1436 [02:31<00:02,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  99%|█████████▊| 1415/1436 [02:31<00:02,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  99%|█████████▊| 1416/1436 [02:31<00:02,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  99%|█████████▊| 1417/1436 [02:31<00:02,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  99%|█████████▊| 1418/1436 [02:31<00:01,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  99%|█████████▉| 1419/1436 [02:31<00:01,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  99%|█████████▉| 1420/1436 [02:31<00:01,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  99%|█████████▉| 1421/1436 [02:31<00:01,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  99%|█████████▉| 1422/1436 [02:32<00:01,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  99%|█████████▉| 1423/1436 [02:32<00:01,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  99%|█████████▉| 1424/1436 [02:32<00:01,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  99%|█████████▉| 1425/1436 [02:32<00:01,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  99%|█████████▉| 1426/1436 [02:32<00:01,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  99%|█████████▉| 1427/1436 [02:32<00:00,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11:  99%|█████████▉| 1428/1436 [02:32<00:00,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11: 100%|█████████▉| 1429/1436 [02:32<00:00,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11: 100%|█████████▉| 1430/1436 [02:32<00:00,  9.36it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11: 100%|█████████▉| 1431/1436 [02:32<00:00,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11: 100%|█████████▉| 1432/1436 [02:33<00:00,  9.36it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11: 100%|█████████▉| 1433/1436 [02:33<00:00,  9.35it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11: 100%|█████████▉| 1434/1436 [02:33<00:00,  9.36it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11: 100%|█████████▉| 1435/1436 [02:33<00:00,  9.37it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0859, train_loss_epoch=0.0706]\n",
      "Epoch 11: 100%|██████████| 1436/1436 [02:33<00:00,  9.37it/s, loss=0.0697, v_num=0, train_loss_step=0.0637, val_loss=0.0852, train_loss_epoch=0.0706]\n",
      "Epoch 12:  94%|█████████▍| 1350/1436 [02:26<00:09,  9.20it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  94%|█████████▍| 1351/1436 [02:27<00:09,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  94%|█████████▍| 1352/1436 [02:27<00:09,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  94%|█████████▍| 1353/1436 [02:27<00:09,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  94%|█████████▍| 1354/1436 [02:27<00:08,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  94%|█████████▍| 1355/1436 [02:27<00:08,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  94%|█████████▍| 1356/1436 [02:27<00:08,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  94%|█████████▍| 1357/1436 [02:27<00:08,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  95%|█████████▍| 1358/1436 [02:28<00:08,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  95%|█████████▍| 1359/1436 [02:28<00:08,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  95%|█████████▍| 1360/1436 [02:28<00:08,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  95%|█████████▍| 1361/1436 [02:28<00:08,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  95%|█████████▍| 1362/1436 [02:28<00:08,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  95%|█████████▍| 1363/1436 [02:28<00:07,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  95%|█████████▍| 1364/1436 [02:28<00:07,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  95%|█████████▌| 1365/1436 [02:28<00:07,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  95%|█████████▌| 1366/1436 [02:28<00:07,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  95%|█████████▌| 1367/1436 [02:29<00:07,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  95%|█████████▌| 1368/1436 [02:29<00:07,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  95%|█████████▌| 1369/1436 [02:29<00:07,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  95%|█████████▌| 1370/1436 [02:29<00:07,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  95%|█████████▌| 1371/1436 [02:29<00:07,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  96%|█████████▌| 1372/1436 [02:29<00:06,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  96%|█████████▌| 1373/1436 [02:29<00:06,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  96%|█████████▌| 1374/1436 [02:29<00:06,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  96%|█████████▌| 1375/1436 [02:30<00:06,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  96%|█████████▌| 1376/1436 [02:30<00:06,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  96%|█████████▌| 1377/1436 [02:30<00:06,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  96%|█████████▌| 1378/1436 [02:30<00:06,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  96%|█████████▌| 1379/1436 [02:30<00:06,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  96%|█████████▌| 1380/1436 [02:30<00:06,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  96%|█████████▌| 1381/1436 [02:30<00:05,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  96%|█████████▌| 1382/1436 [02:30<00:05,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  96%|█████████▋| 1383/1436 [02:30<00:05,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  96%|█████████▋| 1384/1436 [02:30<00:05,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  96%|█████████▋| 1385/1436 [02:31<00:05,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  97%|█████████▋| 1386/1436 [02:31<00:05,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  97%|█████████▋| 1387/1436 [02:31<00:05,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  97%|█████████▋| 1388/1436 [02:31<00:05,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  97%|█████████▋| 1389/1436 [02:31<00:05,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  97%|█████████▋| 1390/1436 [02:31<00:05,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  97%|█████████▋| 1391/1436 [02:31<00:04,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  97%|█████████▋| 1392/1436 [02:31<00:04,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  97%|█████████▋| 1393/1436 [02:31<00:04,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  97%|█████████▋| 1394/1436 [02:32<00:04,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  97%|█████████▋| 1395/1436 [02:32<00:04,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  97%|█████████▋| 1396/1436 [02:32<00:04,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  97%|█████████▋| 1397/1436 [02:32<00:04,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  97%|█████████▋| 1398/1436 [02:32<00:04,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  97%|█████████▋| 1399/1436 [02:32<00:04,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  97%|█████████▋| 1400/1436 [02:32<00:03,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  98%|█████████▊| 1401/1436 [02:32<00:03,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  98%|█████████▊| 1402/1436 [02:32<00:03,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  98%|█████████▊| 1403/1436 [02:32<00:03,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  98%|█████████▊| 1404/1436 [02:33<00:03,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  98%|█████████▊| 1405/1436 [02:33<00:03,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  98%|█████████▊| 1406/1436 [02:33<00:03,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  98%|█████████▊| 1407/1436 [02:33<00:03,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  98%|█████████▊| 1408/1436 [02:33<00:03,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  98%|█████████▊| 1409/1436 [02:33<00:02,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  98%|█████████▊| 1410/1436 [02:33<00:02,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  98%|█████████▊| 1411/1436 [02:33<00:02,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  98%|█████████▊| 1412/1436 [02:33<00:02,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  98%|█████████▊| 1413/1436 [02:34<00:02,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  98%|█████████▊| 1414/1436 [02:34<00:02,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  99%|█████████▊| 1415/1436 [02:34<00:02,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  99%|█████████▊| 1416/1436 [02:34<00:02,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  99%|█████████▊| 1417/1436 [02:34<00:02,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  99%|█████████▊| 1418/1436 [02:34<00:01,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  99%|█████████▉| 1419/1436 [02:34<00:01,  9.18it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  99%|█████████▉| 1420/1436 [02:34<00:01,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  99%|█████████▉| 1421/1436 [02:34<00:01,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  99%|█████████▉| 1422/1436 [02:35<00:01,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  99%|█████████▉| 1423/1436 [02:35<00:01,  9.18it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  99%|█████████▉| 1424/1436 [02:35<00:01,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  99%|█████████▉| 1425/1436 [02:35<00:01,  9.18it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  99%|█████████▉| 1426/1436 [02:35<00:01,  9.17it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  99%|█████████▉| 1427/1436 [02:35<00:00,  9.18it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12:  99%|█████████▉| 1428/1436 [02:35<00:00,  9.18it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12: 100%|█████████▉| 1429/1436 [02:35<00:00,  9.18it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12: 100%|█████████▉| 1430/1436 [02:35<00:00,  9.18it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12: 100%|█████████▉| 1431/1436 [02:35<00:00,  9.18it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12: 100%|█████████▉| 1432/1436 [02:36<00:00,  9.18it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12: 100%|█████████▉| 1433/1436 [02:36<00:00,  9.18it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12: 100%|█████████▉| 1434/1436 [02:36<00:00,  9.18it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12: 100%|█████████▉| 1435/1436 [02:36<00:00,  9.19it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0852, train_loss_epoch=0.0702]\n",
      "Epoch 12: 100%|██████████| 1436/1436 [02:36<00:00,  9.19it/s, loss=0.0734, v_num=0, train_loss_step=0.0616, val_loss=0.0851, train_loss_epoch=0.0702]\n",
      "Epoch 13:  94%|█████████▍| 1350/1436 [02:36<00:09,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  94%|█████████▍| 1351/1436 [02:37<00:09,  8.59it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  94%|█████████▍| 1352/1436 [02:37<00:09,  8.60it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  94%|█████████▍| 1353/1436 [02:37<00:09,  8.59it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  94%|█████████▍| 1354/1436 [02:37<00:09,  8.60it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  94%|█████████▍| 1355/1436 [02:37<00:09,  8.60it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  94%|█████████▍| 1356/1436 [02:37<00:09,  8.60it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  94%|█████████▍| 1357/1436 [02:37<00:09,  8.60it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  95%|█████████▍| 1358/1436 [02:37<00:09,  8.60it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  95%|█████████▍| 1359/1436 [02:38<00:08,  8.60it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  95%|█████████▍| 1360/1436 [02:38<00:08,  8.60it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  95%|█████████▍| 1361/1436 [02:38<00:08,  8.60it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  95%|█████████▍| 1362/1436 [02:38<00:08,  8.60it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  95%|█████████▍| 1363/1436 [02:38<00:08,  8.60it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  95%|█████████▍| 1364/1436 [02:38<00:08,  8.61it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  95%|█████████▌| 1365/1436 [02:38<00:08,  8.60it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  95%|█████████▌| 1366/1436 [02:38<00:08,  8.61it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  95%|█████████▌| 1367/1436 [02:38<00:08,  8.60it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  95%|█████████▌| 1368/1436 [02:38<00:07,  8.61it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  95%|█████████▌| 1369/1436 [02:39<00:07,  8.60it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  95%|█████████▌| 1370/1436 [02:39<00:07,  8.61it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  95%|█████████▌| 1371/1436 [02:39<00:07,  8.60it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  96%|█████████▌| 1372/1436 [02:39<00:07,  8.61it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  96%|█████████▌| 1373/1436 [02:39<00:07,  8.61it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  96%|█████████▌| 1374/1436 [02:39<00:07,  8.61it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  96%|█████████▌| 1375/1436 [02:39<00:07,  8.61it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  96%|█████████▌| 1376/1436 [02:39<00:06,  8.61it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  96%|█████████▌| 1377/1436 [02:39<00:06,  8.61it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  96%|█████████▌| 1378/1436 [02:39<00:06,  8.61it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  96%|█████████▌| 1379/1436 [02:40<00:06,  8.61it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  96%|█████████▌| 1380/1436 [02:40<00:06,  8.61it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  96%|█████████▌| 1381/1436 [02:40<00:06,  8.61it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  96%|█████████▌| 1382/1436 [02:40<00:06,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  96%|█████████▋| 1383/1436 [02:40<00:06,  8.61it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  96%|█████████▋| 1384/1436 [02:40<00:06,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  96%|█████████▋| 1385/1436 [02:40<00:05,  8.61it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  97%|█████████▋| 1386/1436 [02:40<00:05,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  97%|█████████▋| 1387/1436 [02:41<00:05,  8.61it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  97%|█████████▋| 1388/1436 [02:41<00:05,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  97%|█████████▋| 1389/1436 [02:41<00:05,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  97%|█████████▋| 1390/1436 [02:41<00:05,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  97%|█████████▋| 1391/1436 [02:41<00:05,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  97%|█████████▋| 1392/1436 [02:41<00:05,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  97%|█████████▋| 1393/1436 [02:41<00:04,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  97%|█████████▋| 1394/1436 [02:41<00:04,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  97%|█████████▋| 1395/1436 [02:41<00:04,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  97%|█████████▋| 1396/1436 [02:41<00:04,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  97%|█████████▋| 1397/1436 [02:42<00:04,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  97%|█████████▋| 1398/1436 [02:42<00:04,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  97%|█████████▋| 1399/1436 [02:42<00:04,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  97%|█████████▋| 1400/1436 [02:42<00:04,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  98%|█████████▊| 1401/1436 [02:42<00:04,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  98%|█████████▊| 1402/1436 [02:42<00:03,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  98%|█████████▊| 1403/1436 [02:42<00:03,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  98%|█████████▊| 1404/1436 [02:42<00:03,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  98%|█████████▊| 1405/1436 [02:42<00:03,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  98%|█████████▊| 1406/1436 [02:43<00:03,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  98%|█████████▊| 1407/1436 [02:43<00:03,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  98%|█████████▊| 1408/1436 [02:43<00:03,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  98%|█████████▊| 1409/1436 [02:43<00:03,  8.62it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  98%|█████████▊| 1410/1436 [02:43<00:03,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  98%|█████████▊| 1411/1436 [02:43<00:02,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  98%|█████████▊| 1412/1436 [02:43<00:02,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  98%|█████████▊| 1413/1436 [02:43<00:02,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  98%|█████████▊| 1414/1436 [02:43<00:02,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  99%|█████████▊| 1415/1436 [02:44<00:02,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  99%|█████████▊| 1416/1436 [02:44<00:02,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  99%|█████████▊| 1417/1436 [02:44<00:02,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  99%|█████████▊| 1418/1436 [02:44<00:02,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  99%|█████████▉| 1419/1436 [02:44<00:01,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  99%|█████████▉| 1420/1436 [02:44<00:01,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  99%|█████████▉| 1421/1436 [02:44<00:01,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  99%|█████████▉| 1422/1436 [02:44<00:01,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  99%|█████████▉| 1423/1436 [02:44<00:01,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  99%|█████████▉| 1424/1436 [02:44<00:01,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  99%|█████████▉| 1425/1436 [02:45<00:01,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  99%|█████████▉| 1426/1436 [02:45<00:01,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  99%|█████████▉| 1427/1436 [02:45<00:01,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13:  99%|█████████▉| 1428/1436 [02:45<00:00,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13: 100%|█████████▉| 1429/1436 [02:45<00:00,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13: 100%|█████████▉| 1430/1436 [02:45<00:00,  8.64it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13: 100%|█████████▉| 1431/1436 [02:45<00:00,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13: 100%|█████████▉| 1432/1436 [02:45<00:00,  8.64it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13: 100%|█████████▉| 1433/1436 [02:46<00:00,  8.63it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13: 100%|█████████▉| 1434/1436 [02:46<00:00,  8.64it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13: 100%|█████████▉| 1435/1436 [02:46<00:00,  8.64it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0851, train_loss_epoch=0.0698]\n",
      "Epoch 13: 100%|██████████| 1436/1436 [02:46<00:00,  8.65it/s, loss=0.0713, v_num=0, train_loss_step=0.070, val_loss=0.0861, train_loss_epoch=0.0698]\n",
      "Epoch 14:  94%|█████████▍| 1350/1436 [02:31<00:09,  8.92it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  94%|█████████▍| 1351/1436 [02:32<00:09,  8.89it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  94%|█████████▍| 1352/1436 [02:32<00:09,  8.89it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  94%|█████████▍| 1353/1436 [02:32<00:09,  8.89it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  94%|█████████▍| 1354/1436 [02:32<00:09,  8.89it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  94%|█████████▍| 1355/1436 [02:32<00:09,  8.89it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  94%|█████████▍| 1356/1436 [02:32<00:08,  8.89it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  94%|█████████▍| 1357/1436 [02:32<00:08,  8.89it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  95%|█████████▍| 1358/1436 [02:32<00:08,  8.89it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  95%|█████████▍| 1359/1436 [02:32<00:08,  8.89it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  95%|█████████▍| 1360/1436 [02:32<00:08,  8.89it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  95%|█████████▍| 1361/1436 [02:33<00:08,  8.88it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  95%|█████████▍| 1362/1436 [02:33<00:08,  8.89it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  95%|█████████▍| 1363/1436 [02:33<00:08,  8.88it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  95%|█████████▍| 1364/1436 [02:33<00:08,  8.89it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  95%|█████████▌| 1365/1436 [02:33<00:07,  8.88it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  95%|█████████▌| 1366/1436 [02:33<00:07,  8.89it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  95%|█████████▌| 1367/1436 [02:33<00:07,  8.88it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  95%|█████████▌| 1368/1436 [02:34<00:07,  8.88it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  95%|█████████▌| 1369/1436 [02:34<00:07,  8.88it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  95%|█████████▌| 1370/1436 [02:34<00:07,  8.88it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  95%|█████████▌| 1371/1436 [02:34<00:07,  8.88it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  96%|█████████▌| 1372/1436 [02:34<00:07,  8.88it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  96%|█████████▌| 1373/1436 [02:34<00:07,  8.88it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  96%|█████████▌| 1374/1436 [02:34<00:06,  8.88it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  96%|█████████▌| 1375/1436 [02:34<00:06,  8.88it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  96%|█████████▌| 1376/1436 [02:34<00:06,  8.88it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  96%|█████████▌| 1377/1436 [02:35<00:06,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  96%|█████████▌| 1378/1436 [02:35<00:06,  8.88it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  96%|█████████▌| 1379/1436 [02:35<00:06,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  96%|█████████▌| 1380/1436 [02:35<00:06,  8.88it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  96%|█████████▌| 1381/1436 [02:35<00:06,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  96%|█████████▌| 1382/1436 [02:35<00:06,  8.88it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  96%|█████████▋| 1383/1436 [02:35<00:05,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  96%|█████████▋| 1384/1436 [02:35<00:05,  8.88it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  96%|█████████▋| 1385/1436 [02:36<00:05,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  97%|█████████▋| 1386/1436 [02:36<00:05,  8.88it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  97%|█████████▋| 1387/1436 [02:36<00:05,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  97%|█████████▋| 1388/1436 [02:36<00:05,  8.88it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  97%|█████████▋| 1389/1436 [02:36<00:05,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  97%|█████████▋| 1390/1436 [02:36<00:05,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  97%|█████████▋| 1391/1436 [02:36<00:05,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  97%|█████████▋| 1392/1436 [02:36<00:04,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  97%|█████████▋| 1393/1436 [02:37<00:04,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  97%|█████████▋| 1394/1436 [02:37<00:04,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  97%|█████████▋| 1395/1436 [02:37<00:04,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  97%|█████████▋| 1396/1436 [02:37<00:04,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  97%|█████████▋| 1397/1436 [02:37<00:04,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  97%|█████████▋| 1398/1436 [02:37<00:04,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  97%|█████████▋| 1399/1436 [02:37<00:04,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  97%|█████████▋| 1400/1436 [02:37<00:04,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  98%|█████████▊| 1401/1436 [02:38<00:03,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  98%|█████████▊| 1402/1436 [02:38<00:03,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  98%|█████████▊| 1403/1436 [02:38<00:03,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  98%|█████████▊| 1404/1436 [02:38<00:03,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  98%|█████████▊| 1405/1436 [02:38<00:03,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  98%|█████████▊| 1406/1436 [02:38<00:03,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  98%|█████████▊| 1407/1436 [02:38<00:03,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  98%|█████████▊| 1408/1436 [02:38<00:03,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  98%|█████████▊| 1409/1436 [02:39<00:03,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  98%|█████████▊| 1410/1436 [02:39<00:02,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  98%|█████████▊| 1411/1436 [02:39<00:02,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  98%|█████████▊| 1412/1436 [02:39<00:02,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  98%|█████████▊| 1413/1436 [02:39<00:02,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  98%|█████████▊| 1414/1436 [02:39<00:02,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  99%|█████████▊| 1415/1436 [02:39<00:02,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  99%|█████████▊| 1416/1436 [02:39<00:02,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  99%|█████████▊| 1417/1436 [02:39<00:02,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  99%|█████████▊| 1418/1436 [02:40<00:02,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  99%|█████████▉| 1419/1436 [02:40<00:01,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  99%|█████████▉| 1420/1436 [02:40<00:01,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  99%|█████████▉| 1421/1436 [02:40<00:01,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  99%|█████████▉| 1422/1436 [02:40<00:01,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  99%|█████████▉| 1423/1436 [02:40<00:01,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  99%|█████████▉| 1424/1436 [02:40<00:01,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  99%|█████████▉| 1425/1436 [02:40<00:01,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  99%|█████████▉| 1426/1436 [02:40<00:01,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  99%|█████████▉| 1427/1436 [02:41<00:01,  8.85it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14:  99%|█████████▉| 1428/1436 [02:41<00:00,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14: 100%|█████████▉| 1429/1436 [02:41<00:00,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14: 100%|█████████▉| 1430/1436 [02:41<00:00,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14: 100%|█████████▉| 1431/1436 [02:41<00:00,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14: 100%|█████████▉| 1432/1436 [02:41<00:00,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14: 100%|█████████▉| 1433/1436 [02:41<00:00,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14: 100%|█████████▉| 1434/1436 [02:41<00:00,  8.86it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14: 100%|█████████▉| 1435/1436 [02:41<00:00,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0861, train_loss_epoch=0.0693]\n",
      "Epoch 14: 100%|██████████| 1436/1436 [02:41<00:00,  8.87it/s, loss=0.0643, v_num=0, train_loss_step=0.0462, val_loss=0.0871, train_loss_epoch=0.0693]\n",
      "Epoch 15:  94%|█████████▍| 1350/1436 [02:28<00:09,  9.09it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  94%|█████████▍| 1351/1436 [02:29<00:09,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  94%|█████████▍| 1352/1436 [02:29<00:09,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  94%|█████████▍| 1353/1436 [02:29<00:09,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  94%|█████████▍| 1354/1436 [02:29<00:09,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  94%|█████████▍| 1355/1436 [02:29<00:08,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  94%|█████████▍| 1356/1436 [02:29<00:08,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  94%|█████████▍| 1357/1436 [02:29<00:08,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  95%|█████████▍| 1358/1436 [02:29<00:08,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  95%|█████████▍| 1359/1436 [02:30<00:08,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  95%|█████████▍| 1360/1436 [02:30<00:08,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  95%|█████████▍| 1361/1436 [02:30<00:08,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  95%|█████████▍| 1362/1436 [02:30<00:08,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  95%|█████████▍| 1363/1436 [02:30<00:08,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  95%|█████████▍| 1364/1436 [02:30<00:07,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  95%|█████████▌| 1365/1436 [02:30<00:07,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  95%|█████████▌| 1366/1436 [02:30<00:07,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  95%|█████████▌| 1367/1436 [02:30<00:07,  9.05it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  95%|█████████▌| 1368/1436 [02:31<00:07,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  95%|█████████▌| 1369/1436 [02:31<00:07,  9.05it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  95%|█████████▌| 1370/1436 [02:31<00:07,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  95%|█████████▌| 1371/1436 [02:31<00:07,  9.05it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  96%|█████████▌| 1372/1436 [02:31<00:07,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  96%|█████████▌| 1373/1436 [02:31<00:06,  9.05it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  96%|█████████▌| 1374/1436 [02:31<00:06,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  96%|█████████▌| 1375/1436 [02:31<00:06,  9.05it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  96%|█████████▌| 1376/1436 [02:31<00:06,  9.06it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  96%|█████████▌| 1377/1436 [02:32<00:06,  9.05it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  96%|█████████▌| 1378/1436 [02:32<00:06,  9.05it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  96%|█████████▌| 1379/1436 [02:32<00:06,  9.05it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  96%|█████████▌| 1380/1436 [02:32<00:06,  9.05it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  96%|█████████▌| 1381/1436 [02:32<00:06,  9.05it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  96%|█████████▌| 1382/1436 [02:32<00:05,  9.05it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  96%|█████████▋| 1383/1436 [02:32<00:05,  9.05it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  96%|█████████▋| 1384/1436 [02:32<00:05,  9.05it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  96%|█████████▋| 1385/1436 [02:33<00:05,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  97%|█████████▋| 1386/1436 [02:33<00:05,  9.05it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  97%|█████████▋| 1387/1436 [02:33<00:05,  9.05it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  97%|█████████▋| 1388/1436 [02:33<00:05,  9.05it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  97%|█████████▋| 1389/1436 [02:33<00:05,  9.05it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  97%|█████████▋| 1390/1436 [02:33<00:05,  9.05it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  97%|█████████▋| 1391/1436 [02:33<00:04,  9.05it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  97%|█████████▋| 1392/1436 [02:33<00:04,  9.05it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  97%|█████████▋| 1393/1436 [02:34<00:04,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  97%|█████████▋| 1394/1436 [02:34<00:04,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  97%|█████████▋| 1395/1436 [02:34<00:04,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  97%|█████████▋| 1396/1436 [02:34<00:04,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  97%|█████████▋| 1397/1436 [02:34<00:04,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  97%|█████████▋| 1398/1436 [02:34<00:04,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  97%|█████████▋| 1399/1436 [02:34<00:04,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  97%|█████████▋| 1400/1436 [02:34<00:03,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  98%|█████████▊| 1401/1436 [02:35<00:03,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  98%|█████████▊| 1402/1436 [02:35<00:03,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  98%|█████████▊| 1403/1436 [02:35<00:03,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  98%|█████████▊| 1404/1436 [02:35<00:03,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  98%|█████████▊| 1405/1436 [02:35<00:03,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  98%|█████████▊| 1406/1436 [02:35<00:03,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  98%|█████████▊| 1407/1436 [02:35<00:03,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  98%|█████████▊| 1408/1436 [02:35<00:03,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  98%|█████████▊| 1409/1436 [02:35<00:02,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  98%|█████████▊| 1410/1436 [02:35<00:02,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  98%|█████████▊| 1411/1436 [02:36<00:02,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  98%|█████████▊| 1412/1436 [02:36<00:02,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  98%|█████████▊| 1413/1436 [02:36<00:02,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  98%|█████████▊| 1414/1436 [02:36<00:02,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  99%|█████████▊| 1415/1436 [02:36<00:02,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  99%|█████████▊| 1416/1436 [02:36<00:02,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  99%|█████████▊| 1417/1436 [02:36<00:02,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  99%|█████████▊| 1418/1436 [02:36<00:01,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  99%|█████████▉| 1419/1436 [02:37<00:01,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  99%|█████████▉| 1420/1436 [02:37<00:01,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  99%|█████████▉| 1421/1436 [02:37<00:01,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  99%|█████████▉| 1422/1436 [02:37<00:01,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  99%|█████████▉| 1423/1436 [02:37<00:01,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  99%|█████████▉| 1424/1436 [02:37<00:01,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  99%|█████████▉| 1425/1436 [02:37<00:01,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  99%|█████████▉| 1426/1436 [02:37<00:01,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  99%|█████████▉| 1427/1436 [02:38<00:00,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15:  99%|█████████▉| 1428/1436 [02:38<00:00,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15: 100%|█████████▉| 1429/1436 [02:38<00:00,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15: 100%|█████████▉| 1430/1436 [02:38<00:00,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15: 100%|█████████▉| 1431/1436 [02:38<00:00,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15: 100%|█████████▉| 1432/1436 [02:38<00:00,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15: 100%|█████████▉| 1433/1436 [02:38<00:00,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15: 100%|█████████▉| 1434/1436 [02:38<00:00,  9.03it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15: 100%|█████████▉| 1435/1436 [02:38<00:00,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0871, train_loss_epoch=0.0691]\n",
      "Epoch 15: 100%|██████████| 1436/1436 [02:38<00:00,  9.04it/s, loss=0.0705, v_num=0, train_loss_step=0.0556, val_loss=0.0841, train_loss_epoch=0.0691]\n",
      "Epoch 16:  94%|█████████▍| 1350/1436 [02:32<00:09,  8.87it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  94%|█████████▍| 1351/1436 [02:32<00:09,  8.84it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  94%|█████████▍| 1352/1436 [02:32<00:09,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  94%|█████████▍| 1353/1436 [02:32<00:09,  8.84it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  94%|█████████▍| 1354/1436 [02:32<00:09,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  94%|█████████▍| 1355/1436 [02:33<00:09,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  94%|█████████▍| 1356/1436 [02:33<00:09,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  94%|█████████▍| 1357/1436 [02:33<00:08,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  95%|█████████▍| 1358/1436 [02:33<00:08,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  95%|█████████▍| 1359/1436 [02:33<00:08,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  95%|█████████▍| 1360/1436 [02:33<00:08,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  95%|█████████▍| 1361/1436 [02:33<00:08,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  95%|█████████▍| 1362/1436 [02:33<00:08,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  95%|█████████▍| 1363/1436 [02:34<00:08,  8.84it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  95%|█████████▍| 1364/1436 [02:34<00:08,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  95%|█████████▌| 1365/1436 [02:34<00:08,  8.84it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  95%|█████████▌| 1366/1436 [02:34<00:07,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  95%|█████████▌| 1367/1436 [02:34<00:07,  8.84it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  95%|█████████▌| 1368/1436 [02:34<00:07,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  95%|█████████▌| 1369/1436 [02:34<00:07,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  95%|█████████▌| 1370/1436 [02:34<00:07,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  95%|█████████▌| 1371/1436 [02:35<00:07,  8.84it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  96%|█████████▌| 1372/1436 [02:35<00:07,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  96%|█████████▌| 1373/1436 [02:35<00:07,  8.84it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  96%|█████████▌| 1374/1436 [02:35<00:07,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  96%|█████████▌| 1375/1436 [02:35<00:06,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  96%|█████████▌| 1376/1436 [02:35<00:06,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  96%|█████████▌| 1377/1436 [02:35<00:06,  8.84it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  96%|█████████▌| 1378/1436 [02:35<00:06,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  96%|█████████▌| 1379/1436 [02:35<00:06,  8.84it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  96%|█████████▌| 1380/1436 [02:35<00:06,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  96%|█████████▌| 1381/1436 [02:36<00:06,  8.84it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  96%|█████████▌| 1382/1436 [02:36<00:06,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  96%|█████████▋| 1383/1436 [02:36<00:05,  8.84it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  96%|█████████▋| 1384/1436 [02:36<00:05,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  96%|█████████▋| 1385/1436 [02:36<00:05,  8.84it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  97%|█████████▋| 1386/1436 [02:36<00:05,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  97%|█████████▋| 1387/1436 [02:36<00:05,  8.84it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  97%|█████████▋| 1388/1436 [02:36<00:05,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  97%|█████████▋| 1389/1436 [02:37<00:05,  8.84it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  97%|█████████▋| 1390/1436 [02:37<00:05,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  97%|█████████▋| 1391/1436 [02:37<00:05,  8.84it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  97%|█████████▋| 1392/1436 [02:37<00:04,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  97%|█████████▋| 1393/1436 [02:37<00:04,  8.84it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  97%|█████████▋| 1394/1436 [02:37<00:04,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  97%|█████████▋| 1395/1436 [02:37<00:04,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  97%|█████████▋| 1396/1436 [02:37<00:04,  8.84it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  97%|█████████▋| 1397/1436 [02:37<00:04,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  97%|█████████▋| 1398/1436 [02:38<00:04,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  97%|█████████▋| 1399/1436 [02:38<00:04,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  97%|█████████▋| 1400/1436 [02:38<00:04,  8.84it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  98%|█████████▊| 1401/1436 [02:38<00:03,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  98%|█████████▊| 1402/1436 [02:38<00:03,  8.84it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  98%|█████████▊| 1403/1436 [02:38<00:03,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  98%|█████████▊| 1404/1436 [02:38<00:03,  8.84it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  98%|█████████▊| 1405/1436 [02:38<00:03,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  98%|█████████▊| 1406/1436 [02:38<00:03,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  98%|█████████▊| 1407/1436 [02:39<00:03,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  98%|█████████▊| 1408/1436 [02:39<00:03,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  98%|█████████▊| 1409/1436 [02:39<00:03,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  98%|█████████▊| 1410/1436 [02:39<00:02,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  98%|█████████▊| 1411/1436 [02:39<00:02,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  98%|█████████▊| 1412/1436 [02:39<00:02,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  98%|█████████▊| 1413/1436 [02:39<00:02,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  98%|█████████▊| 1414/1436 [02:39<00:02,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  99%|█████████▊| 1415/1436 [02:39<00:02,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  99%|█████████▊| 1416/1436 [02:40<00:02,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  99%|█████████▊| 1417/1436 [02:40<00:02,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  99%|█████████▊| 1418/1436 [02:40<00:02,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  99%|█████████▉| 1419/1436 [02:40<00:01,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  99%|█████████▉| 1420/1436 [02:40<00:01,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  99%|█████████▉| 1421/1436 [02:40<00:01,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  99%|█████████▉| 1422/1436 [02:40<00:01,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  99%|█████████▉| 1423/1436 [02:40<00:01,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  99%|█████████▉| 1424/1436 [02:40<00:01,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  99%|█████████▉| 1425/1436 [02:41<00:01,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  99%|█████████▉| 1426/1436 [02:41<00:01,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  99%|█████████▉| 1427/1436 [02:41<00:01,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16:  99%|█████████▉| 1428/1436 [02:41<00:00,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16: 100%|█████████▉| 1429/1436 [02:41<00:00,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16: 100%|█████████▉| 1430/1436 [02:41<00:00,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16: 100%|█████████▉| 1431/1436 [02:41<00:00,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16: 100%|█████████▉| 1432/1436 [02:41<00:00,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16: 100%|█████████▉| 1433/1436 [02:41<00:00,  8.85it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16: 100%|█████████▉| 1434/1436 [02:41<00:00,  8.86it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16: 100%|█████████▉| 1435/1436 [02:41<00:00,  8.86it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0841, train_loss_epoch=0.0686]\n",
      "Epoch 16: 100%|██████████| 1436/1436 [02:41<00:00,  8.87it/s, loss=0.0658, v_num=0, train_loss_step=0.065, val_loss=0.0855, train_loss_epoch=0.0686]\n",
      "Epoch 17:  94%|█████████▍| 1350/1436 [02:59<00:11,  7.52it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  94%|█████████▍| 1351/1436 [03:00<00:11,  7.49it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  94%|█████████▍| 1352/1436 [03:00<00:11,  7.49it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  94%|█████████▍| 1353/1436 [03:00<00:11,  7.49it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  94%|█████████▍| 1354/1436 [03:00<00:10,  7.49it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  94%|█████████▍| 1355/1436 [03:00<00:10,  7.49it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  94%|█████████▍| 1356/1436 [03:01<00:10,  7.49it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  94%|█████████▍| 1357/1436 [03:01<00:10,  7.49it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  95%|█████████▍| 1358/1436 [03:01<00:10,  7.49it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  95%|█████████▍| 1359/1436 [03:01<00:10,  7.48it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  95%|█████████▍| 1360/1436 [03:01<00:10,  7.49it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  95%|█████████▍| 1361/1436 [03:01<00:10,  7.49it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  95%|█████████▍| 1362/1436 [03:01<00:09,  7.49it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  95%|█████████▍| 1363/1436 [03:02<00:09,  7.48it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  95%|█████████▍| 1364/1436 [03:02<00:09,  7.48it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  95%|█████████▌| 1365/1436 [03:02<00:09,  7.47it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  95%|█████████▌| 1366/1436 [03:02<00:09,  7.48it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  95%|█████████▌| 1367/1436 [03:02<00:09,  7.47it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  95%|█████████▌| 1368/1436 [03:03<00:09,  7.47it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  95%|█████████▌| 1369/1436 [03:03<00:08,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  95%|█████████▌| 1370/1436 [03:03<00:08,  7.47it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  95%|█████████▌| 1371/1436 [03:03<00:08,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  96%|█████████▌| 1372/1436 [03:03<00:08,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  96%|█████████▌| 1373/1436 [03:03<00:08,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  96%|█████████▌| 1374/1436 [03:04<00:08,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  96%|█████████▌| 1375/1436 [03:04<00:08,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  96%|█████████▌| 1376/1436 [03:04<00:08,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  96%|█████████▌| 1377/1436 [03:04<00:07,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  96%|█████████▌| 1378/1436 [03:04<00:07,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  96%|█████████▌| 1379/1436 [03:04<00:07,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  96%|█████████▌| 1380/1436 [03:04<00:07,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  96%|█████████▌| 1381/1436 [03:05<00:07,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  96%|█████████▌| 1382/1436 [03:05<00:07,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  96%|█████████▋| 1383/1436 [03:05<00:07,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  96%|█████████▋| 1384/1436 [03:05<00:06,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  96%|█████████▋| 1385/1436 [03:05<00:06,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  97%|█████████▋| 1386/1436 [03:05<00:06,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  97%|█████████▋| 1387/1436 [03:06<00:06,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  97%|█████████▋| 1388/1436 [03:06<00:06,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  97%|█████████▋| 1389/1436 [03:06<00:06,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  97%|█████████▋| 1390/1436 [03:06<00:06,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  97%|█████████▋| 1391/1436 [03:06<00:06,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  97%|█████████▋| 1392/1436 [03:06<00:05,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  97%|█████████▋| 1393/1436 [03:07<00:05,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  97%|█████████▋| 1394/1436 [03:07<00:05,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  97%|█████████▋| 1395/1436 [03:07<00:05,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  97%|█████████▋| 1396/1436 [03:07<00:05,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  97%|█████████▋| 1397/1436 [03:07<00:05,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  97%|█████████▋| 1398/1436 [03:07<00:05,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  97%|█████████▋| 1399/1436 [03:07<00:04,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  97%|█████████▋| 1400/1436 [03:07<00:04,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  98%|█████████▊| 1401/1436 [03:07<00:04,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  98%|█████████▊| 1402/1436 [03:08<00:04,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  98%|█████████▊| 1403/1436 [03:08<00:04,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  98%|█████████▊| 1404/1436 [03:08<00:04,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  98%|█████████▊| 1405/1436 [03:08<00:04,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  98%|█████████▊| 1406/1436 [03:08<00:04,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  98%|█████████▊| 1407/1436 [03:08<00:03,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  98%|█████████▊| 1408/1436 [03:08<00:03,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  98%|█████████▊| 1409/1436 [03:09<00:03,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  98%|█████████▊| 1410/1436 [03:09<00:03,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  98%|█████████▊| 1411/1436 [03:09<00:03,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  98%|█████████▊| 1412/1436 [03:09<00:03,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  98%|█████████▊| 1413/1436 [03:09<00:03,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  98%|█████████▊| 1414/1436 [03:09<00:02,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  99%|█████████▊| 1415/1436 [03:09<00:02,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  99%|█████████▊| 1416/1436 [03:09<00:02,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  99%|█████████▊| 1417/1436 [03:10<00:02,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  99%|█████████▊| 1418/1436 [03:10<00:02,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  99%|█████████▉| 1419/1436 [03:10<00:02,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  99%|█████████▉| 1420/1436 [03:10<00:02,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  99%|█████████▉| 1421/1436 [03:10<00:02,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  99%|█████████▉| 1422/1436 [03:10<00:01,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  99%|█████████▉| 1423/1436 [03:10<00:01,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  99%|█████████▉| 1424/1436 [03:10<00:01,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  99%|█████████▉| 1425/1436 [03:11<00:01,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  99%|█████████▉| 1426/1436 [03:11<00:01,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  99%|█████████▉| 1427/1436 [03:11<00:01,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17:  99%|█████████▉| 1428/1436 [03:11<00:01,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17: 100%|█████████▉| 1429/1436 [03:11<00:00,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17: 100%|█████████▉| 1430/1436 [03:11<00:00,  7.45it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17: 100%|█████████▉| 1431/1436 [03:11<00:00,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17: 100%|█████████▉| 1432/1436 [03:12<00:00,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17: 100%|█████████▉| 1433/1436 [03:12<00:00,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17: 100%|█████████▉| 1434/1436 [03:12<00:00,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17: 100%|█████████▉| 1435/1436 [03:12<00:00,  7.46it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0855, train_loss_epoch=0.0683]\n",
      "Epoch 17: 100%|██████████| 1436/1436 [03:12<00:00,  7.47it/s, loss=0.0696, v_num=0, train_loss_step=0.0742, val_loss=0.0832, train_loss_epoch=0.0683]\n",
      "Epoch 18:  94%|█████████▍| 1350/1436 [02:30<00:09,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  94%|█████████▍| 1351/1436 [02:31<00:09,  8.94it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  94%|█████████▍| 1352/1436 [02:31<00:09,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  94%|█████████▍| 1353/1436 [02:31<00:09,  8.94it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  94%|█████████▍| 1354/1436 [02:31<00:09,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  94%|█████████▍| 1355/1436 [02:31<00:09,  8.94it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  94%|█████████▍| 1356/1436 [02:31<00:08,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  94%|█████████▍| 1357/1436 [02:31<00:08,  8.94it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  95%|█████████▍| 1358/1436 [02:31<00:08,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  95%|█████████▍| 1359/1436 [02:31<00:08,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  95%|█████████▍| 1360/1436 [02:31<00:08,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  95%|█████████▍| 1361/1436 [02:32<00:08,  8.94it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  95%|█████████▍| 1362/1436 [02:32<00:08,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  95%|█████████▍| 1363/1436 [02:32<00:08,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  95%|█████████▍| 1364/1436 [02:32<00:08,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  95%|█████████▌| 1365/1436 [02:32<00:07,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  95%|█████████▌| 1366/1436 [02:32<00:07,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  95%|█████████▌| 1367/1436 [02:32<00:07,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  95%|█████████▌| 1368/1436 [02:32<00:07,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  95%|█████████▌| 1369/1436 [02:32<00:07,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  95%|█████████▌| 1370/1436 [02:33<00:07,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  95%|█████████▌| 1371/1436 [02:33<00:07,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  96%|█████████▌| 1372/1436 [02:33<00:07,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  96%|█████████▌| 1373/1436 [02:33<00:07,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  96%|█████████▌| 1374/1436 [02:33<00:06,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  96%|█████████▌| 1375/1436 [02:33<00:06,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  96%|█████████▌| 1376/1436 [02:33<00:06,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  96%|█████████▌| 1377/1436 [02:33<00:06,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  96%|█████████▌| 1378/1436 [02:33<00:06,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  96%|█████████▌| 1379/1436 [02:34<00:06,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  96%|█████████▌| 1380/1436 [02:34<00:06,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  96%|█████████▌| 1381/1436 [02:34<00:06,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  96%|█████████▌| 1382/1436 [02:34<00:06,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  96%|█████████▋| 1383/1436 [02:34<00:05,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  96%|█████████▋| 1384/1436 [02:34<00:05,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  96%|█████████▋| 1385/1436 [02:34<00:05,  8.95it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  97%|█████████▋| 1386/1436 [02:34<00:05,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  97%|█████████▋| 1387/1436 [02:34<00:05,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  97%|█████████▋| 1388/1436 [02:34<00:05,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  97%|█████████▋| 1389/1436 [02:35<00:05,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  97%|█████████▋| 1390/1436 [02:35<00:05,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  97%|█████████▋| 1391/1436 [02:35<00:05,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  97%|█████████▋| 1392/1436 [02:35<00:04,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  97%|█████████▋| 1393/1436 [02:35<00:04,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  97%|█████████▋| 1394/1436 [02:35<00:04,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  97%|█████████▋| 1395/1436 [02:35<00:04,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  97%|█████████▋| 1396/1436 [02:35<00:04,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  97%|█████████▋| 1397/1436 [02:35<00:04,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  97%|█████████▋| 1398/1436 [02:36<00:04,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  97%|█████████▋| 1399/1436 [02:36<00:04,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  97%|█████████▋| 1400/1436 [02:36<00:04,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  98%|█████████▊| 1401/1436 [02:36<00:03,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  98%|█████████▊| 1402/1436 [02:36<00:03,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  98%|█████████▊| 1403/1436 [02:36<00:03,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  98%|█████████▊| 1404/1436 [02:36<00:03,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  98%|█████████▊| 1405/1436 [02:36<00:03,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  98%|█████████▊| 1406/1436 [02:36<00:03,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  98%|█████████▊| 1407/1436 [02:36<00:03,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  98%|█████████▊| 1408/1436 [02:37<00:03,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  98%|█████████▊| 1409/1436 [02:37<00:03,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  98%|█████████▊| 1410/1436 [02:37<00:02,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  98%|█████████▊| 1411/1436 [02:37<00:02,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  98%|█████████▊| 1412/1436 [02:37<00:02,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  98%|█████████▊| 1413/1436 [02:37<00:02,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  98%|█████████▊| 1414/1436 [02:37<00:02,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  99%|█████████▊| 1415/1436 [02:37<00:02,  8.96it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  99%|█████████▊| 1416/1436 [02:37<00:02,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  99%|█████████▊| 1417/1436 [02:38<00:02,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  99%|█████████▊| 1418/1436 [02:38<00:02,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  99%|█████████▉| 1419/1436 [02:38<00:01,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  99%|█████████▉| 1420/1436 [02:38<00:01,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  99%|█████████▉| 1421/1436 [02:38<00:01,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  99%|█████████▉| 1422/1436 [02:38<00:01,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  99%|█████████▉| 1423/1436 [02:38<00:01,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  99%|█████████▉| 1424/1436 [02:38<00:01,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  99%|█████████▉| 1425/1436 [02:38<00:01,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  99%|█████████▉| 1426/1436 [02:39<00:01,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  99%|█████████▉| 1427/1436 [02:39<00:01,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18:  99%|█████████▉| 1428/1436 [02:39<00:00,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18: 100%|█████████▉| 1429/1436 [02:39<00:00,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18: 100%|█████████▉| 1430/1436 [02:39<00:00,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18: 100%|█████████▉| 1431/1436 [02:39<00:00,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18: 100%|█████████▉| 1432/1436 [02:39<00:00,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18: 100%|█████████▉| 1433/1436 [02:39<00:00,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18: 100%|█████████▉| 1434/1436 [02:39<00:00,  8.97it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18: 100%|█████████▉| 1435/1436 [02:39<00:00,  8.98it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.0832, train_loss_epoch=0.068]\n",
      "Epoch 18: 100%|██████████| 1436/1436 [02:39<00:00,  8.98it/s, loss=0.0695, v_num=0, train_loss_step=0.0675, val_loss=0.083, train_loss_epoch=0.068] \n",
      "Epoch 19:  94%|█████████▍| 1350/1436 [02:22<00:09,  9.49it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  94%|█████████▍| 1351/1436 [02:22<00:08,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  94%|█████████▍| 1352/1436 [02:22<00:08,  9.46it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  94%|█████████▍| 1353/1436 [02:23<00:08,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  94%|█████████▍| 1354/1436 [02:23<00:08,  9.46it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  94%|█████████▍| 1355/1436 [02:23<00:08,  9.46it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  94%|█████████▍| 1356/1436 [02:23<00:08,  9.46it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  94%|█████████▍| 1357/1436 [02:23<00:08,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  95%|█████████▍| 1358/1436 [02:23<00:08,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  95%|█████████▍| 1359/1436 [02:23<00:08,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  95%|█████████▍| 1360/1436 [02:23<00:08,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  95%|█████████▍| 1361/1436 [02:24<00:07,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  95%|█████████▍| 1362/1436 [02:24<00:07,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  95%|█████████▍| 1363/1436 [02:24<00:07,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  95%|█████████▍| 1364/1436 [02:24<00:07,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  95%|█████████▌| 1365/1436 [02:24<00:07,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  95%|█████████▌| 1366/1436 [02:24<00:07,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  95%|█████████▌| 1367/1436 [02:24<00:07,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  95%|█████████▌| 1368/1436 [02:24<00:07,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  95%|█████████▌| 1369/1436 [02:24<00:07,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  95%|█████████▌| 1370/1436 [02:24<00:06,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  95%|█████████▌| 1371/1436 [02:25<00:06,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  96%|█████████▌| 1372/1436 [02:25<00:06,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  96%|█████████▌| 1373/1436 [02:25<00:06,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  96%|█████████▌| 1374/1436 [02:25<00:06,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  96%|█████████▌| 1375/1436 [02:25<00:06,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  96%|█████████▌| 1376/1436 [02:25<00:06,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  96%|█████████▌| 1377/1436 [02:25<00:06,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  96%|█████████▌| 1378/1436 [02:25<00:06,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  96%|█████████▌| 1379/1436 [02:26<00:06,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  96%|█████████▌| 1380/1436 [02:26<00:05,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  96%|█████████▌| 1381/1436 [02:26<00:05,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  96%|█████████▌| 1382/1436 [02:26<00:05,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  96%|█████████▋| 1383/1436 [02:26<00:05,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  96%|█████████▋| 1384/1436 [02:26<00:05,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  96%|█████████▋| 1385/1436 [02:26<00:05,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  97%|█████████▋| 1386/1436 [02:26<00:05,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  97%|█████████▋| 1387/1436 [02:26<00:05,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  97%|█████████▋| 1388/1436 [02:26<00:05,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  97%|█████████▋| 1389/1436 [02:27<00:04,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  97%|█████████▋| 1390/1436 [02:27<00:04,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  97%|█████████▋| 1391/1436 [02:27<00:04,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  97%|█████████▋| 1392/1436 [02:27<00:04,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  97%|█████████▋| 1393/1436 [02:27<00:04,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  97%|█████████▋| 1394/1436 [02:27<00:04,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  97%|█████████▋| 1395/1436 [02:27<00:04,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  97%|█████████▋| 1396/1436 [02:27<00:04,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  97%|█████████▋| 1397/1436 [02:28<00:04,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  97%|█████████▋| 1398/1436 [02:28<00:04,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  97%|█████████▋| 1399/1436 [02:28<00:03,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  97%|█████████▋| 1400/1436 [02:28<00:03,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  98%|█████████▊| 1401/1436 [02:28<00:03,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  98%|█████████▊| 1402/1436 [02:28<00:03,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  98%|█████████▊| 1403/1436 [02:28<00:03,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  98%|█████████▊| 1404/1436 [02:28<00:03,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  98%|█████████▊| 1405/1436 [02:28<00:03,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  98%|█████████▊| 1406/1436 [02:28<00:03,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  98%|█████████▊| 1407/1436 [02:29<00:03,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  98%|█████████▊| 1408/1436 [02:29<00:02,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  98%|█████████▊| 1409/1436 [02:29<00:02,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  98%|█████████▊| 1410/1436 [02:29<00:02,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  98%|█████████▊| 1411/1436 [02:29<00:02,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  98%|█████████▊| 1412/1436 [02:29<00:02,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  98%|█████████▊| 1413/1436 [02:29<00:02,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  98%|█████████▊| 1414/1436 [02:29<00:02,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  99%|█████████▊| 1415/1436 [02:29<00:02,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  99%|█████████▊| 1416/1436 [02:29<00:02,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  99%|█████████▊| 1417/1436 [02:30<00:02,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  99%|█████████▊| 1418/1436 [02:30<00:01,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  99%|█████████▉| 1419/1436 [02:30<00:01,  9.44it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  99%|█████████▉| 1420/1436 [02:30<00:01,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  99%|█████████▉| 1421/1436 [02:30<00:01,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  99%|█████████▉| 1422/1436 [02:30<00:01,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  99%|█████████▉| 1423/1436 [02:30<00:01,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  99%|█████████▉| 1424/1436 [02:30<00:01,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  99%|█████████▉| 1425/1436 [02:30<00:01,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  99%|█████████▉| 1426/1436 [02:30<00:01,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  99%|█████████▉| 1427/1436 [02:31<00:00,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19:  99%|█████████▉| 1428/1436 [02:31<00:00,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19: 100%|█████████▉| 1429/1436 [02:31<00:00,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19: 100%|█████████▉| 1430/1436 [02:31<00:00,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19: 100%|█████████▉| 1431/1436 [02:31<00:00,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19: 100%|█████████▉| 1432/1436 [02:31<00:00,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19: 100%|█████████▉| 1433/1436 [02:31<00:00,  9.45it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19: 100%|█████████▉| 1434/1436 [02:31<00:00,  9.46it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19: 100%|█████████▉| 1435/1436 [02:31<00:00,  9.46it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.083, train_loss_epoch=0.0676]\n",
      "Epoch 19: 100%|██████████| 1436/1436 [02:31<00:00,  9.47it/s, loss=0.0734, v_num=0, train_loss_step=0.0751, val_loss=0.0848, train_loss_epoch=0.0676]\n",
      "Epoch 20:  94%|█████████▍| 1350/1436 [02:21<00:09,  9.54it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20:  94%|█████████▍| 1351/1436 [02:22<00:08,  9.51it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  94%|█████████▍| 1352/1436 [02:22<00:08,  9.51it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  94%|█████████▍| 1353/1436 [02:22<00:08,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  94%|█████████▍| 1354/1436 [02:22<00:08,  9.51it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  94%|█████████▍| 1355/1436 [02:22<00:08,  9.51it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  94%|█████████▍| 1356/1436 [02:22<00:08,  9.51it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  94%|█████████▍| 1357/1436 [02:22<00:08,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  95%|█████████▍| 1358/1436 [02:22<00:08,  9.51it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  95%|█████████▍| 1359/1436 [02:23<00:08,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  95%|█████████▍| 1360/1436 [02:23<00:07,  9.51it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  95%|█████████▍| 1361/1436 [02:23<00:07,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  95%|█████████▍| 1362/1436 [02:23<00:07,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  95%|█████████▍| 1363/1436 [02:23<00:07,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  95%|█████████▍| 1364/1436 [02:23<00:07,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  95%|█████████▌| 1365/1436 [02:23<00:07,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  95%|█████████▌| 1366/1436 [02:23<00:07,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  95%|█████████▌| 1367/1436 [02:23<00:07,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  95%|█████████▌| 1368/1436 [02:23<00:07,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  95%|█████████▌| 1369/1436 [02:24<00:07,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  95%|█████████▌| 1370/1436 [02:24<00:06,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  95%|█████████▌| 1371/1436 [02:24<00:06,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  96%|█████████▌| 1372/1436 [02:24<00:06,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  96%|█████████▌| 1373/1436 [02:24<00:06,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  96%|█████████▌| 1374/1436 [02:24<00:06,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  96%|█████████▌| 1375/1436 [02:24<00:06,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  96%|█████████▌| 1376/1436 [02:24<00:06,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  96%|█████████▌| 1377/1436 [02:24<00:06,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  96%|█████████▌| 1378/1436 [02:25<00:06,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  96%|█████████▌| 1379/1436 [02:25<00:06,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  96%|█████████▌| 1380/1436 [02:25<00:05,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  96%|█████████▌| 1381/1436 [02:25<00:05,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  96%|█████████▌| 1382/1436 [02:25<00:05,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  96%|█████████▋| 1383/1436 [02:25<00:05,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  96%|█████████▋| 1384/1436 [02:25<00:05,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  96%|█████████▋| 1385/1436 [02:25<00:05,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  97%|█████████▋| 1386/1436 [02:25<00:05,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  97%|█████████▋| 1387/1436 [02:26<00:05,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  97%|█████████▋| 1388/1436 [02:26<00:05,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  97%|█████████▋| 1389/1436 [02:26<00:04,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  97%|█████████▋| 1390/1436 [02:26<00:04,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  97%|█████████▋| 1391/1436 [02:26<00:04,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  97%|█████████▋| 1392/1436 [02:26<00:04,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  97%|█████████▋| 1393/1436 [02:26<00:04,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  97%|█████████▋| 1394/1436 [02:26<00:04,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  97%|█████████▋| 1395/1436 [02:26<00:04,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  97%|█████████▋| 1396/1436 [02:27<00:04,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  97%|█████████▋| 1397/1436 [02:27<00:04,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  97%|█████████▋| 1398/1436 [02:27<00:04,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  97%|█████████▋| 1399/1436 [02:27<00:03,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  97%|█████████▋| 1400/1436 [02:27<00:03,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  98%|█████████▊| 1401/1436 [02:27<00:03,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  98%|█████████▊| 1402/1436 [02:27<00:03,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  98%|█████████▊| 1403/1436 [02:27<00:03,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  98%|█████████▊| 1404/1436 [02:27<00:03,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  98%|█████████▊| 1405/1436 [02:28<00:03,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  98%|█████████▊| 1406/1436 [02:28<00:03,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  98%|█████████▊| 1407/1436 [02:28<00:03,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  98%|█████████▊| 1408/1436 [02:28<00:02,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  98%|█████████▊| 1409/1436 [02:28<00:02,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  98%|█████████▊| 1410/1436 [02:28<00:02,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  98%|█████████▊| 1411/1436 [02:28<00:02,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  98%|█████████▊| 1412/1436 [02:28<00:02,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  98%|█████████▊| 1413/1436 [02:28<00:02,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  98%|█████████▊| 1414/1436 [02:28<00:02,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  99%|█████████▊| 1415/1436 [02:29<00:02,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  99%|█████████▊| 1416/1436 [02:29<00:02,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  99%|█████████▊| 1417/1436 [02:29<00:02,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  99%|█████████▊| 1418/1436 [02:29<00:01,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  99%|█████████▉| 1419/1436 [02:29<00:01,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  99%|█████████▉| 1420/1436 [02:29<00:01,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  99%|█████████▉| 1421/1436 [02:29<00:01,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  99%|█████████▉| 1422/1436 [02:29<00:01,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  99%|█████████▉| 1423/1436 [02:30<00:01,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  99%|█████████▉| 1424/1436 [02:30<00:01,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  99%|█████████▉| 1425/1436 [02:30<00:01,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  99%|█████████▉| 1426/1436 [02:30<00:01,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  99%|█████████▉| 1427/1436 [02:30<00:00,  9.48it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20:  99%|█████████▉| 1428/1436 [02:30<00:00,  9.48it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20: 100%|█████████▉| 1429/1436 [02:30<00:00,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20: 100%|█████████▉| 1430/1436 [02:30<00:00,  9.48it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20: 100%|█████████▉| 1431/1436 [02:30<00:00,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20: 100%|█████████▉| 1432/1436 [02:31<00:00,  9.48it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20: 100%|█████████▉| 1433/1436 [02:31<00:00,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20: 100%|█████████▉| 1434/1436 [02:31<00:00,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20: 100%|█████████▉| 1435/1436 [02:31<00:00,  9.49it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0848, train_loss_epoch=0.0675]\n",
      "Epoch 20: 100%|██████████| 1436/1436 [02:31<00:00,  9.50it/s, loss=0.065, v_num=0, train_loss_step=0.055, val_loss=0.0842, train_loss_epoch=0.0675]\n",
      "Epoch 21:  94%|█████████▍| 1350/1436 [02:24<00:09,  9.32it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21:  94%|█████████▍| 1351/1436 [02:25<00:09,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  94%|█████████▍| 1352/1436 [02:25<00:09,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  94%|█████████▍| 1353/1436 [02:25<00:08,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  94%|█████████▍| 1354/1436 [02:25<00:08,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  94%|█████████▍| 1355/1436 [02:25<00:08,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  94%|█████████▍| 1356/1436 [02:25<00:08,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  94%|█████████▍| 1357/1436 [02:26<00:08,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  95%|█████████▍| 1358/1436 [02:26<00:08,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  95%|█████████▍| 1359/1436 [02:26<00:08,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  95%|█████████▍| 1360/1436 [02:26<00:08,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  95%|█████████▍| 1361/1436 [02:26<00:08,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  95%|█████████▍| 1362/1436 [02:26<00:07,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  95%|█████████▍| 1363/1436 [02:26<00:07,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  95%|█████████▍| 1364/1436 [02:26<00:07,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  95%|█████████▌| 1365/1436 [02:26<00:07,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  95%|█████████▌| 1366/1436 [02:26<00:07,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  95%|█████████▌| 1367/1436 [02:27<00:07,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  95%|█████████▌| 1368/1436 [02:27<00:07,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  95%|█████████▌| 1369/1436 [02:27<00:07,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  95%|█████████▌| 1370/1436 [02:27<00:07,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  95%|█████████▌| 1371/1436 [02:27<00:06,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  96%|█████████▌| 1372/1436 [02:27<00:06,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  96%|█████████▌| 1373/1436 [02:27<00:06,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  96%|█████████▌| 1374/1436 [02:27<00:06,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  96%|█████████▌| 1375/1436 [02:27<00:06,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  96%|█████████▌| 1376/1436 [02:28<00:06,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  96%|█████████▌| 1377/1436 [02:28<00:06,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  96%|█████████▌| 1378/1436 [02:28<00:06,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  96%|█████████▌| 1379/1436 [02:28<00:06,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  96%|█████████▌| 1380/1436 [02:28<00:06,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  96%|█████████▌| 1381/1436 [02:28<00:05,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  96%|█████████▌| 1382/1436 [02:28<00:05,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  96%|█████████▋| 1383/1436 [02:28<00:05,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  96%|█████████▋| 1384/1436 [02:28<00:05,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  96%|█████████▋| 1385/1436 [02:29<00:05,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  97%|█████████▋| 1386/1436 [02:29<00:05,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  97%|█████████▋| 1387/1436 [02:29<00:05,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  97%|█████████▋| 1388/1436 [02:29<00:05,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  97%|█████████▋| 1389/1436 [02:29<00:05,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  97%|█████████▋| 1390/1436 [02:29<00:04,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  97%|█████████▋| 1391/1436 [02:29<00:04,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  97%|█████████▋| 1392/1436 [02:29<00:04,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  97%|█████████▋| 1393/1436 [02:29<00:04,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  97%|█████████▋| 1394/1436 [02:30<00:04,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  97%|█████████▋| 1395/1436 [02:30<00:04,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  97%|█████████▋| 1396/1436 [02:30<00:04,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  97%|█████████▋| 1397/1436 [02:30<00:04,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  97%|█████████▋| 1398/1436 [02:30<00:04,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  97%|█████████▋| 1399/1436 [02:30<00:03,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  97%|█████████▋| 1400/1436 [02:30<00:03,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  98%|█████████▊| 1401/1436 [02:30<00:03,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  98%|█████████▊| 1402/1436 [02:30<00:03,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  98%|█████████▊| 1403/1436 [02:30<00:03,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  98%|█████████▊| 1404/1436 [02:31<00:03,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  98%|█████████▊| 1405/1436 [02:31<00:03,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  98%|█████████▊| 1406/1436 [02:31<00:03,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  98%|█████████▊| 1407/1436 [02:31<00:03,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  98%|█████████▊| 1408/1436 [02:31<00:03,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  98%|█████████▊| 1409/1436 [02:31<00:02,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  98%|█████████▊| 1410/1436 [02:31<00:02,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  98%|█████████▊| 1411/1436 [02:31<00:02,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  98%|█████████▊| 1412/1436 [02:31<00:02,  9.29it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  98%|█████████▊| 1413/1436 [02:31<00:02,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  98%|█████████▊| 1414/1436 [02:32<00:02,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  99%|█████████▊| 1415/1436 [02:32<00:02,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  99%|█████████▊| 1416/1436 [02:32<00:02,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  99%|█████████▊| 1417/1436 [02:32<00:02,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  99%|█████████▊| 1418/1436 [02:32<00:01,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  99%|█████████▉| 1419/1436 [02:32<00:01,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  99%|█████████▉| 1420/1436 [02:32<00:01,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  99%|█████████▉| 1421/1436 [02:32<00:01,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  99%|█████████▉| 1422/1436 [02:32<00:01,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  99%|█████████▉| 1423/1436 [02:33<00:01,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  99%|█████████▉| 1424/1436 [02:33<00:01,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  99%|█████████▉| 1425/1436 [02:33<00:01,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  99%|█████████▉| 1426/1436 [02:33<00:01,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  99%|█████████▉| 1427/1436 [02:33<00:00,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21:  99%|█████████▉| 1428/1436 [02:33<00:00,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21: 100%|█████████▉| 1429/1436 [02:33<00:00,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21: 100%|█████████▉| 1430/1436 [02:33<00:00,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21: 100%|█████████▉| 1431/1436 [02:33<00:00,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21: 100%|█████████▉| 1432/1436 [02:33<00:00,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21: 100%|█████████▉| 1433/1436 [02:34<00:00,  9.30it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21: 100%|█████████▉| 1434/1436 [02:34<00:00,  9.31it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21: 100%|█████████▉| 1435/1436 [02:34<00:00,  9.31it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0842, train_loss_epoch=0.0671]\n",
      "Epoch 21: 100%|██████████| 1436/1436 [02:34<00:00,  9.32it/s, loss=0.0621, v_num=0, train_loss_step=0.0603, val_loss=0.0836, train_loss_epoch=0.0671]\n",
      "Epoch 24:  94%|█████████▍| 1350/1436 [02:24<00:09,  9.34it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24:  94%|█████████▍| 1351/1436 [02:25<00:09,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  94%|█████████▍| 1352/1436 [02:25<00:09,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  94%|█████████▍| 1353/1436 [02:25<00:08,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  94%|█████████▍| 1354/1436 [02:25<00:08,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  94%|█████████▍| 1355/1436 [02:25<00:08,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  94%|█████████▍| 1356/1436 [02:25<00:08,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  94%|█████████▍| 1357/1436 [02:25<00:08,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  95%|█████████▍| 1358/1436 [02:25<00:08,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  95%|█████████▍| 1359/1436 [02:25<00:08,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  95%|█████████▍| 1360/1436 [02:25<00:08,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  95%|█████████▍| 1361/1436 [02:26<00:08,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  95%|█████████▍| 1362/1436 [02:26<00:07,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  95%|█████████▍| 1363/1436 [02:26<00:07,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  95%|█████████▍| 1364/1436 [02:26<00:07,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  95%|█████████▌| 1365/1436 [02:26<00:07,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  95%|█████████▌| 1366/1436 [02:26<00:07,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  95%|█████████▌| 1367/1436 [02:26<00:07,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  95%|█████████▌| 1368/1436 [02:26<00:07,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  95%|█████████▌| 1369/1436 [02:26<00:07,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  95%|█████████▌| 1370/1436 [02:27<00:07,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  95%|█████████▌| 1371/1436 [02:27<00:06,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  96%|█████████▌| 1372/1436 [02:27<00:06,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  96%|█████████▌| 1373/1436 [02:27<00:06,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  96%|█████████▌| 1374/1436 [02:27<00:06,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  96%|█████████▌| 1375/1436 [02:27<00:06,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  96%|█████████▌| 1376/1436 [02:27<00:06,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  96%|█████████▌| 1377/1436 [02:27<00:06,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  96%|█████████▌| 1378/1436 [02:27<00:06,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  96%|█████████▌| 1379/1436 [02:28<00:06,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  96%|█████████▌| 1380/1436 [02:28<00:06,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  96%|█████████▌| 1381/1436 [02:28<00:05,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  96%|█████████▌| 1382/1436 [02:28<00:05,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  96%|█████████▋| 1383/1436 [02:28<00:05,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  96%|█████████▋| 1384/1436 [02:28<00:05,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  96%|█████████▋| 1385/1436 [02:28<00:05,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  97%|█████████▋| 1386/1436 [02:28<00:05,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  97%|█████████▋| 1387/1436 [02:28<00:05,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  97%|█████████▋| 1388/1436 [02:28<00:05,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  97%|█████████▋| 1389/1436 [02:29<00:05,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  97%|█████████▋| 1390/1436 [02:29<00:04,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  97%|█████████▋| 1391/1436 [02:29<00:04,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  97%|█████████▋| 1392/1436 [02:29<00:04,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  97%|█████████▋| 1393/1436 [02:29<00:04,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  97%|█████████▋| 1394/1436 [02:29<00:04,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  97%|█████████▋| 1395/1436 [02:29<00:04,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  97%|█████████▋| 1396/1436 [02:29<00:04,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  97%|█████████▋| 1397/1436 [02:29<00:04,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  97%|█████████▋| 1398/1436 [02:29<00:04,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  97%|█████████▋| 1399/1436 [02:30<00:03,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  97%|█████████▋| 1400/1436 [02:30<00:03,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  98%|█████████▊| 1401/1436 [02:30<00:03,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  98%|█████████▊| 1402/1436 [02:30<00:03,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  98%|█████████▊| 1403/1436 [02:30<00:03,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  98%|█████████▊| 1404/1436 [02:30<00:03,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  98%|█████████▊| 1405/1436 [02:30<00:03,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  98%|█████████▊| 1406/1436 [02:30<00:03,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  98%|█████████▊| 1407/1436 [02:31<00:03,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  98%|█████████▊| 1408/1436 [02:31<00:03,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  98%|█████████▊| 1409/1436 [02:31<00:02,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  98%|█████████▊| 1410/1436 [02:31<00:02,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  98%|█████████▊| 1411/1436 [02:31<00:02,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  98%|█████████▊| 1412/1436 [02:31<00:02,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  98%|█████████▊| 1413/1436 [02:31<00:02,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  98%|█████████▊| 1414/1436 [02:31<00:02,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  99%|█████████▊| 1415/1436 [02:31<00:02,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  99%|█████████▊| 1416/1436 [02:31<00:02,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  99%|█████████▊| 1417/1436 [02:32<00:02,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  99%|█████████▊| 1418/1436 [02:32<00:01,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  99%|█████████▉| 1419/1436 [02:32<00:01,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  99%|█████████▉| 1420/1436 [02:32<00:01,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  99%|█████████▉| 1421/1436 [02:32<00:01,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  99%|█████████▉| 1422/1436 [02:32<00:01,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  99%|█████████▉| 1423/1436 [02:32<00:01,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  99%|█████████▉| 1424/1436 [02:32<00:01,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  99%|█████████▉| 1425/1436 [02:32<00:01,  9.31it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  99%|█████████▉| 1426/1436 [02:33<00:01,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  99%|█████████▉| 1427/1436 [02:33<00:00,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24:  99%|█████████▉| 1428/1436 [02:33<00:00,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24: 100%|█████████▉| 1429/1436 [02:33<00:00,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24: 100%|█████████▉| 1430/1436 [02:33<00:00,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24: 100%|█████████▉| 1431/1436 [02:33<00:00,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24: 100%|█████████▉| 1432/1436 [02:33<00:00,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24: 100%|█████████▉| 1433/1436 [02:33<00:00,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24: 100%|█████████▉| 1434/1436 [02:33<00:00,  9.32it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24: 100%|█████████▉| 1435/1436 [02:33<00:00,  9.33it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0857, train_loss_epoch=0.0662]\n",
      "Epoch 24: 100%|██████████| 1436/1436 [02:33<00:00,  9.33it/s, loss=0.0646, v_num=0, train_loss_step=0.0451, val_loss=0.0873, train_loss_epoch=0.0662]\n",
      "Epoch 25:  90%|████████▉ | 1287/1436 [02:24<00:16,  8.93it/s, loss=0.0597, v_num=0, train_loss_step=0.0599, val_loss=0.0873, train_loss_epoch=0.066] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27:  94%|█████████▍| 1350/1436 [02:23<00:09,  9.42it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27:  94%|█████████▍| 1351/1436 [02:23<00:09,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  94%|█████████▍| 1352/1436 [02:23<00:08,  9.40it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  94%|█████████▍| 1353/1436 [02:24<00:08,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  94%|█████████▍| 1354/1436 [02:24<00:08,  9.40it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  94%|█████████▍| 1355/1436 [02:24<00:08,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  94%|█████████▍| 1356/1436 [02:24<00:08,  9.40it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  94%|█████████▍| 1357/1436 [02:24<00:08,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  95%|█████████▍| 1358/1436 [02:24<00:08,  9.40it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  95%|█████████▍| 1359/1436 [02:24<00:08,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  95%|█████████▍| 1360/1436 [02:24<00:08,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  95%|█████████▍| 1361/1436 [02:24<00:07,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  95%|█████████▍| 1362/1436 [02:24<00:07,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  95%|█████████▍| 1363/1436 [02:25<00:07,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  95%|█████████▍| 1364/1436 [02:25<00:07,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  95%|█████████▌| 1365/1436 [02:25<00:07,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  95%|█████████▌| 1366/1436 [02:25<00:07,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  95%|█████████▌| 1367/1436 [02:25<00:07,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  95%|█████████▌| 1368/1436 [02:25<00:07,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  95%|█████████▌| 1369/1436 [02:25<00:07,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  95%|█████████▌| 1370/1436 [02:25<00:07,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  95%|█████████▌| 1371/1436 [02:26<00:06,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  96%|█████████▌| 1372/1436 [02:26<00:06,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  96%|█████████▌| 1373/1436 [02:26<00:06,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  96%|█████████▌| 1374/1436 [02:26<00:06,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  96%|█████████▌| 1375/1436 [02:26<00:06,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  96%|█████████▌| 1376/1436 [02:26<00:06,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  96%|█████████▌| 1377/1436 [02:26<00:06,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  96%|█████████▌| 1378/1436 [02:26<00:06,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  96%|█████████▌| 1379/1436 [02:26<00:06,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  96%|█████████▌| 1380/1436 [02:26<00:05,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  96%|█████████▌| 1381/1436 [02:27<00:05,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  96%|█████████▌| 1382/1436 [02:27<00:05,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  96%|█████████▋| 1383/1436 [02:27<00:05,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  96%|█████████▋| 1384/1436 [02:27<00:05,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  96%|█████████▋| 1385/1436 [02:27<00:05,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  97%|█████████▋| 1386/1436 [02:27<00:05,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  97%|█████████▋| 1387/1436 [02:27<00:05,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  97%|█████████▋| 1388/1436 [02:27<00:05,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  97%|█████████▋| 1389/1436 [02:27<00:05,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  97%|█████████▋| 1390/1436 [02:28<00:04,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  97%|█████████▋| 1391/1436 [02:28<00:04,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  97%|█████████▋| 1392/1436 [02:28<00:04,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  97%|█████████▋| 1393/1436 [02:28<00:04,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  97%|█████████▋| 1394/1436 [02:28<00:04,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  97%|█████████▋| 1395/1436 [02:28<00:04,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  97%|█████████▋| 1396/1436 [02:28<00:04,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  97%|█████████▋| 1397/1436 [02:28<00:04,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  97%|█████████▋| 1398/1436 [02:28<00:04,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  97%|█████████▋| 1399/1436 [02:29<00:03,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  97%|█████████▋| 1400/1436 [02:29<00:03,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  98%|█████████▊| 1401/1436 [02:29<00:03,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  98%|█████████▊| 1402/1436 [02:29<00:03,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  98%|█████████▊| 1403/1436 [02:29<00:03,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  98%|█████████▊| 1404/1436 [02:29<00:03,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  98%|█████████▊| 1405/1436 [02:29<00:03,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  98%|█████████▊| 1406/1436 [02:29<00:03,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  98%|█████████▊| 1407/1436 [02:29<00:03,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  98%|█████████▊| 1408/1436 [02:29<00:02,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  98%|█████████▊| 1409/1436 [02:30<00:02,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  98%|█████████▊| 1410/1436 [02:30<00:02,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  98%|█████████▊| 1411/1436 [02:30<00:02,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  98%|█████████▊| 1412/1436 [02:30<00:02,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  98%|█████████▊| 1413/1436 [02:30<00:02,  9.38it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  98%|█████████▊| 1414/1436 [02:30<00:02,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  99%|█████████▊| 1415/1436 [02:30<00:02,  9.38it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  99%|█████████▊| 1416/1436 [02:30<00:02,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  99%|█████████▊| 1417/1436 [02:30<00:02,  9.38it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  99%|█████████▊| 1418/1436 [02:31<00:01,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  99%|█████████▉| 1419/1436 [02:31<00:01,  9.38it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  99%|█████████▉| 1420/1436 [02:31<00:01,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  99%|█████████▉| 1421/1436 [02:31<00:01,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  99%|█████████▉| 1422/1436 [02:31<00:01,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  99%|█████████▉| 1423/1436 [02:31<00:01,  9.38it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  99%|█████████▉| 1424/1436 [02:31<00:01,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  99%|█████████▉| 1425/1436 [02:31<00:01,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  99%|█████████▉| 1426/1436 [02:31<00:01,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  99%|█████████▉| 1427/1436 [02:32<00:00,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27:  99%|█████████▉| 1428/1436 [02:32<00:00,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27: 100%|█████████▉| 1429/1436 [02:32<00:00,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27: 100%|█████████▉| 1430/1436 [02:32<00:00,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27: 100%|█████████▉| 1431/1436 [02:32<00:00,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27: 100%|█████████▉| 1432/1436 [02:32<00:00,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27: 100%|█████████▉| 1433/1436 [02:32<00:00,  9.38it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27: 100%|█████████▉| 1434/1436 [02:32<00:00,  9.39it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27: 100%|█████████▉| 1435/1436 [02:32<00:00,  9.40it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0839, train_loss_epoch=0.0655]\n",
      "Epoch 27: 100%|██████████| 1436/1436 [02:32<00:00,  9.40it/s, loss=0.0634, v_num=0, train_loss_step=0.0657, val_loss=0.0862, train_loss_epoch=0.0655]\n",
      "Epoch 28:  92%|█████████▏| 1319/1436 [02:20<00:12,  9.41it/s, loss=0.0614, v_num=0, train_loss_step=0.0604, val_loss=0.0862, train_loss_epoch=0.0653]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30:  94%|█████████▍| 1350/1436 [02:27<00:09,  9.16it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 30:  94%|█████████▍| 1351/1436 [02:28<00:09,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  94%|█████████▍| 1352/1436 [02:28<00:09,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  94%|█████████▍| 1353/1436 [02:28<00:09,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  94%|█████████▍| 1354/1436 [02:28<00:08,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  94%|█████████▍| 1355/1436 [02:28<00:08,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  94%|█████████▍| 1356/1436 [02:28<00:08,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  94%|█████████▍| 1357/1436 [02:28<00:08,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  95%|█████████▍| 1358/1436 [02:28<00:08,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  95%|█████████▍| 1359/1436 [02:28<00:08,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  95%|█████████▍| 1360/1436 [02:28<00:08,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  95%|█████████▍| 1361/1436 [02:29<00:08,  9.12it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  95%|█████████▍| 1362/1436 [02:29<00:08,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  95%|█████████▍| 1363/1436 [02:29<00:08,  9.12it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  95%|█████████▍| 1364/1436 [02:29<00:07,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  95%|█████████▌| 1365/1436 [02:29<00:07,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  95%|█████████▌| 1366/1436 [02:29<00:07,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  95%|█████████▌| 1367/1436 [02:29<00:07,  9.12it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  95%|█████████▌| 1368/1436 [02:29<00:07,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  95%|█████████▌| 1369/1436 [02:30<00:07,  9.12it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  95%|█████████▌| 1370/1436 [02:30<00:07,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  95%|█████████▌| 1371/1436 [02:30<00:07,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  96%|█████████▌| 1372/1436 [02:30<00:07,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  96%|█████████▌| 1373/1436 [02:30<00:06,  9.12it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  96%|█████████▌| 1374/1436 [02:30<00:06,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  96%|█████████▌| 1375/1436 [02:30<00:06,  9.12it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  96%|█████████▌| 1376/1436 [02:30<00:06,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  96%|█████████▌| 1377/1436 [02:30<00:06,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  96%|█████████▌| 1378/1436 [02:30<00:06,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  96%|█████████▌| 1379/1436 [02:31<00:06,  9.12it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  96%|█████████▌| 1380/1436 [02:31<00:06,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  96%|█████████▌| 1381/1436 [02:31<00:06,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  96%|█████████▌| 1382/1436 [02:31<00:05,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  96%|█████████▋| 1383/1436 [02:31<00:05,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  96%|█████████▋| 1384/1436 [02:31<00:05,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  96%|█████████▋| 1385/1436 [02:31<00:05,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  97%|█████████▋| 1386/1436 [02:31<00:05,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  97%|█████████▋| 1387/1436 [02:31<00:05,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  97%|█████████▋| 1388/1436 [02:31<00:05,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  97%|█████████▋| 1389/1436 [02:32<00:05,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  97%|█████████▋| 1390/1436 [02:32<00:05,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  97%|█████████▋| 1391/1436 [02:32<00:04,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  97%|█████████▋| 1392/1436 [02:32<00:04,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  97%|█████████▋| 1393/1436 [02:32<00:04,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  97%|█████████▋| 1394/1436 [02:32<00:04,  9.14it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  97%|█████████▋| 1395/1436 [02:32<00:04,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  97%|█████████▋| 1396/1436 [02:32<00:04,  9.14it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  97%|█████████▋| 1397/1436 [02:32<00:04,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  97%|█████████▋| 1398/1436 [02:33<00:04,  9.14it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  97%|█████████▋| 1399/1436 [02:33<00:04,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  97%|█████████▋| 1400/1436 [02:33<00:03,  9.14it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  98%|█████████▊| 1401/1436 [02:33<00:03,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  98%|█████████▊| 1402/1436 [02:33<00:03,  9.14it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  98%|█████████▊| 1403/1436 [02:33<00:03,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  98%|█████████▊| 1404/1436 [02:33<00:03,  9.14it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  98%|█████████▊| 1405/1436 [02:33<00:03,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  98%|█████████▊| 1406/1436 [02:33<00:03,  9.14it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  98%|█████████▊| 1407/1436 [02:34<00:03,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  98%|█████████▊| 1408/1436 [02:34<00:03,  9.14it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  98%|█████████▊| 1409/1436 [02:34<00:02,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  98%|█████████▊| 1410/1436 [02:34<00:02,  9.14it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  98%|█████████▊| 1411/1436 [02:34<00:02,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  98%|█████████▊| 1412/1436 [02:34<00:02,  9.14it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  98%|█████████▊| 1413/1436 [02:34<00:02,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  98%|█████████▊| 1414/1436 [02:34<00:02,  9.14it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  99%|█████████▊| 1415/1436 [02:34<00:02,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  99%|█████████▊| 1416/1436 [02:35<00:02,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  99%|█████████▊| 1417/1436 [02:35<00:02,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  99%|█████████▊| 1418/1436 [02:35<00:01,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  99%|█████████▉| 1419/1436 [02:35<00:01,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  99%|█████████▉| 1420/1436 [02:35<00:01,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  99%|█████████▉| 1421/1436 [02:35<00:01,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  99%|█████████▉| 1422/1436 [02:35<00:01,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  99%|█████████▉| 1423/1436 [02:35<00:01,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  99%|█████████▉| 1424/1436 [02:35<00:01,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  99%|█████████▉| 1425/1436 [02:36<00:01,  9.12it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  99%|█████████▉| 1426/1436 [02:36<00:01,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  99%|█████████▉| 1427/1436 [02:36<00:00,  9.12it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30:  99%|█████████▉| 1428/1436 [02:36<00:00,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30: 100%|█████████▉| 1429/1436 [02:36<00:00,  9.12it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30: 100%|█████████▉| 1430/1436 [02:36<00:00,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30: 100%|█████████▉| 1431/1436 [02:36<00:00,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30: 100%|█████████▉| 1432/1436 [02:36<00:00,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30: 100%|█████████▉| 1433/1436 [02:37<00:00,  9.12it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30: 100%|█████████▉| 1434/1436 [02:37<00:00,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30: 100%|█████████▉| 1435/1436 [02:37<00:00,  9.13it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.085, train_loss_epoch=0.0647]\n",
      "Epoch 30: 100%|██████████| 1436/1436 [02:37<00:00,  9.14it/s, loss=0.0666, v_num=0, train_loss_step=0.0632, val_loss=0.0841, train_loss_epoch=0.0647]\n",
      "Epoch 31:  85%|████████▍ | 1215/1436 [02:15<00:24,  9.00it/s, loss=0.0643, v_num=0, train_loss_step=0.102, val_loss=0.0841, train_loss_epoch=0.0645] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32:  94%|█████████▍| 1350/1436 [02:29<00:09,  9.01it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 32:  94%|█████████▍| 1351/1436 [02:30<00:09,  8.98it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  94%|█████████▍| 1352/1436 [02:30<00:09,  8.98it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  94%|█████████▍| 1353/1436 [02:30<00:09,  8.98it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  94%|█████████▍| 1354/1436 [02:30<00:09,  8.98it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  94%|█████████▍| 1355/1436 [02:30<00:09,  8.98it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  94%|█████████▍| 1356/1436 [02:30<00:08,  8.98it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  94%|█████████▍| 1357/1436 [02:31<00:08,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  95%|█████████▍| 1358/1436 [02:31<00:08,  8.98it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  95%|█████████▍| 1359/1436 [02:31<00:08,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  95%|█████████▍| 1360/1436 [02:31<00:08,  8.98it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  95%|█████████▍| 1361/1436 [02:31<00:08,  8.98it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  95%|█████████▍| 1362/1436 [02:31<00:08,  8.98it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  95%|█████████▍| 1363/1436 [02:31<00:08,  8.98it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  95%|█████████▍| 1364/1436 [02:31<00:08,  8.98it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  95%|█████████▌| 1365/1436 [02:32<00:07,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  95%|█████████▌| 1366/1436 [02:32<00:07,  8.98it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  95%|█████████▌| 1367/1436 [02:32<00:07,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  95%|█████████▌| 1368/1436 [02:32<00:07,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  95%|█████████▌| 1369/1436 [02:32<00:07,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  95%|█████████▌| 1370/1436 [02:32<00:07,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  95%|█████████▌| 1371/1436 [02:32<00:07,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  96%|█████████▌| 1372/1436 [02:32<00:07,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  96%|█████████▌| 1373/1436 [02:33<00:07,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  96%|█████████▌| 1374/1436 [02:33<00:06,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  96%|█████████▌| 1375/1436 [02:33<00:06,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  96%|█████████▌| 1376/1436 [02:33<00:06,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  96%|█████████▌| 1377/1436 [02:33<00:06,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  96%|█████████▌| 1378/1436 [02:33<00:06,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  96%|█████████▌| 1379/1436 [02:33<00:06,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  96%|█████████▌| 1380/1436 [02:33<00:06,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  96%|█████████▌| 1381/1436 [02:33<00:06,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  96%|█████████▌| 1382/1436 [02:34<00:06,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  96%|█████████▋| 1383/1436 [02:34<00:05,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  96%|█████████▋| 1384/1436 [02:34<00:05,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  96%|█████████▋| 1385/1436 [02:34<00:05,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  97%|█████████▋| 1386/1436 [02:34<00:05,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  97%|█████████▋| 1387/1436 [02:34<00:05,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  97%|█████████▋| 1388/1436 [02:34<00:05,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  97%|█████████▋| 1389/1436 [02:34<00:05,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  97%|█████████▋| 1390/1436 [02:34<00:05,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  97%|█████████▋| 1391/1436 [02:35<00:05,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  97%|█████████▋| 1392/1436 [02:35<00:04,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  97%|█████████▋| 1393/1436 [02:35<00:04,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  97%|█████████▋| 1394/1436 [02:35<00:04,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  97%|█████████▋| 1395/1436 [02:35<00:04,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  97%|█████████▋| 1396/1436 [02:35<00:04,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  97%|█████████▋| 1397/1436 [02:35<00:04,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  97%|█████████▋| 1398/1436 [02:35<00:04,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  97%|█████████▋| 1399/1436 [02:35<00:04,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  97%|█████████▋| 1400/1436 [02:36<00:04,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  98%|█████████▊| 1401/1436 [02:36<00:03,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  98%|█████████▊| 1402/1436 [02:36<00:03,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  98%|█████████▊| 1403/1436 [02:36<00:03,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  98%|█████████▊| 1404/1436 [02:36<00:03,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  98%|█████████▊| 1405/1436 [02:36<00:03,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  98%|█████████▊| 1406/1436 [02:36<00:03,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  98%|█████████▊| 1407/1436 [02:36<00:03,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  98%|█████████▊| 1408/1436 [02:36<00:03,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  98%|█████████▊| 1409/1436 [02:37<00:03,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  98%|█████████▊| 1410/1436 [02:37<00:02,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  98%|█████████▊| 1411/1436 [02:37<00:02,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  98%|█████████▊| 1412/1436 [02:37<00:02,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  98%|█████████▊| 1413/1436 [02:37<00:02,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  98%|█████████▊| 1414/1436 [02:37<00:02,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  99%|█████████▊| 1415/1436 [02:37<00:02,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  99%|█████████▊| 1416/1436 [02:37<00:02,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  99%|█████████▊| 1417/1436 [02:37<00:02,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  99%|█████████▊| 1418/1436 [02:38<00:02,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  99%|█████████▉| 1419/1436 [02:38<00:01,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  99%|█████████▉| 1420/1436 [02:38<00:01,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  99%|█████████▉| 1421/1436 [02:38<00:01,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  99%|█████████▉| 1422/1436 [02:38<00:01,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  99%|█████████▉| 1423/1436 [02:38<00:01,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  99%|█████████▉| 1424/1436 [02:38<00:01,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  99%|█████████▉| 1425/1436 [02:38<00:01,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  99%|█████████▉| 1426/1436 [02:39<00:01,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  99%|█████████▉| 1427/1436 [02:39<00:01,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32:  99%|█████████▉| 1428/1436 [02:39<00:00,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32: 100%|█████████▉| 1429/1436 [02:39<00:00,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32: 100%|█████████▉| 1430/1436 [02:39<00:00,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32: 100%|█████████▉| 1431/1436 [02:39<00:00,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32: 100%|█████████▉| 1432/1436 [02:39<00:00,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32: 100%|█████████▉| 1433/1436 [02:39<00:00,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32: 100%|█████████▉| 1434/1436 [02:39<00:00,  8.97it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32: 100%|█████████▉| 1435/1436 [02:39<00:00,  8.98it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0852, train_loss_epoch=0.0644]\n",
      "Epoch 32: 100%|██████████| 1436/1436 [02:39<00:00,  8.98it/s, loss=0.0634, v_num=0, train_loss_step=0.0444, val_loss=0.0873, train_loss_epoch=0.0644]\n",
      "Epoch 33:  94%|█████████▍| 1350/1436 [02:31<00:09,  8.88it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 33:  94%|█████████▍| 1351/1436 [02:32<00:09,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  94%|█████████▍| 1352/1436 [02:32<00:09,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  94%|█████████▍| 1353/1436 [02:32<00:09,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  94%|█████████▍| 1354/1436 [02:32<00:09,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  94%|█████████▍| 1355/1436 [02:33<00:09,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  94%|█████████▍| 1356/1436 [02:33<00:09,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  94%|█████████▍| 1357/1436 [02:33<00:08,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  95%|█████████▍| 1358/1436 [02:33<00:08,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  95%|█████████▍| 1359/1436 [02:33<00:08,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  95%|█████████▍| 1360/1436 [02:33<00:08,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  95%|█████████▍| 1361/1436 [02:33<00:08,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  95%|█████████▍| 1362/1436 [02:33<00:08,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  95%|█████████▍| 1363/1436 [02:33<00:08,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  95%|█████████▍| 1364/1436 [02:33<00:08,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  95%|█████████▌| 1365/1436 [02:34<00:08,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  95%|█████████▌| 1366/1436 [02:34<00:07,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  95%|█████████▌| 1367/1436 [02:34<00:07,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  95%|█████████▌| 1368/1436 [02:34<00:07,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  95%|█████████▌| 1369/1436 [02:34<00:07,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  95%|█████████▌| 1370/1436 [02:34<00:07,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  95%|█████████▌| 1371/1436 [02:34<00:07,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  96%|█████████▌| 1372/1436 [02:34<00:07,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  96%|█████████▌| 1373/1436 [02:35<00:07,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  96%|█████████▌| 1374/1436 [02:35<00:06,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  96%|█████████▌| 1375/1436 [02:35<00:06,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  96%|█████████▌| 1376/1436 [02:35<00:06,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  96%|█████████▌| 1377/1436 [02:35<00:06,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  96%|█████████▌| 1378/1436 [02:35<00:06,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  96%|█████████▌| 1379/1436 [02:35<00:06,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  96%|█████████▌| 1380/1436 [02:35<00:06,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  96%|█████████▌| 1381/1436 [02:36<00:06,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  96%|█████████▌| 1382/1436 [02:36<00:06,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  96%|█████████▋| 1383/1436 [02:36<00:05,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  96%|█████████▋| 1384/1436 [02:36<00:05,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  96%|█████████▋| 1385/1436 [02:36<00:05,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  97%|█████████▋| 1386/1436 [02:36<00:05,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  97%|█████████▋| 1387/1436 [02:36<00:05,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  97%|█████████▋| 1388/1436 [02:36<00:05,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  97%|█████████▋| 1389/1436 [02:37<00:05,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  97%|█████████▋| 1390/1436 [02:37<00:05,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  97%|█████████▋| 1391/1436 [02:37<00:05,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  97%|█████████▋| 1392/1436 [02:37<00:04,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  97%|█████████▋| 1393/1436 [02:37<00:04,  8.84it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  97%|█████████▋| 1394/1436 [02:37<00:04,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  97%|█████████▋| 1395/1436 [02:37<00:04,  8.84it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  97%|█████████▋| 1396/1436 [02:37<00:04,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  97%|█████████▋| 1397/1436 [02:37<00:04,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  97%|█████████▋| 1398/1436 [02:37<00:04,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  97%|█████████▋| 1399/1436 [02:38<00:04,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  97%|█████████▋| 1400/1436 [02:38<00:04,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  98%|█████████▊| 1401/1436 [02:38<00:03,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  98%|█████████▊| 1402/1436 [02:38<00:03,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  98%|█████████▊| 1403/1436 [02:38<00:03,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  98%|█████████▊| 1404/1436 [02:38<00:03,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  98%|█████████▊| 1405/1436 [02:38<00:03,  8.84it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  98%|█████████▊| 1406/1436 [02:38<00:03,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  98%|█████████▊| 1407/1436 [02:39<00:03,  8.84it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  98%|█████████▊| 1408/1436 [02:39<00:03,  8.84it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  98%|█████████▊| 1409/1436 [02:39<00:03,  8.84it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  98%|█████████▊| 1410/1436 [02:39<00:02,  8.84it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  98%|█████████▊| 1411/1436 [02:39<00:02,  8.84it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  98%|█████████▊| 1412/1436 [02:39<00:02,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  98%|█████████▊| 1413/1436 [02:39<00:02,  8.84it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  98%|█████████▊| 1414/1436 [02:39<00:02,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  99%|█████████▊| 1415/1436 [02:40<00:02,  8.84it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  99%|█████████▊| 1416/1436 [02:40<00:02,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  99%|█████████▊| 1417/1436 [02:40<00:02,  8.84it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  99%|█████████▊| 1418/1436 [02:40<00:02,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  99%|█████████▉| 1419/1436 [02:40<00:01,  8.84it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  99%|█████████▉| 1420/1436 [02:40<00:01,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  99%|█████████▉| 1421/1436 [02:40<00:01,  8.84it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  99%|█████████▉| 1422/1436 [02:40<00:01,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  99%|█████████▉| 1423/1436 [02:40<00:01,  8.84it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  99%|█████████▉| 1424/1436 [02:40<00:01,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  99%|█████████▉| 1425/1436 [02:41<00:01,  8.84it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  99%|█████████▉| 1426/1436 [02:41<00:01,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  99%|█████████▉| 1427/1436 [02:41<00:01,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33:  99%|█████████▉| 1428/1436 [02:41<00:00,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33: 100%|█████████▉| 1429/1436 [02:41<00:00,  8.84it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33: 100%|█████████▉| 1430/1436 [02:41<00:00,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33: 100%|█████████▉| 1431/1436 [02:41<00:00,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33: 100%|█████████▉| 1432/1436 [02:41<00:00,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33: 100%|█████████▉| 1433/1436 [02:41<00:00,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33: 100%|█████████▉| 1434/1436 [02:41<00:00,  8.85it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33: 100%|█████████▉| 1435/1436 [02:42<00:00,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.0873, train_loss_epoch=0.0641]\n",
      "Epoch 33: 100%|██████████| 1436/1436 [02:42<00:00,  8.86it/s, loss=0.0596, v_num=0, train_loss_step=0.056, val_loss=0.086, train_loss_epoch=0.0641] \n",
      "Epoch 34:  94%|█████████▍| 1350/1436 [02:32<00:09,  8.87it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 34:  94%|█████████▍| 1351/1436 [02:32<00:09,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  94%|█████████▍| 1352/1436 [02:32<00:09,  8.85it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  94%|█████████▍| 1353/1436 [02:33<00:09,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  94%|█████████▍| 1354/1436 [02:33<00:09,  8.85it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  94%|█████████▍| 1355/1436 [02:33<00:09,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  94%|█████████▍| 1356/1436 [02:33<00:09,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  94%|█████████▍| 1357/1436 [02:33<00:08,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  95%|█████████▍| 1358/1436 [02:33<00:08,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  95%|█████████▍| 1359/1436 [02:33<00:08,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  95%|█████████▍| 1360/1436 [02:33<00:08,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  95%|█████████▍| 1361/1436 [02:33<00:08,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  95%|█████████▍| 1362/1436 [02:34<00:08,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  95%|█████████▍| 1363/1436 [02:34<00:08,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  95%|█████████▍| 1364/1436 [02:34<00:08,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  95%|█████████▌| 1365/1436 [02:34<00:08,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  95%|█████████▌| 1366/1436 [02:34<00:07,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  95%|█████████▌| 1367/1436 [02:34<00:07,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  95%|█████████▌| 1368/1436 [02:34<00:07,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  95%|█████████▌| 1369/1436 [02:34<00:07,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  95%|█████████▌| 1370/1436 [02:34<00:07,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  95%|█████████▌| 1371/1436 [02:35<00:07,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  96%|█████████▌| 1372/1436 [02:35<00:07,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  96%|█████████▌| 1373/1436 [02:35<00:07,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  96%|█████████▌| 1374/1436 [02:35<00:07,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  96%|█████████▌| 1375/1436 [02:35<00:06,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  96%|█████████▌| 1376/1436 [02:35<00:06,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  96%|█████████▌| 1377/1436 [02:35<00:06,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  96%|█████████▌| 1378/1436 [02:35<00:06,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  96%|█████████▌| 1379/1436 [02:35<00:06,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  96%|█████████▌| 1380/1436 [02:36<00:06,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  96%|█████████▌| 1381/1436 [02:36<00:06,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  96%|█████████▌| 1382/1436 [02:36<00:06,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  96%|█████████▋| 1383/1436 [02:36<00:05,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  96%|█████████▋| 1384/1436 [02:36<00:05,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  96%|█████████▋| 1385/1436 [02:36<00:05,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  97%|█████████▋| 1386/1436 [02:36<00:05,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  97%|█████████▋| 1387/1436 [02:36<00:05,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  97%|█████████▋| 1388/1436 [02:37<00:05,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  97%|█████████▋| 1389/1436 [02:37<00:05,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  97%|█████████▋| 1390/1436 [02:37<00:05,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  97%|█████████▋| 1391/1436 [02:37<00:05,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  97%|█████████▋| 1392/1436 [02:37<00:04,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  97%|█████████▋| 1393/1436 [02:37<00:04,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  97%|█████████▋| 1394/1436 [02:37<00:04,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  97%|█████████▋| 1395/1436 [02:37<00:04,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  97%|█████████▋| 1396/1436 [02:37<00:04,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  97%|█████████▋| 1397/1436 [02:37<00:04,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  97%|█████████▋| 1398/1436 [02:38<00:04,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  97%|█████████▋| 1399/1436 [02:38<00:04,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  97%|█████████▋| 1400/1436 [02:38<00:04,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  98%|█████████▊| 1401/1436 [02:38<00:03,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  98%|█████████▊| 1402/1436 [02:38<00:03,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  98%|█████████▊| 1403/1436 [02:38<00:03,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  98%|█████████▊| 1404/1436 [02:38<00:03,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  98%|█████████▊| 1405/1436 [02:38<00:03,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  98%|█████████▊| 1406/1436 [02:38<00:03,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  98%|█████████▊| 1407/1436 [02:39<00:03,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  98%|█████████▊| 1408/1436 [02:39<00:03,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  98%|█████████▊| 1409/1436 [02:39<00:03,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  98%|█████████▊| 1410/1436 [02:39<00:02,  8.85it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  98%|█████████▊| 1411/1436 [02:39<00:02,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  98%|█████████▊| 1412/1436 [02:39<00:02,  8.85it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  98%|█████████▊| 1413/1436 [02:39<00:02,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  98%|█████████▊| 1414/1436 [02:39<00:02,  8.85it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  99%|█████████▊| 1415/1436 [02:39<00:02,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  99%|█████████▊| 1416/1436 [02:40<00:02,  8.85it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  99%|█████████▊| 1417/1436 [02:40<00:02,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  99%|█████████▊| 1418/1436 [02:40<00:02,  8.85it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  99%|█████████▉| 1419/1436 [02:40<00:01,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  99%|█████████▉| 1420/1436 [02:40<00:01,  8.85it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  99%|█████████▉| 1421/1436 [02:40<00:01,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  99%|█████████▉| 1422/1436 [02:40<00:01,  8.85it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  99%|█████████▉| 1423/1436 [02:40<00:01,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  99%|█████████▉| 1424/1436 [02:40<00:01,  8.85it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  99%|█████████▉| 1425/1436 [02:41<00:01,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  99%|█████████▉| 1426/1436 [02:41<00:01,  8.85it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  99%|█████████▉| 1427/1436 [02:41<00:01,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34:  99%|█████████▉| 1428/1436 [02:41<00:00,  8.85it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34: 100%|█████████▉| 1429/1436 [02:41<00:00,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34: 100%|█████████▉| 1430/1436 [02:41<00:00,  8.85it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34: 100%|█████████▉| 1431/1436 [02:41<00:00,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34: 100%|█████████▉| 1432/1436 [02:41<00:00,  8.85it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34: 100%|█████████▉| 1433/1436 [02:42<00:00,  8.84it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34: 100%|█████████▉| 1434/1436 [02:42<00:00,  8.85it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34: 100%|█████████▉| 1435/1436 [02:42<00:00,  8.85it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.086, train_loss_epoch=0.064]\n",
      "Epoch 34: 100%|██████████| 1436/1436 [02:42<00:00,  8.86it/s, loss=0.0614, v_num=0, train_loss_step=0.0629, val_loss=0.0855, train_loss_epoch=0.064]\n",
      "Epoch 35:  94%|█████████▍| 1350/1436 [02:32<00:09,  8.84it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 35:  94%|█████████▍| 1351/1436 [02:33<00:09,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  94%|█████████▍| 1352/1436 [02:33<00:09,  8.82it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  94%|█████████▍| 1353/1436 [02:33<00:09,  8.82it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  94%|█████████▍| 1354/1436 [02:33<00:09,  8.82it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  94%|█████████▍| 1355/1436 [02:33<00:09,  8.82it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  94%|█████████▍| 1356/1436 [02:33<00:09,  8.82it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  94%|█████████▍| 1357/1436 [02:33<00:08,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  95%|█████████▍| 1358/1436 [02:33<00:08,  8.82it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  95%|█████████▍| 1359/1436 [02:34<00:08,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  95%|█████████▍| 1360/1436 [02:34<00:08,  8.82it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  95%|█████████▍| 1361/1436 [02:34<00:08,  8.82it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  95%|█████████▍| 1362/1436 [02:34<00:08,  8.82it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  95%|█████████▍| 1363/1436 [02:34<00:08,  8.82it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  95%|█████████▍| 1364/1436 [02:34<00:08,  8.82it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  95%|█████████▌| 1365/1436 [02:34<00:08,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  95%|█████████▌| 1366/1436 [02:34<00:07,  8.82it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  95%|█████████▌| 1367/1436 [02:35<00:07,  8.82it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  95%|█████████▌| 1368/1436 [02:35<00:07,  8.82it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  95%|█████████▌| 1369/1436 [02:35<00:07,  8.82it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  95%|█████████▌| 1370/1436 [02:35<00:07,  8.82it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  95%|█████████▌| 1371/1436 [02:35<00:07,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  96%|█████████▌| 1372/1436 [02:35<00:07,  8.82it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  96%|█████████▌| 1373/1436 [02:35<00:07,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  96%|█████████▌| 1374/1436 [02:35<00:07,  8.82it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  96%|█████████▌| 1375/1436 [02:36<00:06,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  96%|█████████▌| 1376/1436 [02:36<00:06,  8.82it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  96%|█████████▌| 1377/1436 [02:36<00:06,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  96%|█████████▌| 1378/1436 [02:36<00:06,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  96%|█████████▌| 1379/1436 [02:36<00:06,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  96%|█████████▌| 1380/1436 [02:36<00:06,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  96%|█████████▌| 1381/1436 [02:36<00:06,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  96%|█████████▌| 1382/1436 [02:36<00:06,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  96%|█████████▋| 1383/1436 [02:37<00:06,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  96%|█████████▋| 1384/1436 [02:37<00:05,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  96%|█████████▋| 1385/1436 [02:37<00:05,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  97%|█████████▋| 1386/1436 [02:37<00:05,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  97%|█████████▋| 1387/1436 [02:37<00:05,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  97%|█████████▋| 1388/1436 [02:37<00:05,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  97%|█████████▋| 1389/1436 [02:37<00:05,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  97%|█████████▋| 1390/1436 [02:37<00:05,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  97%|█████████▋| 1391/1436 [02:37<00:05,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  97%|█████████▋| 1392/1436 [02:38<00:04,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  97%|█████████▋| 1393/1436 [02:38<00:04,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  97%|█████████▋| 1394/1436 [02:38<00:04,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  97%|█████████▋| 1395/1436 [02:38<00:04,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  97%|█████████▋| 1396/1436 [02:38<00:04,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  97%|█████████▋| 1397/1436 [02:38<00:04,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  97%|█████████▋| 1398/1436 [02:38<00:04,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  97%|█████████▋| 1399/1436 [02:38<00:04,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  97%|█████████▋| 1400/1436 [02:39<00:04,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  98%|█████████▊| 1401/1436 [02:39<00:03,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  98%|█████████▊| 1402/1436 [02:39<00:03,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  98%|█████████▊| 1403/1436 [02:39<00:03,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  98%|█████████▊| 1404/1436 [02:39<00:03,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  98%|█████████▊| 1405/1436 [02:39<00:03,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  98%|█████████▊| 1406/1436 [02:39<00:03,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  98%|█████████▊| 1407/1436 [02:39<00:03,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  98%|█████████▊| 1408/1436 [02:39<00:03,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  98%|█████████▊| 1409/1436 [02:40<00:03,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  98%|█████████▊| 1410/1436 [02:40<00:02,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  98%|█████████▊| 1411/1436 [02:40<00:02,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  98%|█████████▊| 1412/1436 [02:40<00:02,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  98%|█████████▊| 1413/1436 [02:40<00:02,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  98%|█████████▊| 1414/1436 [02:40<00:02,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  99%|█████████▊| 1415/1436 [02:40<00:02,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  99%|█████████▊| 1416/1436 [02:40<00:02,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  99%|█████████▊| 1417/1436 [02:41<00:02,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  99%|█████████▊| 1418/1436 [02:41<00:02,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  99%|█████████▉| 1419/1436 [02:41<00:01,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  99%|█████████▉| 1420/1436 [02:41<00:01,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  99%|█████████▉| 1421/1436 [02:41<00:01,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  99%|█████████▉| 1422/1436 [02:41<00:01,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  99%|█████████▉| 1423/1436 [02:41<00:01,  8.79it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  99%|█████████▉| 1424/1436 [02:41<00:01,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  99%|█████████▉| 1425/1436 [02:42<00:01,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  99%|█████████▉| 1426/1436 [02:42<00:01,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  99%|█████████▉| 1427/1436 [02:42<00:01,  8.79it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35:  99%|█████████▉| 1428/1436 [02:42<00:00,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35: 100%|█████████▉| 1429/1436 [02:42<00:00,  8.79it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35: 100%|█████████▉| 1430/1436 [02:42<00:00,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35: 100%|█████████▉| 1431/1436 [02:42<00:00,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35: 100%|█████████▉| 1432/1436 [02:42<00:00,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35: 100%|█████████▉| 1433/1436 [02:42<00:00,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35: 100%|█████████▉| 1434/1436 [02:42<00:00,  8.80it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35: 100%|█████████▉| 1435/1436 [02:42<00:00,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0855, train_loss_epoch=0.0637]\n",
      "Epoch 35: 100%|██████████| 1436/1436 [02:42<00:00,  8.81it/s, loss=0.0647, v_num=0, train_loss_step=0.043, val_loss=0.0865, train_loss_epoch=0.0637]\n",
      "Epoch 36:  94%|█████████▍| 1350/1436 [02:30<00:09,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 36:  94%|█████████▍| 1351/1436 [02:31<00:09,  8.94it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  94%|█████████▍| 1352/1436 [02:31<00:09,  8.95it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  94%|█████████▍| 1353/1436 [02:31<00:09,  8.94it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  94%|█████████▍| 1354/1436 [02:31<00:09,  8.95it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  94%|█████████▍| 1355/1436 [02:31<00:09,  8.95it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  94%|█████████▍| 1356/1436 [02:31<00:08,  8.95it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  94%|█████████▍| 1357/1436 [02:31<00:08,  8.95it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  95%|█████████▍| 1358/1436 [02:31<00:08,  8.95it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  95%|█████████▍| 1359/1436 [02:31<00:08,  8.95it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  95%|█████████▍| 1360/1436 [02:31<00:08,  8.95it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  95%|█████████▍| 1361/1436 [02:32<00:08,  8.95it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  95%|█████████▍| 1362/1436 [02:32<00:08,  8.95it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  95%|█████████▍| 1363/1436 [02:32<00:08,  8.95it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  95%|█████████▍| 1364/1436 [02:32<00:08,  8.95it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  95%|█████████▌| 1365/1436 [02:32<00:07,  8.95it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  95%|█████████▌| 1366/1436 [02:32<00:07,  8.95it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  95%|█████████▌| 1367/1436 [02:32<00:07,  8.95it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  95%|█████████▌| 1368/1436 [02:32<00:07,  8.95it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  95%|█████████▌| 1369/1436 [02:32<00:07,  8.95it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  95%|█████████▌| 1370/1436 [02:32<00:07,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  95%|█████████▌| 1371/1436 [02:33<00:07,  8.95it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  96%|█████████▌| 1372/1436 [02:33<00:07,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  96%|█████████▌| 1373/1436 [02:33<00:07,  8.95it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  96%|█████████▌| 1374/1436 [02:33<00:06,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  96%|█████████▌| 1375/1436 [02:33<00:06,  8.95it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  96%|█████████▌| 1376/1436 [02:33<00:06,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  96%|█████████▌| 1377/1436 [02:33<00:06,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  96%|█████████▌| 1378/1436 [02:33<00:06,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  96%|█████████▌| 1379/1436 [02:33<00:06,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  96%|█████████▌| 1380/1436 [02:34<00:06,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  96%|█████████▌| 1381/1436 [02:34<00:06,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  96%|█████████▌| 1382/1436 [02:34<00:06,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  96%|█████████▋| 1383/1436 [02:34<00:05,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  96%|█████████▋| 1384/1436 [02:34<00:05,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  96%|█████████▋| 1385/1436 [02:34<00:05,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  97%|█████████▋| 1386/1436 [02:34<00:05,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  97%|█████████▋| 1387/1436 [02:34<00:05,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  97%|█████████▋| 1388/1436 [02:34<00:05,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  97%|█████████▋| 1389/1436 [02:35<00:05,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  97%|█████████▋| 1390/1436 [02:35<00:05,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  97%|█████████▋| 1391/1436 [02:35<00:05,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  97%|█████████▋| 1392/1436 [02:35<00:04,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  97%|█████████▋| 1393/1436 [02:35<00:04,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  97%|█████████▋| 1394/1436 [02:35<00:04,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  97%|█████████▋| 1395/1436 [02:35<00:04,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  97%|█████████▋| 1396/1436 [02:35<00:04,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  97%|█████████▋| 1397/1436 [02:35<00:04,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  97%|█████████▋| 1398/1436 [02:35<00:04,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  97%|█████████▋| 1399/1436 [02:36<00:04,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  97%|█████████▋| 1400/1436 [02:36<00:04,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  98%|█████████▊| 1401/1436 [02:36<00:03,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  98%|█████████▊| 1402/1436 [02:36<00:03,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  98%|█████████▊| 1403/1436 [02:36<00:03,  8.96it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  98%|█████████▊| 1404/1436 [02:36<00:03,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  98%|█████████▊| 1405/1436 [02:36<00:03,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  98%|█████████▊| 1406/1436 [02:36<00:03,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  98%|█████████▊| 1407/1436 [02:36<00:03,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  98%|█████████▊| 1408/1436 [02:36<00:03,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  98%|█████████▊| 1409/1436 [02:37<00:03,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  98%|█████████▊| 1410/1436 [02:37<00:02,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  98%|█████████▊| 1411/1436 [02:37<00:02,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  98%|█████████▊| 1412/1436 [02:37<00:02,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  98%|█████████▊| 1413/1436 [02:37<00:02,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  98%|█████████▊| 1414/1436 [02:37<00:02,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  99%|█████████▊| 1415/1436 [02:37<00:02,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  99%|█████████▊| 1416/1436 [02:37<00:02,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  99%|█████████▊| 1417/1436 [02:37<00:02,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  99%|█████████▊| 1418/1436 [02:38<00:02,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  99%|█████████▉| 1419/1436 [02:38<00:01,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  99%|█████████▉| 1420/1436 [02:38<00:01,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  99%|█████████▉| 1421/1436 [02:38<00:01,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  99%|█████████▉| 1422/1436 [02:38<00:01,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  99%|█████████▉| 1423/1436 [02:38<00:01,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  99%|█████████▉| 1424/1436 [02:38<00:01,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  99%|█████████▉| 1425/1436 [02:38<00:01,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  99%|█████████▉| 1426/1436 [02:38<00:01,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  99%|█████████▉| 1427/1436 [02:39<00:01,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36:  99%|█████████▉| 1428/1436 [02:39<00:00,  8.98it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36: 100%|█████████▉| 1429/1436 [02:39<00:00,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36: 100%|█████████▉| 1430/1436 [02:39<00:00,  8.98it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36: 100%|█████████▉| 1431/1436 [02:39<00:00,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36: 100%|█████████▉| 1432/1436 [02:39<00:00,  8.98it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36: 100%|█████████▉| 1433/1436 [02:39<00:00,  8.97it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36: 100%|█████████▉| 1434/1436 [02:39<00:00,  8.98it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36: 100%|█████████▉| 1435/1436 [02:39<00:00,  8.99it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0865, train_loss_epoch=0.0635]\n",
      "Epoch 36: 100%|██████████| 1436/1436 [02:39<00:00,  8.99it/s, loss=0.0637, v_num=0, train_loss_step=0.0563, val_loss=0.0838, train_loss_epoch=0.0635]\n",
      "Epoch 37:  94%|█████████▍| 1350/1436 [02:30<00:09,  8.99it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 37:  94%|█████████▍| 1351/1436 [02:30<00:09,  8.96it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  94%|█████████▍| 1352/1436 [02:30<00:09,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  94%|█████████▍| 1353/1436 [02:30<00:09,  8.96it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  94%|█████████▍| 1354/1436 [02:31<00:09,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  94%|█████████▍| 1355/1436 [02:31<00:09,  8.96it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  94%|█████████▍| 1356/1436 [02:31<00:08,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  94%|█████████▍| 1357/1436 [02:31<00:08,  8.96it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  95%|█████████▍| 1358/1436 [02:31<00:08,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  95%|█████████▍| 1359/1436 [02:31<00:08,  8.96it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  95%|█████████▍| 1360/1436 [02:31<00:08,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  95%|█████████▍| 1361/1436 [02:31<00:08,  8.96it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  95%|█████████▍| 1362/1436 [02:31<00:08,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  95%|█████████▍| 1363/1436 [02:32<00:08,  8.96it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  95%|█████████▍| 1364/1436 [02:32<00:08,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  95%|█████████▌| 1365/1436 [02:32<00:07,  8.96it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  95%|█████████▌| 1366/1436 [02:32<00:07,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  95%|█████████▌| 1367/1436 [02:32<00:07,  8.96it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  95%|█████████▌| 1368/1436 [02:32<00:07,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  95%|█████████▌| 1369/1436 [02:32<00:07,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  95%|█████████▌| 1370/1436 [02:32<00:07,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  95%|█████████▌| 1371/1436 [02:32<00:07,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  96%|█████████▌| 1372/1436 [02:32<00:07,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  96%|█████████▌| 1373/1436 [02:33<00:07,  8.96it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  96%|█████████▌| 1374/1436 [02:33<00:06,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  96%|█████████▌| 1375/1436 [02:33<00:06,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  96%|█████████▌| 1376/1436 [02:33<00:06,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  96%|█████████▌| 1377/1436 [02:33<00:06,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  96%|█████████▌| 1378/1436 [02:33<00:06,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  96%|█████████▌| 1379/1436 [02:33<00:06,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  96%|█████████▌| 1380/1436 [02:33<00:06,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  96%|█████████▌| 1381/1436 [02:34<00:06,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  96%|█████████▌| 1382/1436 [02:34<00:06,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  96%|█████████▋| 1383/1436 [02:34<00:05,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  96%|█████████▋| 1384/1436 [02:34<00:05,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  96%|█████████▋| 1385/1436 [02:34<00:05,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  97%|█████████▋| 1386/1436 [02:34<00:05,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  97%|█████████▋| 1387/1436 [02:34<00:05,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  97%|█████████▋| 1388/1436 [02:34<00:05,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  97%|█████████▋| 1389/1436 [02:34<00:05,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  97%|█████████▋| 1390/1436 [02:34<00:05,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  97%|█████████▋| 1391/1436 [02:35<00:05,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  97%|█████████▋| 1392/1436 [02:35<00:04,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  97%|█████████▋| 1393/1436 [02:35<00:04,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  97%|█████████▋| 1394/1436 [02:35<00:04,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  97%|█████████▋| 1395/1436 [02:35<00:04,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  97%|█████████▋| 1396/1436 [02:35<00:04,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  97%|█████████▋| 1397/1436 [02:35<00:04,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  97%|█████████▋| 1398/1436 [02:35<00:04,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  97%|█████████▋| 1399/1436 [02:35<00:04,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  97%|█████████▋| 1400/1436 [02:36<00:04,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  98%|█████████▊| 1401/1436 [02:36<00:03,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  98%|█████████▊| 1402/1436 [02:36<00:03,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  98%|█████████▊| 1403/1436 [02:36<00:03,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  98%|█████████▊| 1404/1436 [02:36<00:03,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  98%|█████████▊| 1405/1436 [02:36<00:03,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  98%|█████████▊| 1406/1436 [02:36<00:03,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  98%|█████████▊| 1407/1436 [02:36<00:03,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  98%|█████████▊| 1408/1436 [02:36<00:03,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  98%|█████████▊| 1409/1436 [02:37<00:03,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  98%|█████████▊| 1410/1436 [02:37<00:02,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  98%|█████████▊| 1411/1436 [02:37<00:02,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  98%|█████████▊| 1412/1436 [02:37<00:02,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  98%|█████████▊| 1413/1436 [02:37<00:02,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  98%|█████████▊| 1414/1436 [02:37<00:02,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  99%|█████████▊| 1415/1436 [02:37<00:02,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  99%|█████████▊| 1416/1436 [02:37<00:02,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  99%|█████████▊| 1417/1436 [02:37<00:02,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  99%|█████████▊| 1418/1436 [02:38<00:02,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  99%|█████████▉| 1419/1436 [02:38<00:01,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  99%|█████████▉| 1420/1436 [02:38<00:01,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  99%|█████████▉| 1421/1436 [02:38<00:01,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  99%|█████████▉| 1422/1436 [02:38<00:01,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  99%|█████████▉| 1423/1436 [02:38<00:01,  8.98it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  99%|█████████▉| 1424/1436 [02:38<00:01,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  99%|█████████▉| 1425/1436 [02:38<00:01,  8.98it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  99%|█████████▉| 1426/1436 [02:38<00:01,  8.98it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  99%|█████████▉| 1427/1436 [02:38<00:01,  8.98it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37:  99%|█████████▉| 1428/1436 [02:39<00:00,  8.98it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37: 100%|█████████▉| 1429/1436 [02:39<00:00,  8.98it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37: 100%|█████████▉| 1430/1436 [02:39<00:00,  8.98it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37: 100%|█████████▉| 1431/1436 [02:39<00:00,  8.98it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37: 100%|█████████▉| 1432/1436 [02:39<00:00,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37: 100%|█████████▉| 1433/1436 [02:39<00:00,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37: 100%|█████████▉| 1434/1436 [02:39<00:00,  8.97it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37: 100%|█████████▉| 1435/1436 [02:39<00:00,  8.98it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 37: 100%|██████████| 1436/1436 [02:39<00:00,  8.98it/s, loss=0.0609, v_num=0, train_loss_step=0.0443, val_loss=0.0838, train_loss_epoch=0.0633]\n",
      "Epoch 38:  94%|█████████▍| 1350/1436 [02:29<00:09,  9.04it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 38:  94%|█████████▍| 1351/1436 [02:30<00:09,  9.00it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  94%|█████████▍| 1352/1436 [02:30<00:09,  9.01it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  94%|█████████▍| 1353/1436 [02:30<00:09,  9.01it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  94%|█████████▍| 1354/1436 [02:30<00:09,  9.01it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  94%|█████████▍| 1355/1436 [02:30<00:08,  9.01it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  94%|█████████▍| 1356/1436 [02:30<00:08,  9.01it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  94%|█████████▍| 1357/1436 [02:30<00:08,  9.00it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  95%|█████████▍| 1358/1436 [02:30<00:08,  9.01it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  95%|█████████▍| 1359/1436 [02:30<00:08,  9.00it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  95%|█████████▍| 1360/1436 [02:31<00:08,  9.01it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  95%|█████████▍| 1361/1436 [02:31<00:08,  9.00it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  95%|█████████▍| 1362/1436 [02:31<00:08,  9.00it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  95%|█████████▍| 1363/1436 [02:31<00:08,  9.00it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  95%|█████████▍| 1364/1436 [02:31<00:07,  9.00it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  95%|█████████▌| 1365/1436 [02:31<00:07,  9.00it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  95%|█████████▌| 1366/1436 [02:31<00:07,  9.00it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  95%|█████████▌| 1367/1436 [02:31<00:07,  8.99it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  95%|█████████▌| 1368/1436 [02:31<00:07,  9.00it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  95%|█████████▌| 1369/1436 [02:32<00:07,  9.00it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  95%|█████████▌| 1370/1436 [02:32<00:07,  9.00it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  95%|█████████▌| 1371/1436 [02:32<00:07,  9.00it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  96%|█████████▌| 1372/1436 [02:32<00:07,  9.00it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  96%|█████████▌| 1373/1436 [02:32<00:07,  8.99it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  96%|█████████▌| 1374/1436 [02:32<00:06,  9.00it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  96%|█████████▌| 1375/1436 [02:32<00:06,  8.99it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  96%|█████████▌| 1376/1436 [02:32<00:06,  9.00it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  96%|█████████▌| 1377/1436 [02:33<00:06,  8.99it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  96%|█████████▌| 1378/1436 [02:33<00:06,  8.99it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  96%|█████████▌| 1379/1436 [02:33<00:06,  8.99it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  96%|█████████▌| 1380/1436 [02:33<00:06,  8.99it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  96%|█████████▌| 1381/1436 [02:33<00:06,  8.99it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  96%|█████████▌| 1382/1436 [02:33<00:06,  8.99it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  96%|█████████▋| 1383/1436 [02:34<00:05,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  96%|█████████▋| 1384/1436 [02:34<00:05,  8.99it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  96%|█████████▋| 1385/1436 [02:34<00:05,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  97%|█████████▋| 1386/1436 [02:34<00:05,  8.99it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  97%|█████████▋| 1387/1436 [02:34<00:05,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  97%|█████████▋| 1388/1436 [02:34<00:05,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  97%|█████████▋| 1389/1436 [02:34<00:05,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  97%|█████████▋| 1390/1436 [02:34<00:05,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  97%|█████████▋| 1391/1436 [02:34<00:05,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  97%|█████████▋| 1392/1436 [02:34<00:04,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  97%|█████████▋| 1393/1436 [02:35<00:04,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  97%|█████████▋| 1394/1436 [02:35<00:04,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  97%|█████████▋| 1395/1436 [02:35<00:04,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  97%|█████████▋| 1396/1436 [02:35<00:04,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  97%|█████████▋| 1397/1436 [02:35<00:04,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  97%|█████████▋| 1398/1436 [02:35<00:04,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  97%|█████████▋| 1399/1436 [02:35<00:04,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  97%|█████████▋| 1400/1436 [02:35<00:04,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  98%|█████████▊| 1401/1436 [02:36<00:03,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  98%|█████████▊| 1402/1436 [02:36<00:03,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  98%|█████████▊| 1403/1436 [02:36<00:03,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  98%|█████████▊| 1404/1436 [02:36<00:03,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  98%|█████████▊| 1405/1436 [02:36<00:03,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  98%|█████████▊| 1406/1436 [02:36<00:03,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  98%|█████████▊| 1407/1436 [02:36<00:03,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  98%|█████████▊| 1408/1436 [02:36<00:03,  8.98it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  98%|█████████▊| 1409/1436 [02:37<00:03,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  98%|█████████▊| 1410/1436 [02:37<00:02,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  98%|█████████▊| 1411/1436 [02:37<00:02,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  98%|█████████▊| 1412/1436 [02:37<00:02,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  98%|█████████▊| 1413/1436 [02:37<00:02,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  98%|█████████▊| 1414/1436 [02:37<00:02,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  99%|█████████▊| 1415/1436 [02:37<00:02,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  99%|█████████▊| 1416/1436 [02:37<00:02,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  99%|█████████▊| 1417/1436 [02:38<00:02,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  99%|█████████▊| 1418/1436 [02:38<00:02,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  99%|█████████▉| 1419/1436 [02:38<00:01,  8.96it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  99%|█████████▉| 1420/1436 [02:38<00:01,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  99%|█████████▉| 1421/1436 [02:38<00:01,  8.96it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  99%|█████████▉| 1422/1436 [02:38<00:01,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  99%|█████████▉| 1423/1436 [02:38<00:01,  8.96it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  99%|█████████▉| 1424/1436 [02:38<00:01,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  99%|█████████▉| 1425/1436 [02:38<00:01,  8.96it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  99%|█████████▉| 1426/1436 [02:39<00:01,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  99%|█████████▉| 1427/1436 [02:39<00:01,  8.96it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38:  99%|█████████▉| 1428/1436 [02:39<00:00,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38: 100%|█████████▉| 1429/1436 [02:39<00:00,  8.96it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38: 100%|█████████▉| 1430/1436 [02:39<00:00,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38: 100%|█████████▉| 1431/1436 [02:39<00:00,  8.96it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38: 100%|█████████▉| 1432/1436 [02:39<00:00,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38: 100%|█████████▉| 1433/1436 [02:39<00:00,  8.96it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38: 100%|█████████▉| 1434/1436 [02:39<00:00,  8.96it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38: 100%|█████████▉| 1435/1436 [02:39<00:00,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0838, train_loss_epoch=0.0632]\n",
      "Epoch 38: 100%|██████████| 1436/1436 [02:40<00:00,  8.97it/s, loss=0.0623, v_num=0, train_loss_step=0.058, val_loss=0.0854, train_loss_epoch=0.0632]\n",
      "Epoch 39:  94%|█████████▍| 1350/1436 [02:30<00:09,  8.98it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 39:  94%|█████████▍| 1351/1436 [02:30<00:09,  8.95it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  94%|█████████▍| 1352/1436 [02:30<00:09,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  94%|█████████▍| 1353/1436 [02:31<00:09,  8.95it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  94%|█████████▍| 1354/1436 [02:31<00:09,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  94%|█████████▍| 1355/1436 [02:31<00:09,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  94%|█████████▍| 1356/1436 [02:31<00:08,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  94%|█████████▍| 1357/1436 [02:31<00:08,  8.95it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  95%|█████████▍| 1358/1436 [02:31<00:08,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  95%|█████████▍| 1359/1436 [02:31<00:08,  8.95it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  95%|█████████▍| 1360/1436 [02:31<00:08,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  95%|█████████▍| 1361/1436 [02:31<00:08,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  95%|█████████▍| 1362/1436 [02:32<00:08,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  95%|█████████▍| 1363/1436 [02:32<00:08,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  95%|█████████▍| 1364/1436 [02:32<00:08,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  95%|█████████▌| 1365/1436 [02:32<00:07,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  95%|█████████▌| 1366/1436 [02:32<00:07,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  95%|█████████▌| 1367/1436 [02:32<00:07,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  95%|█████████▌| 1368/1436 [02:32<00:07,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  95%|█████████▌| 1369/1436 [02:32<00:07,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  95%|█████████▌| 1370/1436 [02:32<00:07,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  95%|█████████▌| 1371/1436 [02:33<00:07,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  96%|█████████▌| 1372/1436 [02:33<00:07,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  96%|█████████▌| 1373/1436 [02:33<00:07,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  96%|█████████▌| 1374/1436 [02:33<00:06,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  96%|█████████▌| 1375/1436 [02:33<00:06,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  96%|█████████▌| 1376/1436 [02:33<00:06,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  96%|█████████▌| 1377/1436 [02:33<00:06,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  96%|█████████▌| 1378/1436 [02:33<00:06,  8.96it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  96%|█████████▌| 1379/1436 [02:33<00:06,  8.95it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  96%|█████████▌| 1380/1436 [02:34<00:06,  8.95it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  96%|█████████▌| 1381/1436 [02:34<00:06,  8.95it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  96%|█████████▌| 1382/1436 [02:34<00:06,  8.95it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  96%|█████████▋| 1383/1436 [02:34<00:05,  8.95it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  96%|█████████▋| 1384/1436 [02:34<00:05,  8.95it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  96%|█████████▋| 1385/1436 [02:34<00:05,  8.95it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  97%|█████████▋| 1386/1436 [02:34<00:05,  8.95it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  97%|█████████▋| 1387/1436 [02:35<00:05,  8.94it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  97%|█████████▋| 1388/1436 [02:35<00:05,  8.95it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  97%|█████████▋| 1389/1436 [02:35<00:05,  8.94it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  97%|█████████▋| 1390/1436 [02:35<00:05,  8.95it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  97%|█████████▋| 1391/1436 [02:35<00:05,  8.94it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  97%|█████████▋| 1392/1436 [02:35<00:04,  8.95it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  97%|█████████▋| 1393/1436 [02:35<00:04,  8.94it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  97%|█████████▋| 1394/1436 [02:35<00:04,  8.95it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  97%|█████████▋| 1395/1436 [02:35<00:04,  8.95it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  97%|█████████▋| 1396/1436 [02:36<00:04,  8.95it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  97%|█████████▋| 1397/1436 [02:36<00:04,  8.94it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  97%|█████████▋| 1398/1436 [02:36<00:04,  8.95it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  97%|█████████▋| 1399/1436 [02:36<00:04,  8.94it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  97%|█████████▋| 1400/1436 [02:36<00:04,  8.94it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  98%|█████████▊| 1401/1436 [02:36<00:03,  8.94it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  98%|█████████▊| 1402/1436 [02:36<00:03,  8.94it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  98%|█████████▊| 1403/1436 [02:37<00:03,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  98%|█████████▊| 1404/1436 [02:37<00:03,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  98%|█████████▊| 1405/1436 [02:37<00:03,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  98%|█████████▊| 1406/1436 [02:37<00:03,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  98%|█████████▊| 1407/1436 [02:37<00:03,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  98%|█████████▊| 1408/1436 [02:37<00:03,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  98%|█████████▊| 1409/1436 [02:37<00:03,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  98%|█████████▊| 1410/1436 [02:37<00:02,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  98%|█████████▊| 1411/1436 [02:38<00:02,  8.92it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  98%|█████████▊| 1412/1436 [02:38<00:02,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  98%|█████████▊| 1413/1436 [02:38<00:02,  8.92it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  98%|█████████▊| 1414/1436 [02:38<00:02,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  99%|█████████▊| 1415/1436 [02:38<00:02,  8.92it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  99%|█████████▊| 1416/1436 [02:38<00:02,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  99%|█████████▊| 1417/1436 [02:38<00:02,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  99%|█████████▊| 1418/1436 [02:38<00:02,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  99%|█████████▉| 1419/1436 [02:38<00:01,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  99%|█████████▉| 1420/1436 [02:39<00:01,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  99%|█████████▉| 1421/1436 [02:39<00:01,  8.92it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  99%|█████████▉| 1422/1436 [02:39<00:01,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  99%|█████████▉| 1423/1436 [02:39<00:01,  8.92it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  99%|█████████▉| 1424/1436 [02:39<00:01,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  99%|█████████▉| 1425/1436 [02:39<00:01,  8.92it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  99%|█████████▉| 1426/1436 [02:39<00:01,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  99%|█████████▉| 1427/1436 [02:39<00:01,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39:  99%|█████████▉| 1428/1436 [02:39<00:00,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39: 100%|█████████▉| 1429/1436 [02:40<00:00,  8.92it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39: 100%|█████████▉| 1430/1436 [02:40<00:00,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39: 100%|█████████▉| 1431/1436 [02:40<00:00,  8.92it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39: 100%|█████████▉| 1432/1436 [02:40<00:00,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39: 100%|█████████▉| 1433/1436 [02:40<00:00,  8.92it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39: 100%|█████████▉| 1434/1436 [02:40<00:00,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39: 100%|█████████▉| 1435/1436 [02:40<00:00,  8.93it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0854, train_loss_epoch=0.0629]\n",
      "Epoch 39: 100%|██████████| 1436/1436 [02:40<00:00,  8.94it/s, loss=0.0591, v_num=0, train_loss_step=0.061, val_loss=0.0857, train_loss_epoch=0.0629]\n",
      "Epoch 40:  94%|█████████▍| 1350/1436 [02:27<00:09,  9.16it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 40:  94%|█████████▍| 1351/1436 [02:27<00:09,  9.13it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  94%|█████████▍| 1352/1436 [02:28<00:09,  9.13it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  94%|█████████▍| 1353/1436 [02:28<00:09,  9.13it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  94%|█████████▍| 1354/1436 [02:28<00:08,  9.13it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  94%|█████████▍| 1355/1436 [02:28<00:08,  9.13it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  94%|█████████▍| 1356/1436 [02:28<00:08,  9.13it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  94%|█████████▍| 1357/1436 [02:28<00:08,  9.13it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  95%|█████████▍| 1358/1436 [02:28<00:08,  9.13it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  95%|█████████▍| 1359/1436 [02:28<00:08,  9.13it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  95%|█████████▍| 1360/1436 [02:28<00:08,  9.13it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  95%|█████████▍| 1361/1436 [02:29<00:08,  9.13it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  95%|█████████▍| 1362/1436 [02:29<00:08,  9.13it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  95%|█████████▍| 1363/1436 [02:29<00:07,  9.13it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  95%|█████████▍| 1364/1436 [02:29<00:07,  9.13it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  95%|█████████▌| 1365/1436 [02:29<00:07,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  95%|█████████▌| 1366/1436 [02:29<00:07,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  95%|█████████▌| 1367/1436 [02:29<00:07,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  95%|█████████▌| 1368/1436 [02:29<00:07,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  95%|█████████▌| 1369/1436 [02:30<00:07,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  95%|█████████▌| 1370/1436 [02:30<00:07,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  95%|█████████▌| 1371/1436 [02:30<00:07,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  96%|█████████▌| 1372/1436 [02:30<00:07,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  96%|█████████▌| 1373/1436 [02:30<00:06,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  96%|█████████▌| 1374/1436 [02:30<00:06,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  96%|█████████▌| 1375/1436 [02:30<00:06,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  96%|█████████▌| 1376/1436 [02:30<00:06,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  96%|█████████▌| 1377/1436 [02:30<00:06,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  96%|█████████▌| 1378/1436 [02:31<00:06,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  96%|█████████▌| 1379/1436 [02:31<00:06,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  96%|█████████▌| 1380/1436 [02:31<00:06,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  96%|█████████▌| 1381/1436 [02:31<00:06,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  96%|█████████▌| 1382/1436 [02:31<00:05,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  96%|█████████▋| 1383/1436 [02:31<00:05,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  96%|█████████▋| 1384/1436 [02:31<00:05,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  96%|█████████▋| 1385/1436 [02:31<00:05,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  97%|█████████▋| 1386/1436 [02:32<00:05,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  97%|█████████▋| 1387/1436 [02:32<00:05,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  97%|█████████▋| 1388/1436 [02:32<00:05,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  97%|█████████▋| 1389/1436 [02:32<00:05,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  97%|█████████▋| 1390/1436 [02:32<00:05,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  97%|█████████▋| 1391/1436 [02:32<00:04,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  97%|█████████▋| 1392/1436 [02:32<00:04,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  97%|█████████▋| 1393/1436 [02:32<00:04,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  97%|█████████▋| 1394/1436 [02:32<00:04,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  97%|█████████▋| 1395/1436 [02:33<00:04,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  97%|█████████▋| 1396/1436 [02:33<00:04,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  97%|█████████▋| 1397/1436 [02:33<00:04,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  97%|█████████▋| 1398/1436 [02:33<00:04,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  97%|█████████▋| 1399/1436 [02:33<00:04,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  97%|█████████▋| 1400/1436 [02:33<00:03,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  98%|█████████▊| 1401/1436 [02:33<00:03,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  98%|█████████▊| 1402/1436 [02:33<00:03,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  98%|█████████▊| 1403/1436 [02:33<00:03,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  98%|█████████▊| 1404/1436 [02:34<00:03,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  98%|█████████▊| 1405/1436 [02:34<00:03,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  98%|█████████▊| 1406/1436 [02:34<00:03,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  98%|█████████▊| 1407/1436 [02:34<00:03,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  98%|█████████▊| 1408/1436 [02:34<00:03,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  98%|█████████▊| 1409/1436 [02:34<00:02,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  98%|█████████▊| 1410/1436 [02:34<00:02,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  98%|█████████▊| 1411/1436 [02:34<00:02,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  98%|█████████▊| 1412/1436 [02:34<00:02,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  98%|█████████▊| 1413/1436 [02:35<00:02,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  98%|█████████▊| 1414/1436 [02:35<00:02,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  99%|█████████▊| 1415/1436 [02:35<00:02,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  99%|█████████▊| 1416/1436 [02:35<00:02,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  99%|█████████▊| 1417/1436 [02:35<00:02,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  99%|█████████▊| 1418/1436 [02:35<00:01,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  99%|█████████▉| 1419/1436 [02:35<00:01,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  99%|█████████▉| 1420/1436 [02:35<00:01,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  99%|█████████▉| 1421/1436 [02:36<00:01,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  99%|█████████▉| 1422/1436 [02:36<00:01,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  99%|█████████▉| 1423/1436 [02:36<00:01,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  99%|█████████▉| 1424/1436 [02:36<00:01,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  99%|█████████▉| 1425/1436 [02:36<00:01,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  99%|█████████▉| 1426/1436 [02:36<00:01,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  99%|█████████▉| 1427/1436 [02:36<00:00,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40:  99%|█████████▉| 1428/1436 [02:36<00:00,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40: 100%|█████████▉| 1429/1436 [02:36<00:00,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40: 100%|█████████▉| 1430/1436 [02:37<00:00,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40: 100%|█████████▉| 1431/1436 [02:37<00:00,  9.10it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40: 100%|█████████▉| 1432/1436 [02:37<00:00,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40: 100%|█████████▉| 1433/1436 [02:37<00:00,  9.10it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40: 100%|█████████▉| 1434/1436 [02:37<00:00,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40: 100%|█████████▉| 1435/1436 [02:37<00:00,  9.11it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0857, train_loss_epoch=0.0627]\n",
      "Epoch 40: 100%|██████████| 1436/1436 [02:37<00:00,  9.12it/s, loss=0.0647, v_num=0, train_loss_step=0.0934, val_loss=0.0846, train_loss_epoch=0.0627]\n",
      "Epoch 42:  94%|█████████▍| 1350/1436 [02:30<00:09,  8.99it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 42:  94%|█████████▍| 1351/1436 [02:30<00:09,  8.96it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  94%|█████████▍| 1352/1436 [02:30<00:09,  8.96it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  94%|█████████▍| 1353/1436 [02:31<00:09,  8.96it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  94%|█████████▍| 1354/1436 [02:31<00:09,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  94%|█████████▍| 1355/1436 [02:31<00:09,  8.96it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  94%|█████████▍| 1356/1436 [02:31<00:08,  8.96it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  94%|█████████▍| 1357/1436 [02:31<00:08,  8.96it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  95%|█████████▍| 1358/1436 [02:31<00:08,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  95%|█████████▍| 1359/1436 [02:31<00:08,  8.96it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  95%|█████████▍| 1360/1436 [02:31<00:08,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  95%|█████████▍| 1361/1436 [02:31<00:08,  8.96it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  95%|█████████▍| 1362/1436 [02:31<00:08,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  95%|█████████▍| 1363/1436 [02:32<00:08,  8.96it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  95%|█████████▍| 1364/1436 [02:32<00:08,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  95%|█████████▌| 1365/1436 [02:32<00:07,  8.96it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  95%|█████████▌| 1366/1436 [02:32<00:07,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  95%|█████████▌| 1367/1436 [02:32<00:07,  8.96it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  95%|█████████▌| 1368/1436 [02:32<00:07,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  95%|█████████▌| 1369/1436 [02:32<00:07,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  95%|█████████▌| 1370/1436 [02:32<00:07,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  95%|█████████▌| 1371/1436 [02:32<00:07,  8.96it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  96%|█████████▌| 1372/1436 [02:33<00:07,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  96%|█████████▌| 1373/1436 [02:33<00:07,  8.96it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  96%|█████████▌| 1374/1436 [02:33<00:06,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  96%|█████████▌| 1375/1436 [02:33<00:06,  8.96it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  96%|█████████▌| 1376/1436 [02:33<00:06,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  96%|█████████▌| 1377/1436 [02:33<00:06,  8.96it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  96%|█████████▌| 1378/1436 [02:33<00:06,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  96%|█████████▌| 1379/1436 [02:33<00:06,  8.96it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  96%|█████████▌| 1380/1436 [02:33<00:06,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  96%|█████████▌| 1381/1436 [02:34<00:06,  8.96it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  96%|█████████▌| 1382/1436 [02:34<00:06,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  96%|█████████▋| 1383/1436 [02:34<00:05,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  96%|█████████▋| 1384/1436 [02:34<00:05,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  96%|█████████▋| 1385/1436 [02:34<00:05,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  97%|█████████▋| 1386/1436 [02:34<00:05,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  97%|█████████▋| 1387/1436 [02:34<00:05,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  97%|█████████▋| 1388/1436 [02:34<00:05,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  97%|█████████▋| 1389/1436 [02:34<00:05,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  97%|█████████▋| 1390/1436 [02:34<00:05,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  97%|█████████▋| 1391/1436 [02:35<00:05,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  97%|█████████▋| 1392/1436 [02:35<00:04,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  97%|█████████▋| 1393/1436 [02:35<00:04,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  97%|█████████▋| 1394/1436 [02:35<00:04,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  97%|█████████▋| 1395/1436 [02:35<00:04,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  97%|█████████▋| 1396/1436 [02:35<00:04,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  97%|█████████▋| 1397/1436 [02:35<00:04,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  97%|█████████▋| 1398/1436 [02:35<00:04,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  97%|█████████▋| 1399/1436 [02:35<00:04,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  97%|█████████▋| 1400/1436 [02:35<00:04,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  98%|█████████▊| 1401/1436 [02:36<00:03,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  98%|█████████▊| 1402/1436 [02:36<00:03,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  98%|█████████▊| 1403/1436 [02:36<00:03,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  98%|█████████▊| 1404/1436 [02:36<00:03,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  98%|█████████▊| 1405/1436 [02:36<00:03,  8.97it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  98%|█████████▊| 1406/1436 [02:36<00:03,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  98%|█████████▊| 1407/1436 [02:36<00:03,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  98%|█████████▊| 1408/1436 [02:36<00:03,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  98%|█████████▊| 1409/1436 [02:36<00:03,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  98%|█████████▊| 1410/1436 [02:37<00:02,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  98%|█████████▊| 1411/1436 [02:37<00:02,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  98%|█████████▊| 1412/1436 [02:37<00:02,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  98%|█████████▊| 1413/1436 [02:37<00:02,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  98%|█████████▊| 1414/1436 [02:37<00:02,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  99%|█████████▊| 1415/1436 [02:37<00:02,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  99%|█████████▊| 1416/1436 [02:37<00:02,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  99%|█████████▊| 1417/1436 [02:37<00:02,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  99%|█████████▊| 1418/1436 [02:37<00:02,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  99%|█████████▉| 1419/1436 [02:38<00:01,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  99%|█████████▉| 1420/1436 [02:38<00:01,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  99%|█████████▉| 1421/1436 [02:38<00:01,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  99%|█████████▉| 1422/1436 [02:38<00:01,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  99%|█████████▉| 1423/1436 [02:38<00:01,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  99%|█████████▉| 1424/1436 [02:38<00:01,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  99%|█████████▉| 1425/1436 [02:38<00:01,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  99%|█████████▉| 1426/1436 [02:38<00:01,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  99%|█████████▉| 1427/1436 [02:38<00:01,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42:  99%|█████████▉| 1428/1436 [02:38<00:00,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42: 100%|█████████▉| 1429/1436 [02:39<00:00,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42: 100%|█████████▉| 1430/1436 [02:39<00:00,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42: 100%|█████████▉| 1431/1436 [02:39<00:00,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42: 100%|█████████▉| 1432/1436 [02:39<00:00,  8.98it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42: 100%|█████████▉| 1433/1436 [02:39<00:00,  8.99it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42: 100%|█████████▉| 1434/1436 [02:39<00:00,  8.99it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42: 100%|█████████▉| 1435/1436 [02:39<00:00,  8.99it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0855, train_loss_epoch=0.0623]\n",
      "Epoch 42: 100%|██████████| 1436/1436 [02:39<00:00,  9.00it/s, loss=0.0685, v_num=0, train_loss_step=0.0637, val_loss=0.0853, train_loss_epoch=0.0623]\n",
      "Epoch 43:  94%|█████████▍| 1350/1436 [02:29<00:09,  9.02it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 43:  94%|█████████▍| 1351/1436 [02:30<00:09,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  94%|█████████▍| 1352/1436 [02:30<00:09,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  94%|█████████▍| 1353/1436 [02:30<00:09,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  94%|█████████▍| 1354/1436 [02:30<00:09,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  94%|█████████▍| 1355/1436 [02:30<00:09,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  94%|█████████▍| 1356/1436 [02:30<00:08,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  94%|█████████▍| 1357/1436 [02:30<00:08,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  95%|█████████▍| 1358/1436 [02:31<00:08,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  95%|█████████▍| 1359/1436 [02:31<00:08,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  95%|█████████▍| 1360/1436 [02:31<00:08,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  95%|█████████▍| 1361/1436 [02:31<00:08,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  95%|█████████▍| 1362/1436 [02:31<00:08,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  95%|█████████▍| 1363/1436 [02:31<00:08,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  95%|█████████▍| 1364/1436 [02:31<00:08,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  95%|█████████▌| 1365/1436 [02:31<00:07,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  95%|█████████▌| 1366/1436 [02:31<00:07,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  95%|█████████▌| 1367/1436 [02:32<00:07,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  95%|█████████▌| 1368/1436 [02:32<00:07,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  95%|█████████▌| 1369/1436 [02:32<00:07,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  95%|█████████▌| 1370/1436 [02:32<00:07,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  95%|█████████▌| 1371/1436 [02:32<00:07,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  96%|█████████▌| 1372/1436 [02:32<00:07,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  96%|█████████▌| 1373/1436 [02:32<00:07,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  96%|█████████▌| 1374/1436 [02:32<00:06,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  96%|█████████▌| 1375/1436 [02:32<00:06,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  96%|█████████▌| 1376/1436 [02:33<00:06,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  96%|█████████▌| 1377/1436 [02:33<00:06,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  96%|█████████▌| 1378/1436 [02:33<00:06,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  96%|█████████▌| 1379/1436 [02:33<00:06,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  96%|█████████▌| 1380/1436 [02:33<00:06,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  96%|█████████▌| 1381/1436 [02:33<00:06,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  96%|█████████▌| 1382/1436 [02:33<00:06,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  96%|█████████▋| 1383/1436 [02:33<00:05,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  96%|█████████▋| 1384/1436 [02:34<00:05,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  96%|█████████▋| 1385/1436 [02:34<00:05,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  97%|█████████▋| 1386/1436 [02:34<00:05,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  97%|█████████▋| 1387/1436 [02:34<00:05,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  97%|█████████▋| 1388/1436 [02:34<00:05,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  97%|█████████▋| 1389/1436 [02:34<00:05,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  97%|█████████▋| 1390/1436 [02:34<00:05,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  97%|█████████▋| 1391/1436 [02:34<00:05,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  97%|█████████▋| 1392/1436 [02:34<00:04,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  97%|█████████▋| 1393/1436 [02:35<00:04,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  97%|█████████▋| 1394/1436 [02:35<00:04,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  97%|█████████▋| 1395/1436 [02:35<00:04,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  97%|█████████▋| 1396/1436 [02:35<00:04,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  97%|█████████▋| 1397/1436 [02:35<00:04,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  97%|█████████▋| 1398/1436 [02:35<00:04,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  97%|█████████▋| 1399/1436 [02:35<00:04,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  97%|█████████▋| 1400/1436 [02:35<00:04,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  98%|█████████▊| 1401/1436 [02:35<00:03,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  98%|█████████▊| 1402/1436 [02:36<00:03,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  98%|█████████▊| 1403/1436 [02:36<00:03,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  98%|█████████▊| 1404/1436 [02:36<00:03,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  98%|█████████▊| 1405/1436 [02:36<00:03,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  98%|█████████▊| 1406/1436 [02:36<00:03,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  98%|█████████▊| 1407/1436 [02:36<00:03,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  98%|█████████▊| 1408/1436 [02:36<00:03,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  98%|█████████▊| 1409/1436 [02:36<00:03,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  98%|█████████▊| 1410/1436 [02:36<00:02,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  98%|█████████▊| 1411/1436 [02:37<00:02,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  98%|█████████▊| 1412/1436 [02:37<00:02,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  98%|█████████▊| 1413/1436 [02:37<00:02,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  98%|█████████▊| 1414/1436 [02:37<00:02,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  99%|█████████▊| 1415/1436 [02:37<00:02,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  99%|█████████▊| 1416/1436 [02:37<00:02,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  99%|█████████▊| 1417/1436 [02:37<00:02,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  99%|█████████▊| 1418/1436 [02:37<00:02,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  99%|█████████▉| 1419/1436 [02:38<00:01,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  99%|█████████▉| 1420/1436 [02:38<00:01,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  99%|█████████▉| 1421/1436 [02:38<00:01,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  99%|█████████▉| 1422/1436 [02:38<00:01,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  99%|█████████▉| 1423/1436 [02:38<00:01,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  99%|█████████▉| 1424/1436 [02:38<00:01,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  99%|█████████▉| 1425/1436 [02:38<00:01,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  99%|█████████▉| 1426/1436 [02:38<00:01,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  99%|█████████▉| 1427/1436 [02:38<00:01,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43:  99%|█████████▉| 1428/1436 [02:38<00:00,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43: 100%|█████████▉| 1429/1436 [02:39<00:00,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43: 100%|█████████▉| 1430/1436 [02:39<00:00,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43: 100%|█████████▉| 1431/1436 [02:39<00:00,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43: 100%|█████████▉| 1432/1436 [02:39<00:00,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43: 100%|█████████▉| 1433/1436 [02:39<00:00,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43: 100%|█████████▉| 1434/1436 [02:39<00:00,  8.98it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43: 100%|█████████▉| 1435/1436 [02:39<00:00,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0853, train_loss_epoch=0.0622]\n",
      "Epoch 43: 100%|██████████| 1436/1436 [02:39<00:00,  8.99it/s, loss=0.0624, v_num=0, train_loss_step=0.054, val_loss=0.0845, train_loss_epoch=0.0622]\n",
      "Epoch 44:  94%|█████████▍| 1350/1436 [02:27<00:09,  9.14it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 44:  94%|█████████▍| 1351/1436 [02:28<00:09,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  94%|█████████▍| 1352/1436 [02:28<00:09,  9.12it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  94%|█████████▍| 1353/1436 [02:28<00:09,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  94%|█████████▍| 1354/1436 [02:28<00:08,  9.12it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  94%|█████████▍| 1355/1436 [02:28<00:08,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  94%|█████████▍| 1356/1436 [02:28<00:08,  9.12it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  94%|█████████▍| 1357/1436 [02:28<00:08,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  95%|█████████▍| 1358/1436 [02:28<00:08,  9.12it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  95%|█████████▍| 1359/1436 [02:29<00:08,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  95%|█████████▍| 1360/1436 [02:29<00:08,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  95%|█████████▍| 1361/1436 [02:29<00:08,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  95%|█████████▍| 1362/1436 [02:29<00:08,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  95%|█████████▍| 1363/1436 [02:29<00:08,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  95%|█████████▍| 1364/1436 [02:29<00:07,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  95%|█████████▌| 1365/1436 [02:29<00:07,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  95%|█████████▌| 1366/1436 [02:29<00:07,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  95%|█████████▌| 1367/1436 [02:30<00:07,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  95%|█████████▌| 1368/1436 [02:30<00:07,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  95%|█████████▌| 1369/1436 [02:30<00:07,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  95%|█████████▌| 1370/1436 [02:30<00:07,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  95%|█████████▌| 1371/1436 [02:30<00:07,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  96%|█████████▌| 1372/1436 [02:30<00:07,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  96%|█████████▌| 1373/1436 [02:30<00:06,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  96%|█████████▌| 1374/1436 [02:30<00:06,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  96%|█████████▌| 1375/1436 [02:30<00:06,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  96%|█████████▌| 1376/1436 [02:30<00:06,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  96%|█████████▌| 1377/1436 [02:31<00:06,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  96%|█████████▌| 1378/1436 [02:31<00:06,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  96%|█████████▌| 1379/1436 [02:31<00:06,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  96%|█████████▌| 1380/1436 [02:31<00:06,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  96%|█████████▌| 1381/1436 [02:31<00:06,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  96%|█████████▌| 1382/1436 [02:31<00:05,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  96%|█████████▋| 1383/1436 [02:31<00:05,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  96%|█████████▋| 1384/1436 [02:31<00:05,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  96%|█████████▋| 1385/1436 [02:32<00:05,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  97%|█████████▋| 1386/1436 [02:32<00:05,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  97%|█████████▋| 1387/1436 [02:32<00:05,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  97%|█████████▋| 1388/1436 [02:32<00:05,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  97%|█████████▋| 1389/1436 [02:32<00:05,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  97%|█████████▋| 1390/1436 [02:32<00:05,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  97%|█████████▋| 1391/1436 [02:32<00:04,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  97%|█████████▋| 1392/1436 [02:32<00:04,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  97%|█████████▋| 1393/1436 [02:32<00:04,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  97%|█████████▋| 1394/1436 [02:32<00:04,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  97%|█████████▋| 1395/1436 [02:33<00:04,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  97%|█████████▋| 1396/1436 [02:33<00:04,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  97%|█████████▋| 1397/1436 [02:33<00:04,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  97%|█████████▋| 1398/1436 [02:33<00:04,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  97%|█████████▋| 1399/1436 [02:33<00:04,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  97%|█████████▋| 1400/1436 [02:33<00:03,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  98%|█████████▊| 1401/1436 [02:33<00:03,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  98%|█████████▊| 1402/1436 [02:33<00:03,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  98%|█████████▊| 1403/1436 [02:33<00:03,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  98%|█████████▊| 1404/1436 [02:34<00:03,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  98%|█████████▊| 1405/1436 [02:34<00:03,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  98%|█████████▊| 1406/1436 [02:34<00:03,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  98%|█████████▊| 1407/1436 [02:34<00:03,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  98%|█████████▊| 1408/1436 [02:34<00:03,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  98%|█████████▊| 1409/1436 [02:34<00:02,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  98%|█████████▊| 1410/1436 [02:34<00:02,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  98%|█████████▊| 1411/1436 [02:34<00:02,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  98%|█████████▊| 1412/1436 [02:34<00:02,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  98%|█████████▊| 1413/1436 [02:35<00:02,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  98%|█████████▊| 1414/1436 [02:35<00:02,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  99%|█████████▊| 1415/1436 [02:35<00:02,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  99%|█████████▊| 1416/1436 [02:35<00:02,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  99%|█████████▊| 1417/1436 [02:35<00:02,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  99%|█████████▊| 1418/1436 [02:35<00:01,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  99%|█████████▉| 1419/1436 [02:35<00:01,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  99%|█████████▉| 1420/1436 [02:35<00:01,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  99%|█████████▉| 1421/1436 [02:36<00:01,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  99%|█████████▉| 1422/1436 [02:36<00:01,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  99%|█████████▉| 1423/1436 [02:36<00:01,  9.11it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  99%|█████████▉| 1424/1436 [02:36<00:01,  9.10it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  99%|█████████▉| 1425/1436 [02:36<00:01,  9.10it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  99%|█████████▉| 1426/1436 [02:36<00:01,  9.10it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  99%|█████████▉| 1427/1436 [02:36<00:00,  9.10it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44:  99%|█████████▉| 1428/1436 [02:36<00:00,  9.10it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44: 100%|█████████▉| 1429/1436 [02:37<00:00,  9.10it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44: 100%|█████████▉| 1430/1436 [02:37<00:00,  9.09it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44: 100%|█████████▉| 1431/1436 [02:37<00:00,  9.09it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44: 100%|█████████▉| 1432/1436 [02:37<00:00,  9.09it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44: 100%|█████████▉| 1433/1436 [02:37<00:00,  9.09it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44: 100%|█████████▉| 1434/1436 [02:37<00:00,  9.09it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44: 100%|█████████▉| 1435/1436 [02:37<00:00,  9.10it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0845, train_loss_epoch=0.062]\n",
      "Epoch 44: 100%|██████████| 1436/1436 [02:37<00:00,  9.10it/s, loss=0.0612, v_num=0, train_loss_step=0.0437, val_loss=0.0841, train_loss_epoch=0.062]\n",
      "Epoch 45:  94%|█████████▍| 1350/1436 [02:29<00:09,  9.02it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 45:  94%|█████████▍| 1351/1436 [02:30<00:09,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  94%|█████████▍| 1352/1436 [02:30<00:09,  8.99it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  94%|█████████▍| 1353/1436 [02:30<00:09,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  94%|█████████▍| 1354/1436 [02:30<00:09,  8.99it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  94%|█████████▍| 1355/1436 [02:30<00:09,  8.99it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  94%|█████████▍| 1356/1436 [02:30<00:08,  8.99it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  94%|█████████▍| 1357/1436 [02:30<00:08,  8.99it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  95%|█████████▍| 1358/1436 [02:31<00:08,  8.99it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  95%|█████████▍| 1359/1436 [02:31<00:08,  8.99it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  95%|█████████▍| 1360/1436 [02:31<00:08,  8.99it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  95%|█████████▍| 1361/1436 [02:31<00:08,  8.99it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  95%|█████████▍| 1362/1436 [02:31<00:08,  8.99it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  95%|█████████▍| 1363/1436 [02:31<00:08,  8.99it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  95%|█████████▍| 1364/1436 [02:31<00:08,  8.99it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  95%|█████████▌| 1365/1436 [02:31<00:07,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  95%|█████████▌| 1366/1436 [02:31<00:07,  8.99it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  95%|█████████▌| 1367/1436 [02:32<00:07,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  95%|█████████▌| 1368/1436 [02:32<00:07,  8.99it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  95%|█████████▌| 1369/1436 [02:32<00:07,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  95%|█████████▌| 1370/1436 [02:32<00:07,  8.99it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  95%|█████████▌| 1371/1436 [02:32<00:07,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  96%|█████████▌| 1372/1436 [02:32<00:07,  8.99it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  96%|█████████▌| 1373/1436 [02:32<00:07,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  96%|█████████▌| 1374/1436 [02:32<00:06,  8.99it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  96%|█████████▌| 1375/1436 [02:33<00:06,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  96%|█████████▌| 1376/1436 [02:33<00:06,  8.99it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  96%|█████████▌| 1377/1436 [02:33<00:06,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  96%|█████████▌| 1378/1436 [02:33<00:06,  8.99it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  96%|█████████▌| 1379/1436 [02:33<00:06,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  96%|█████████▌| 1380/1436 [02:33<00:06,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  96%|█████████▌| 1381/1436 [02:33<00:06,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  96%|█████████▌| 1382/1436 [02:33<00:06,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  96%|█████████▋| 1383/1436 [02:34<00:05,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  96%|█████████▋| 1384/1436 [02:34<00:05,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  96%|█████████▋| 1385/1436 [02:34<00:05,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  97%|█████████▋| 1386/1436 [02:34<00:05,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  97%|█████████▋| 1387/1436 [02:34<00:05,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  97%|█████████▋| 1388/1436 [02:34<00:05,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  97%|█████████▋| 1389/1436 [02:34<00:05,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  97%|█████████▋| 1390/1436 [02:34<00:05,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  97%|█████████▋| 1391/1436 [02:34<00:05,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  97%|█████████▋| 1392/1436 [02:35<00:04,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  97%|█████████▋| 1393/1436 [02:35<00:04,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  97%|█████████▋| 1394/1436 [02:35<00:04,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  97%|█████████▋| 1395/1436 [02:35<00:04,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  97%|█████████▋| 1396/1436 [02:35<00:04,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  97%|█████████▋| 1397/1436 [02:35<00:04,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  97%|█████████▋| 1398/1436 [02:35<00:04,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  97%|█████████▋| 1399/1436 [02:35<00:04,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  97%|█████████▋| 1400/1436 [02:35<00:04,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  98%|█████████▊| 1401/1436 [02:36<00:03,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  98%|█████████▊| 1402/1436 [02:36<00:03,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  98%|█████████▊| 1403/1436 [02:36<00:03,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  98%|█████████▊| 1404/1436 [02:36<00:03,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  98%|█████████▊| 1405/1436 [02:36<00:03,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  98%|█████████▊| 1406/1436 [02:36<00:03,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  98%|█████████▊| 1407/1436 [02:36<00:03,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  98%|█████████▊| 1408/1436 [02:36<00:03,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  98%|█████████▊| 1409/1436 [02:36<00:03,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  98%|█████████▊| 1410/1436 [02:37<00:02,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  98%|█████████▊| 1411/1436 [02:37<00:02,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  98%|█████████▊| 1412/1436 [02:37<00:02,  8.97it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  98%|█████████▊| 1413/1436 [02:37<00:02,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  98%|█████████▊| 1414/1436 [02:37<00:02,  8.97it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  99%|█████████▊| 1415/1436 [02:37<00:02,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  99%|█████████▊| 1416/1436 [02:37<00:02,  8.97it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  99%|█████████▊| 1417/1436 [02:37<00:02,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  99%|█████████▊| 1418/1436 [02:38<00:02,  8.97it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  99%|█████████▉| 1419/1436 [02:38<00:01,  8.97it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  99%|█████████▉| 1420/1436 [02:38<00:01,  8.97it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  99%|█████████▉| 1421/1436 [02:38<00:01,  8.97it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  99%|█████████▉| 1422/1436 [02:38<00:01,  8.97it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  99%|█████████▉| 1423/1436 [02:38<00:01,  8.97it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  99%|█████████▉| 1424/1436 [02:38<00:01,  8.97it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  99%|█████████▉| 1425/1436 [02:38<00:01,  8.97it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  99%|█████████▉| 1426/1436 [02:39<00:01,  8.97it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  99%|█████████▉| 1427/1436 [02:39<00:01,  8.97it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45:  99%|█████████▉| 1428/1436 [02:39<00:00,  8.97it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45: 100%|█████████▉| 1429/1436 [02:39<00:00,  8.97it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45: 100%|█████████▉| 1430/1436 [02:39<00:00,  8.97it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45: 100%|█████████▉| 1431/1436 [02:39<00:00,  8.97it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45: 100%|█████████▉| 1432/1436 [02:39<00:00,  8.97it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45: 100%|█████████▉| 1433/1436 [02:39<00:00,  8.97it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45: 100%|█████████▉| 1434/1436 [02:39<00:00,  8.97it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45: 100%|█████████▉| 1435/1436 [02:39<00:00,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.0841, train_loss_epoch=0.0618]\n",
      "Epoch 45: 100%|██████████| 1436/1436 [02:39<00:00,  8.98it/s, loss=0.0586, v_num=0, train_loss_step=0.0714, val_loss=0.086, train_loss_epoch=0.0618] \n",
      "Epoch 46:  94%|█████████▍| 1350/1436 [02:30<00:09,  8.99it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 46:  94%|█████████▍| 1351/1436 [02:30<00:09,  8.95it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  94%|█████████▍| 1352/1436 [02:30<00:09,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  94%|█████████▍| 1353/1436 [02:31<00:09,  8.95it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  94%|█████████▍| 1354/1436 [02:31<00:09,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  94%|█████████▍| 1355/1436 [02:31<00:09,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  94%|█████████▍| 1356/1436 [02:31<00:08,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  94%|█████████▍| 1357/1436 [02:31<00:08,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  95%|█████████▍| 1358/1436 [02:31<00:08,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  95%|█████████▍| 1359/1436 [02:31<00:08,  8.95it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  95%|█████████▍| 1360/1436 [02:31<00:08,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  95%|█████████▍| 1361/1436 [02:31<00:08,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  95%|█████████▍| 1362/1436 [02:32<00:08,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  95%|█████████▍| 1363/1436 [02:32<00:08,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  95%|█████████▍| 1364/1436 [02:32<00:08,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  95%|█████████▌| 1365/1436 [02:32<00:07,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  95%|█████████▌| 1366/1436 [02:32<00:07,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  95%|█████████▌| 1367/1436 [02:32<00:07,  8.95it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  95%|█████████▌| 1368/1436 [02:32<00:07,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  95%|█████████▌| 1369/1436 [02:32<00:07,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  95%|█████████▌| 1370/1436 [02:32<00:07,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  95%|█████████▌| 1371/1436 [02:33<00:07,  8.95it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  96%|█████████▌| 1372/1436 [02:33<00:07,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  96%|█████████▌| 1373/1436 [02:33<00:07,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  96%|█████████▌| 1374/1436 [02:33<00:06,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  96%|█████████▌| 1375/1436 [02:33<00:06,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  96%|█████████▌| 1376/1436 [02:33<00:06,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  96%|█████████▌| 1377/1436 [02:33<00:06,  8.95it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  96%|█████████▌| 1378/1436 [02:33<00:06,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  96%|█████████▌| 1379/1436 [02:34<00:06,  8.95it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  96%|█████████▌| 1380/1436 [02:34<00:06,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  96%|█████████▌| 1381/1436 [02:34<00:06,  8.95it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  96%|█████████▌| 1382/1436 [02:34<00:06,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  96%|█████████▋| 1383/1436 [02:34<00:05,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  96%|█████████▋| 1384/1436 [02:34<00:05,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  96%|█████████▋| 1385/1436 [02:34<00:05,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  97%|█████████▋| 1386/1436 [02:34<00:05,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  97%|█████████▋| 1387/1436 [02:34<00:05,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  97%|█████████▋| 1388/1436 [02:34<00:05,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  97%|█████████▋| 1389/1436 [02:35<00:05,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  97%|█████████▋| 1390/1436 [02:35<00:05,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  97%|█████████▋| 1391/1436 [02:35<00:05,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  97%|█████████▋| 1392/1436 [02:35<00:04,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  97%|█████████▋| 1393/1436 [02:35<00:04,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  97%|█████████▋| 1394/1436 [02:35<00:04,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  97%|█████████▋| 1395/1436 [02:35<00:04,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  97%|█████████▋| 1396/1436 [02:35<00:04,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  97%|█████████▋| 1397/1436 [02:35<00:04,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  97%|█████████▋| 1398/1436 [02:36<00:04,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  97%|█████████▋| 1399/1436 [02:36<00:04,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  97%|█████████▋| 1400/1436 [02:36<00:04,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  98%|█████████▊| 1401/1436 [02:36<00:03,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  98%|█████████▊| 1402/1436 [02:36<00:03,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  98%|█████████▊| 1403/1436 [02:36<00:03,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  98%|█████████▊| 1404/1436 [02:36<00:03,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  98%|█████████▊| 1405/1436 [02:36<00:03,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  98%|█████████▊| 1406/1436 [02:36<00:03,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  98%|█████████▊| 1407/1436 [02:37<00:03,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  98%|█████████▊| 1408/1436 [02:37<00:03,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  98%|█████████▊| 1409/1436 [02:37<00:03,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  98%|█████████▊| 1410/1436 [02:37<00:02,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  98%|█████████▊| 1411/1436 [02:37<00:02,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  98%|█████████▊| 1412/1436 [02:37<00:02,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  98%|█████████▊| 1413/1436 [02:37<00:02,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  98%|█████████▊| 1414/1436 [02:37<00:02,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  99%|█████████▊| 1415/1436 [02:37<00:02,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  99%|█████████▊| 1416/1436 [02:37<00:02,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  99%|█████████▊| 1417/1436 [02:38<00:02,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  99%|█████████▊| 1418/1436 [02:38<00:02,  8.97it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  99%|█████████▉| 1419/1436 [02:38<00:01,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  99%|█████████▉| 1420/1436 [02:38<00:01,  8.97it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  99%|█████████▉| 1421/1436 [02:38<00:01,  8.96it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  99%|█████████▉| 1422/1436 [02:38<00:01,  8.97it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  99%|█████████▉| 1423/1436 [02:38<00:01,  8.97it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  99%|█████████▉| 1424/1436 [02:38<00:01,  8.97it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  99%|█████████▉| 1425/1436 [02:38<00:01,  8.97it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  99%|█████████▉| 1426/1436 [02:38<00:01,  8.97it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  99%|█████████▉| 1427/1436 [02:39<00:01,  8.97it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46:  99%|█████████▉| 1428/1436 [02:39<00:00,  8.97it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46: 100%|█████████▉| 1429/1436 [02:39<00:00,  8.97it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46: 100%|█████████▉| 1430/1436 [02:39<00:00,  8.97it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46: 100%|█████████▉| 1431/1436 [02:39<00:00,  8.97it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46: 100%|█████████▉| 1432/1436 [02:39<00:00,  8.97it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46: 100%|█████████▉| 1433/1436 [02:39<00:00,  8.97it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46: 100%|█████████▉| 1434/1436 [02:39<00:00,  8.98it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46: 100%|█████████▉| 1435/1436 [02:39<00:00,  8.98it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.086, train_loss_epoch=0.0618]\n",
      "Epoch 46: 100%|██████████| 1436/1436 [02:39<00:00,  8.99it/s, loss=0.0616, v_num=0, train_loss_step=0.0752, val_loss=0.0864, train_loss_epoch=0.0618]\n",
      "Epoch 47:  94%|█████████▍| 1350/1436 [02:29<00:09,  9.03it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 47:  94%|█████████▍| 1351/1436 [02:30<00:09,  9.01it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  94%|█████████▍| 1352/1436 [02:30<00:09,  9.01it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  94%|█████████▍| 1353/1436 [02:30<00:09,  9.01it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  94%|█████████▍| 1354/1436 [02:30<00:09,  9.01it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  94%|█████████▍| 1355/1436 [02:30<00:08,  9.01it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  94%|█████████▍| 1356/1436 [02:30<00:08,  9.01it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  94%|█████████▍| 1357/1436 [02:30<00:08,  9.01it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  95%|█████████▍| 1358/1436 [02:30<00:08,  9.01it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  95%|█████████▍| 1359/1436 [02:30<00:08,  9.01it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  95%|█████████▍| 1360/1436 [02:30<00:08,  9.02it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  95%|█████████▍| 1361/1436 [02:30<00:08,  9.01it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  95%|█████████▍| 1362/1436 [02:31<00:08,  9.02it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  95%|█████████▍| 1363/1436 [02:31<00:08,  9.02it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  95%|█████████▍| 1364/1436 [02:31<00:07,  9.02it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  95%|█████████▌| 1365/1436 [02:31<00:07,  9.02it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  95%|█████████▌| 1366/1436 [02:31<00:07,  9.02it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  95%|█████████▌| 1367/1436 [02:31<00:07,  9.02it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  95%|█████████▌| 1368/1436 [02:31<00:07,  9.02it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  95%|█████████▌| 1369/1436 [02:31<00:07,  9.02it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  95%|█████████▌| 1370/1436 [02:31<00:07,  9.02it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  95%|█████████▌| 1371/1436 [02:32<00:07,  9.02it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  96%|█████████▌| 1372/1436 [02:32<00:07,  9.02it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  96%|█████████▌| 1373/1436 [02:32<00:06,  9.02it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  96%|█████████▌| 1374/1436 [02:32<00:06,  9.02it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  96%|█████████▌| 1375/1436 [02:32<00:06,  9.02it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  96%|█████████▌| 1376/1436 [02:32<00:06,  9.02it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  96%|█████████▌| 1377/1436 [02:32<00:06,  9.02it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  96%|█████████▌| 1378/1436 [02:32<00:06,  9.03it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  96%|█████████▌| 1379/1436 [02:32<00:06,  9.02it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  96%|█████████▌| 1380/1436 [02:32<00:06,  9.03it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  96%|█████████▌| 1381/1436 [02:33<00:06,  9.02it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  96%|█████████▌| 1382/1436 [02:33<00:05,  9.03it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  96%|█████████▋| 1383/1436 [02:33<00:05,  9.02it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  96%|█████████▋| 1384/1436 [02:33<00:05,  9.03it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  96%|█████████▋| 1385/1436 [02:33<00:05,  9.03it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  97%|█████████▋| 1386/1436 [02:33<00:05,  9.03it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  97%|█████████▋| 1387/1436 [02:33<00:05,  9.03it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  97%|█████████▋| 1388/1436 [02:33<00:05,  9.03it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  97%|█████████▋| 1389/1436 [02:33<00:05,  9.03it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  97%|█████████▋| 1390/1436 [02:33<00:05,  9.03it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  97%|█████████▋| 1391/1436 [02:34<00:04,  9.03it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  97%|█████████▋| 1392/1436 [02:34<00:04,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  97%|█████████▋| 1393/1436 [02:34<00:04,  9.03it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  97%|█████████▋| 1394/1436 [02:34<00:04,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  97%|█████████▋| 1395/1436 [02:34<00:04,  9.03it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  97%|█████████▋| 1396/1436 [02:34<00:04,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  97%|█████████▋| 1397/1436 [02:34<00:04,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  97%|█████████▋| 1398/1436 [02:34<00:04,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  97%|█████████▋| 1399/1436 [02:34<00:04,  9.03it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  97%|█████████▋| 1400/1436 [02:34<00:03,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  98%|█████████▊| 1401/1436 [02:35<00:03,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  98%|█████████▊| 1402/1436 [02:35<00:03,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  98%|█████████▊| 1403/1436 [02:35<00:03,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  98%|█████████▊| 1404/1436 [02:35<00:03,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  98%|█████████▊| 1405/1436 [02:35<00:03,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  98%|█████████▊| 1406/1436 [02:35<00:03,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  98%|█████████▊| 1407/1436 [02:35<00:03,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  98%|█████████▊| 1408/1436 [02:35<00:03,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  98%|█████████▊| 1409/1436 [02:35<00:02,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  98%|█████████▊| 1410/1436 [02:35<00:02,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  98%|█████████▊| 1411/1436 [02:36<00:02,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  98%|█████████▊| 1412/1436 [02:36<00:02,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  98%|█████████▊| 1413/1436 [02:36<00:02,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  98%|█████████▊| 1414/1436 [02:36<00:02,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  99%|█████████▊| 1415/1436 [02:36<00:02,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  99%|█████████▊| 1416/1436 [02:36<00:02,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  99%|█████████▊| 1417/1436 [02:36<00:02,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  99%|█████████▊| 1418/1436 [02:36<00:01,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  99%|█████████▉| 1419/1436 [02:36<00:01,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  99%|█████████▉| 1420/1436 [02:36<00:01,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  99%|█████████▉| 1421/1436 [02:37<00:01,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  99%|█████████▉| 1422/1436 [02:37<00:01,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  99%|█████████▉| 1423/1436 [02:37<00:01,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  99%|█████████▉| 1424/1436 [02:37<00:01,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  99%|█████████▉| 1425/1436 [02:37<00:01,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  99%|█████████▉| 1426/1436 [02:37<00:01,  9.05it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  99%|█████████▉| 1427/1436 [02:37<00:00,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47:  99%|█████████▉| 1428/1436 [02:37<00:00,  9.05it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47: 100%|█████████▉| 1429/1436 [02:38<00:00,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47: 100%|█████████▉| 1430/1436 [02:38<00:00,  9.05it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47: 100%|█████████▉| 1431/1436 [02:38<00:00,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47: 100%|█████████▉| 1432/1436 [02:38<00:00,  9.05it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47: 100%|█████████▉| 1433/1436 [02:38<00:00,  9.04it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47: 100%|█████████▉| 1434/1436 [02:38<00:00,  9.05it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47: 100%|█████████▉| 1435/1436 [02:38<00:00,  9.05it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0864, train_loss_epoch=0.0615]\n",
      "Epoch 47: 100%|██████████| 1436/1436 [02:38<00:00,  9.06it/s, loss=0.0637, v_num=0, train_loss_step=0.0431, val_loss=0.0836, train_loss_epoch=0.0615]\n",
      "Epoch 48:  94%|█████████▍| 1350/1436 [02:32<00:09,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 48:  94%|█████████▍| 1351/1436 [02:32<00:09,  8.85it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  94%|█████████▍| 1352/1436 [02:32<00:09,  8.85it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  94%|█████████▍| 1353/1436 [02:32<00:09,  8.85it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  94%|█████████▍| 1354/1436 [02:32<00:09,  8.85it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  94%|█████████▍| 1355/1436 [02:33<00:09,  8.85it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  94%|█████████▍| 1356/1436 [02:33<00:09,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  94%|█████████▍| 1357/1436 [02:33<00:08,  8.85it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  95%|█████████▍| 1358/1436 [02:33<00:08,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  95%|█████████▍| 1359/1436 [02:33<00:08,  8.85it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  95%|█████████▍| 1360/1436 [02:33<00:08,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  95%|█████████▍| 1361/1436 [02:33<00:08,  8.85it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  95%|█████████▍| 1362/1436 [02:33<00:08,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  95%|█████████▍| 1363/1436 [02:33<00:08,  8.85it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  95%|█████████▍| 1364/1436 [02:33<00:08,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  95%|█████████▌| 1365/1436 [02:34<00:08,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  95%|█████████▌| 1366/1436 [02:34<00:07,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  95%|█████████▌| 1367/1436 [02:34<00:07,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  95%|█████████▌| 1368/1436 [02:34<00:07,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  95%|█████████▌| 1369/1436 [02:34<00:07,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  95%|█████████▌| 1370/1436 [02:34<00:07,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  95%|█████████▌| 1371/1436 [02:34<00:07,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  96%|█████████▌| 1372/1436 [02:34<00:07,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  96%|█████████▌| 1373/1436 [02:34<00:07,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  96%|█████████▌| 1374/1436 [02:34<00:06,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  96%|█████████▌| 1375/1436 [02:35<00:06,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  96%|█████████▌| 1376/1436 [02:35<00:06,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  96%|█████████▌| 1377/1436 [02:35<00:06,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  96%|█████████▌| 1378/1436 [02:35<00:06,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  96%|█████████▌| 1379/1436 [02:35<00:06,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  96%|█████████▌| 1380/1436 [02:35<00:06,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  96%|█████████▌| 1381/1436 [02:35<00:06,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  96%|█████████▌| 1382/1436 [02:35<00:06,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  96%|█████████▋| 1383/1436 [02:35<00:05,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  96%|█████████▋| 1384/1436 [02:36<00:05,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  96%|█████████▋| 1385/1436 [02:36<00:05,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  97%|█████████▋| 1386/1436 [02:36<00:05,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  97%|█████████▋| 1387/1436 [02:36<00:05,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  97%|█████████▋| 1388/1436 [02:36<00:05,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  97%|█████████▋| 1389/1436 [02:36<00:05,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  97%|█████████▋| 1390/1436 [02:36<00:05,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  97%|█████████▋| 1391/1436 [02:36<00:05,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  97%|█████████▋| 1392/1436 [02:36<00:04,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  97%|█████████▋| 1393/1436 [02:37<00:04,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  97%|█████████▋| 1394/1436 [02:37<00:04,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  97%|█████████▋| 1395/1436 [02:37<00:04,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  97%|█████████▋| 1396/1436 [02:37<00:04,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  97%|█████████▋| 1397/1436 [02:37<00:04,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  97%|█████████▋| 1398/1436 [02:37<00:04,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  97%|█████████▋| 1399/1436 [02:37<00:04,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  97%|█████████▋| 1400/1436 [02:37<00:04,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  98%|█████████▊| 1401/1436 [02:38<00:03,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  98%|█████████▊| 1402/1436 [02:38<00:03,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  98%|█████████▊| 1403/1436 [02:38<00:03,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  98%|█████████▊| 1404/1436 [02:38<00:03,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  98%|█████████▊| 1405/1436 [02:38<00:03,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  98%|█████████▊| 1406/1436 [02:38<00:03,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  98%|█████████▊| 1407/1436 [02:38<00:03,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  98%|█████████▊| 1408/1436 [02:38<00:03,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  98%|█████████▊| 1409/1436 [02:38<00:03,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  98%|█████████▊| 1410/1436 [02:39<00:02,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  98%|█████████▊| 1411/1436 [02:39<00:02,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  98%|█████████▊| 1412/1436 [02:39<00:02,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  98%|█████████▊| 1413/1436 [02:39<00:02,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  98%|█████████▊| 1414/1436 [02:39<00:02,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  99%|█████████▊| 1415/1436 [02:39<00:02,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  99%|█████████▊| 1416/1436 [02:39<00:02,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  99%|█████████▊| 1417/1436 [02:39<00:02,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  99%|█████████▊| 1418/1436 [02:39<00:02,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  99%|█████████▉| 1419/1436 [02:40<00:01,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  99%|█████████▉| 1420/1436 [02:40<00:01,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  99%|█████████▉| 1421/1436 [02:40<00:01,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  99%|█████████▉| 1422/1436 [02:40<00:01,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  99%|█████████▉| 1423/1436 [02:40<00:01,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  99%|█████████▉| 1424/1436 [02:40<00:01,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  99%|█████████▉| 1425/1436 [02:40<00:01,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  99%|█████████▉| 1426/1436 [02:40<00:01,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  99%|█████████▉| 1427/1436 [02:40<00:01,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48:  99%|█████████▉| 1428/1436 [02:41<00:00,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48: 100%|█████████▉| 1429/1436 [02:41<00:00,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48: 100%|█████████▉| 1430/1436 [02:41<00:00,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48: 100%|█████████▉| 1431/1436 [02:41<00:00,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48: 100%|█████████▉| 1432/1436 [02:41<00:00,  8.86it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48: 100%|█████████▉| 1433/1436 [02:41<00:00,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48: 100%|█████████▉| 1434/1436 [02:41<00:00,  8.87it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48: 100%|█████████▉| 1435/1436 [02:41<00:00,  8.88it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0836, train_loss_epoch=0.0614]\n",
      "Epoch 48: 100%|██████████| 1436/1436 [02:41<00:00,  8.88it/s, loss=0.0564, v_num=0, train_loss_step=0.0508, val_loss=0.0849, train_loss_epoch=0.0614]\n",
      "Epoch 49:  94%|█████████▍| 1350/1436 [02:27<00:09,  9.16it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 49:  94%|█████████▍| 1351/1436 [02:27<00:09,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  94%|█████████▍| 1352/1436 [02:27<00:09,  9.14it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  94%|█████████▍| 1353/1436 [02:28<00:09,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  94%|█████████▍| 1354/1436 [02:28<00:08,  9.14it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  94%|█████████▍| 1355/1436 [02:28<00:08,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  94%|█████████▍| 1356/1436 [02:28<00:08,  9.14it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  94%|█████████▍| 1357/1436 [02:28<00:08,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  95%|█████████▍| 1358/1436 [02:28<00:08,  9.14it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  95%|█████████▍| 1359/1436 [02:28<00:08,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  95%|█████████▍| 1360/1436 [02:28<00:08,  9.14it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  95%|█████████▍| 1361/1436 [02:29<00:08,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  95%|█████████▍| 1362/1436 [02:29<00:08,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  95%|█████████▍| 1363/1436 [02:29<00:07,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  95%|█████████▍| 1364/1436 [02:29<00:07,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  95%|█████████▌| 1365/1436 [02:29<00:07,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  95%|█████████▌| 1366/1436 [02:29<00:07,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  95%|█████████▌| 1367/1436 [02:29<00:07,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  95%|█████████▌| 1368/1436 [02:29<00:07,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  95%|█████████▌| 1369/1436 [02:29<00:07,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  95%|█████████▌| 1370/1436 [02:30<00:07,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  95%|█████████▌| 1371/1436 [02:30<00:07,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  96%|█████████▌| 1372/1436 [02:30<00:07,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  96%|█████████▌| 1373/1436 [02:30<00:06,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  96%|█████████▌| 1374/1436 [02:30<00:06,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  96%|█████████▌| 1375/1436 [02:30<00:06,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  96%|█████████▌| 1376/1436 [02:30<00:06,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  96%|█████████▌| 1377/1436 [02:30<00:06,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  96%|█████████▌| 1378/1436 [02:30<00:06,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  96%|█████████▌| 1379/1436 [02:31<00:06,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  96%|█████████▌| 1380/1436 [02:31<00:06,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  96%|█████████▌| 1381/1436 [02:31<00:06,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  96%|█████████▌| 1382/1436 [02:31<00:05,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  96%|█████████▋| 1383/1436 [02:31<00:05,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  96%|█████████▋| 1384/1436 [02:31<00:05,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  96%|█████████▋| 1385/1436 [02:31<00:05,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  97%|█████████▋| 1386/1436 [02:31<00:05,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  97%|█████████▋| 1387/1436 [02:31<00:05,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  97%|█████████▋| 1388/1436 [02:32<00:05,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  97%|█████████▋| 1389/1436 [02:32<00:05,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  97%|█████████▋| 1390/1436 [02:32<00:05,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  97%|█████████▋| 1391/1436 [02:32<00:04,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  97%|█████████▋| 1392/1436 [02:32<00:04,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  97%|█████████▋| 1393/1436 [02:32<00:04,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  97%|█████████▋| 1394/1436 [02:32<00:04,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  97%|█████████▋| 1395/1436 [02:32<00:04,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  97%|█████████▋| 1396/1436 [02:32<00:04,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  97%|█████████▋| 1397/1436 [02:32<00:04,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  97%|█████████▋| 1398/1436 [02:33<00:04,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  97%|█████████▋| 1399/1436 [02:33<00:04,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  97%|█████████▋| 1400/1436 [02:33<00:03,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  98%|█████████▊| 1401/1436 [02:33<00:03,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  98%|█████████▊| 1402/1436 [02:33<00:03,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  98%|█████████▊| 1403/1436 [02:33<00:03,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  98%|█████████▊| 1404/1436 [02:33<00:03,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  98%|█████████▊| 1405/1436 [02:33<00:03,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  98%|█████████▊| 1406/1436 [02:33<00:03,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  98%|█████████▊| 1407/1436 [02:34<00:03,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  98%|█████████▊| 1408/1436 [02:34<00:03,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  98%|█████████▊| 1409/1436 [02:34<00:02,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  98%|█████████▊| 1410/1436 [02:34<00:02,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  98%|█████████▊| 1411/1436 [02:34<00:02,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  98%|█████████▊| 1412/1436 [02:34<00:02,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  98%|█████████▊| 1413/1436 [02:34<00:02,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  98%|█████████▊| 1414/1436 [02:34<00:02,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  99%|█████████▊| 1415/1436 [02:34<00:02,  9.14it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  99%|█████████▊| 1416/1436 [02:35<00:02,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  99%|█████████▊| 1417/1436 [02:35<00:02,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  99%|█████████▊| 1418/1436 [02:35<00:01,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  99%|█████████▉| 1419/1436 [02:35<00:01,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  99%|█████████▉| 1420/1436 [02:35<00:01,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  99%|█████████▉| 1421/1436 [02:35<00:01,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  99%|█████████▉| 1422/1436 [02:35<00:01,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  99%|█████████▉| 1423/1436 [02:35<00:01,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  99%|█████████▉| 1424/1436 [02:35<00:01,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  99%|█████████▉| 1425/1436 [02:36<00:01,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  99%|█████████▉| 1426/1436 [02:36<00:01,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  99%|█████████▉| 1427/1436 [02:36<00:00,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49:  99%|█████████▉| 1428/1436 [02:36<00:00,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49: 100%|█████████▉| 1429/1436 [02:36<00:00,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49: 100%|█████████▉| 1430/1436 [02:36<00:00,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49: 100%|█████████▉| 1431/1436 [02:36<00:00,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49: 100%|█████████▉| 1432/1436 [02:36<00:00,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49: 100%|█████████▉| 1433/1436 [02:36<00:00,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49: 100%|█████████▉| 1434/1436 [02:36<00:00,  9.13it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49: 100%|█████████▉| 1435/1436 [02:36<00:00,  9.14it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0849, train_loss_epoch=0.0612]\n",
      "Epoch 49: 100%|██████████| 1436/1436 [02:37<00:00,  9.15it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0848, train_loss_epoch=0.0612]\n",
      "Epoch 49: 100%|██████████| 1436/1436 [02:37<00:00,  9.14it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0848, train_loss_epoch=0.0611]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 1436/1436 [02:37<00:00,  9.14it/s, loss=0.0626, v_num=0, train_loss_step=0.0659, val_loss=0.0848, train_loss_epoch=0.0611]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "\n",
    "logger = TensorBoardLogger('/tf_logs', name=f\"FCNN_3: batch_size={batch_size}, encoder_length={encoder_length}\")\n",
    "\n",
    "# trainer = Trainer(gpus=1, max_epochs=100, limit_train_batches=2606, logger=logger)\n",
    "trainer = Trainer(accelerator='gpu', devices=1, logger=logger, max_epochs=200)\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader)\n",
    "# trainer.validate(model=model, dataloaders=valid_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model according to the validation loss\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "# best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "# best_tft = FullyConnectedModelWithCovariates.load_from_checkpoint(best_model_path)\n",
    "trainer.save_checkpoint(\"fc_best_model3.ckpt\")\n",
    "best_tft = FullyConnectedModelWithCovariates.load_from_checkpoint(\"fc_best_model3.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0493)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calcualte mean absolute error on validation set\n",
    "actuals = torch.cat([y[0] for x, y in iter(valid_dataloader)])\n",
    "predictions = best_tft.predict(valid_dataloader)\n",
    "(actuals - predictions).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([2736, 1]) <class 'torch.Tensor'> torch.Size([2736, 1])\n",
      "tensor([[0.8284],\n",
      "        [0.8284],\n",
      "        [0.8284],\n",
      "        ...,\n",
      "        [0.5816],\n",
      "        [0.5816],\n",
      "        [0.5816]]) tensor([[0.6300],\n",
      "        [0.6261],\n",
      "        [0.6451],\n",
      "        ...,\n",
      "        [0.6804],\n",
      "        [0.6848],\n",
      "        [0.7123]])\n"
     ]
    }
   ],
   "source": [
    "print(type(actuals), actuals.shape, type(predictions), predictions.shape)\n",
    "print(actuals, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X = [X for X in range(0, actuals.shape[0])]\n",
    "\n",
    "fig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(20,5))\n",
    "\n",
    "ax1.plot(X, actuals, color='b', label=\"Actual\")\n",
    "ax1.plot(X, predictions, color='r', label=\"Predicted\")\n",
    "\n",
    "files = os.path.join(home_dir, f'FCNN_3.png')\n",
    "plt.savefig(files, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfn\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fn' is not defined"
     ]
    }
   ],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xm65lJFvnnLs",
    "outputId": "ffd88daa-bbd8-4b3a-c1f8-e693f3ffece0"
   },
   "outputs": [],
   "source": [
    "max_prediction_length = 3\n",
    "max_encoder_length = 8\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "# print(training_cutoff)\n",
    "# x = data[lambda x: x.time_idx <= training_cutoff]\n",
    "# print()\n",
    "\n",
    "bins_name = list([\"yield\"])\n",
    "for bin in range(0, 512):\n",
    "  bins_name.append(f'bin{bin}')\n",
    "\n",
    "print(bins_name)\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx = \"time_idx\",\n",
    "    target = \"yield\",\n",
    "    group_ids = [\"county\", \"bands\", \"time\"],\n",
    "    min_encoder_length = max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length = max_encoder_length,\n",
    "    min_prediction_length = 1,\n",
    "    max_prediction_length = max_prediction_length,\n",
    "    # static_categoricals = [\"county\"],\n",
    "    # static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],\n",
    "    # time_varying_known_categoricals=[\"special_days\", \"month\"],\n",
    "    # variable_groups={\"special_days\": special_days},  # group of categorical variables can be treated as one variable\n",
    "    # time_varying_known_reals=[\"time_idx\", \"price_regular\", \"discount_in_percent\"],\n",
    "    # time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals = bins_name,\n",
    "    allow_missing_timesteps = True,\n",
    "    # target_normalizer=GroupNormalizer(\n",
    "    #     groups=[\"agency\", \"sku\"], transformation=\"softplus\"\n",
    "    # ),  # use softplus and normalize by group\n",
    "    # add_relative_time_idx = True,\n",
    "    # add_target_scales = True,\n",
    "    # add_encoder_length = True,\n",
    ")\n",
    "\n",
    "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "# for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 1  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train =  True, batch_size = batch_size, num_workers = 0)\n",
    "val_dataloader = validation.to_dataloader(train = False, batch_size = batch_size, num_workers = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gThPXEaensCI",
    "outputId": "45c75a22-e2c8-4538-f11e-eaf1aea74050"
   },
   "outputs": [],
   "source": [
    "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "actuals = torch.cat([y for x, (y, weight) in iter(valid_dataloader)])\n",
    "baseline_predictions = Baseline().predict(valid_dataloader)\n",
    "(actuals - baseline_predictions).abs().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BR2PH1m8opGv"
   },
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "# pl.seed_everything(42)\n",
    "# trainer = pl.Trainer(\n",
    "#     gpus=0,\n",
    "#     # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "#     # of the gradient for recurrent neural networks\n",
    "#     gradient_clip_val=0.1,\n",
    "#     auto_lr_find = True,\n",
    "# )\n",
    "\n",
    "\n",
    "# tft = TemporalFusionTransformer.from_dataset(\n",
    "#     training,\n",
    "#     # not meaningful for finding the learning rate but otherwise very important\n",
    "#     learning_rate=0.03,\n",
    "#     hidden_size=16,  # most important hyperparameter apart from learning rate\n",
    "#     # number of attention heads. Set to up to 4 for large datasets\n",
    "#     attention_head_size=1,\n",
    "#     dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "#     hidden_continuous_size=8,  # set to <= hidden_size\n",
    "#     output_size=1,  # 7 quantiles by default\n",
    "#     loss=QuantileLoss(),\n",
    "#     # reduce learning rate if no improvement in validation loss after x epochs\n",
    "#     reduce_on_plateau_patience=4,\n",
    "# )\n",
    "# print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQexRoPJopqa"
   },
   "outputs": [],
   "source": [
    "# # find optimal learning rate\n",
    "# res = trainer.tuner.lr_find(\n",
    "#     tft,\n",
    "#     train_dataloaders=train_dataloader,\n",
    "#     val_dataloaders=val_dataloader,\n",
    "#     max_lr=10.0,\n",
    "#     min_lr=1e-6,\n",
    "# )\n",
    "\n",
    "# # res = trainer.tuner.lr_find(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader,)\n",
    "\n",
    "# print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "# fig = res.plot(show=True, suggest=True)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x8ohrZoqosSX",
    "outputId": "4f54da2d-924c-4403-93ee-6ce56f78cc4c"
   },
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=300,\n",
    "    gpus=0,\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=1,  # 7 quantiles by default\n",
    "    loss=MAPE(),  #QuantileLoss(), #MAPE(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zcFzxHVrVQ-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorboard as tb \n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 939,
     "referenced_widgets": [
      "14d07476dd484e5e8a7bca7bf59ceae5",
      "c56fd12d72db4e4caf6d5ef0ade8b9f8",
      "fa10f2402f224290815d92ee89213162",
      "daf65e7e84e34266929b680250bc7a46",
      "ba34c1e6d0cb4b03870cb4aef1b6c32c",
      "cdd9d4ddccdc4e06a76aafddce2701f0",
      "ae9c80f166694da09e56af90999a7c6b",
      "5eeae71bc8384c98921f6ead0351b047",
      "74f31ab6b038471ca73833d340ba5606",
      "88d278124f704f90a67d37da11072342",
      "325556a59ba44f10bea30d875b032bcd"
     ]
    },
    "id": "L3O1F_Joo0YG",
    "outputId": "a2851582-3695-4376-9460-bdf095cbcd12"
   },
   "outputs": [],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyhBzIoi5cqP"
   },
   "outputs": [],
   "source": [
    "# load the best model according to the validation loss\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtZA-Mom5gaW"
   },
   "outputs": [],
   "source": [
    "# calcualte mean absolute error on validation set\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "predictions = best_tft.predict(val_dataloader)\n",
    "(actuals - predictions).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXtVdTBs5jZP"
   },
   "outputs": [],
   "source": [
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoYevMoT5wGA"
   },
   "outputs": [],
   "source": [
    "for idx in range(10):  # plot 10 examples\n",
    "    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQT6Kxwl53gu"
   },
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cufHwzV8pLiR"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "# create study\n",
    "study = optimize_hyperparameters(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=200,\n",
    "    max_epochs=50,\n",
    "    gradient_clip_val_range=(0.01, 1.0),\n",
    "    hidden_size_range=(8, 128),\n",
    "    hidden_continuous_size_range=(8, 128),\n",
    "    attention_head_size_range=(1, 4),\n",
    "    learning_rate_range=(0.001, 0.1),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=30),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08YeXDvqo3ZE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "14d07476dd484e5e8a7bca7bf59ceae5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c56fd12d72db4e4caf6d5ef0ade8b9f8",
       "IPY_MODEL_fa10f2402f224290815d92ee89213162",
       "IPY_MODEL_daf65e7e84e34266929b680250bc7a46"
      ],
      "layout": "IPY_MODEL_ba34c1e6d0cb4b03870cb4aef1b6c32c"
     }
    },
    "325556a59ba44f10bea30d875b032bcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5eeae71bc8384c98921f6ead0351b047": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74f31ab6b038471ca73833d340ba5606": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "88d278124f704f90a67d37da11072342": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae9c80f166694da09e56af90999a7c6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba34c1e6d0cb4b03870cb4aef1b6c32c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "c56fd12d72db4e4caf6d5ef0ade8b9f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdd9d4ddccdc4e06a76aafddce2701f0",
      "placeholder": "​",
      "style": "IPY_MODEL_ae9c80f166694da09e56af90999a7c6b",
      "value": "Sanity Checking DataLoader 0:   0%"
     }
    },
    "cdd9d4ddccdc4e06a76aafddce2701f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "daf65e7e84e34266929b680250bc7a46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88d278124f704f90a67d37da11072342",
      "placeholder": "​",
      "style": "IPY_MODEL_325556a59ba44f10bea30d875b032bcd",
      "value": " 0/2 [00:00&lt;?, ?it/s]"
     }
    },
    "fa10f2402f224290815d92ee89213162": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5eeae71bc8384c98921f6ead0351b047",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_74f31ab6b038471ca73833d340ba5606",
      "value": 0
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
