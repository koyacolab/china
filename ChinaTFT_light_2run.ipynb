{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvpNHOs_nTQK",
    "outputId": "58e5e73c-d790-40a6-a1dc-1f6638a75f55"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive/')\n",
    "\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pDXvdYFuLYSI",
    "outputId": "db9c1dcc-d2d2-450c-bab9-29f775f571d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hy-tmp\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "home_dir = '/content/gdrive/My Drive/AChina' \n",
    "home_dir = '/hy-tmp'\n",
    "os.chdir(home_dir)\n",
    "!pwd\n",
    "\n",
    "!pip install tqdm \n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WPwRIbsMnaq8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "\n",
    "# os.chdir(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYbhRWaxneDl",
    "outputId": "c79dccdb-c661-4f1c-ca13-a14d7bd2c1c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (1.8.1)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scipy) (1.22.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.12.0+cu113)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.8/dist-packages (1.8.6)\n",
      "Requirement already satisfied: pytorch_forecasting in /usr/local/lib/python3.8/dist-packages (0.10.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (0.11.0)\n",
      "Requirement already satisfied: lightning-utilities!=0.4.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (0.4.2)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2022.11.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (21.3)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: tensorboardX>=2.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2.5.1)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.22.3)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.64.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (3.5.2)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (1.4.3)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (0.13.5)\n",
      "Requirement already satisfied: scikit-learn<1.2,>=0.24 in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (1.1.1)\n",
      "Requirement already satisfied: scipy<2.0,>=1.8 in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (1.8.1)\n",
      "Requirement already satisfied: optuna<3.0.0,>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (2.10.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.28.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.3)\n",
      "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch_forecasting) (0.9.0)\n",
      "Requirement already satisfied: cliff in /usr/local/lib/python3.8/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch_forecasting) (4.1.0)\n",
      "Requirement already satisfied: alembic in /usr/local/lib/python3.8/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch_forecasting) (1.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch_forecasting) (1.4.45)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.8/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch_forecasting) (6.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas<2.0.0,>=1.3.0->pytorch_forecasting) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas<2.0.0,>=1.3.0->pytorch_forecasting) (2.8.2)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<1.2,>=0.24->pytorch_forecasting) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<1.2,>=0.24->pytorch_forecasting) (3.1.0)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=2.2->pytorch-lightning) (3.19.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pytorch_forecasting) (1.4.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pytorch_forecasting) (9.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pytorch_forecasting) (4.34.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pytorch_forecasting) (0.11.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.8/dist-packages (from statsmodels->pytorch_forecasting) (0.5.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from patsy>=0.5.2->statsmodels->pytorch_forecasting) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy>=1.1.0->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (4.12.0)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.8/dist-packages (from alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (1.2.4)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (5.8.0)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (3.5.0)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (2.4.2)\n",
      "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (0.5.1)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (4.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.6.15)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (0.2.5)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (1.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (3.8.1)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from stevedore>=2.0.1->cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (5.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from Mako->alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (2.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install torch torchvision\n",
    "# !pip install pytorch_lightning\n",
    "# !pip install pytorch_forecasting\n",
    "\n",
    "# !pip install torch -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install pytorch-forecasting\n",
    "\n",
    "!pip install scipy\n",
    "!pip install torch pytorch-lightning pytorch_forecasting\n",
    "\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAPE, SMAPE, PoissonLoss, QuantileLoss\n",
    "# from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "65_fJ6MIbncZ",
    "outputId": "06e9c0ab-8d58-480b-86d1-4ca275159e4a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>years</th>\n",
       "      <th>yield</th>\n",
       "      <th>sownareas</th>\n",
       "      <th>yieldvals</th>\n",
       "      <th>county</th>\n",
       "      <th>MODIS</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>band_0_0</th>\n",
       "      <th>band_0_1</th>\n",
       "      <th>...</th>\n",
       "      <th>band_8_502</th>\n",
       "      <th>band_8_503</th>\n",
       "      <th>band_8_504</th>\n",
       "      <th>band_8_505</th>\n",
       "      <th>band_8_506</th>\n",
       "      <th>band_8_507</th>\n",
       "      <th>band_8_508</th>\n",
       "      <th>band_8_509</th>\n",
       "      <th>band_8_510</th>\n",
       "      <th>band_8_511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>MOD_0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>MOD_2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>MOD_3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>MOD_4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>MOD_5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>MOD_6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>MOD_7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>MOD_8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>MOD_9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>MOD_10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>MOD_11</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>MOD_12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>MOD_13</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>75.21</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0</td>\n",
       "      <td>MOD_14</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 4616 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  years    yield  sownareas  yieldvals  county   MODIS  \\\n",
       "0            0   2003  0.61295      75.21       46.1       0   MOD_0   \n",
       "1            1   2003  0.61295      75.21       46.1       0   MOD_1   \n",
       "2            2   2003  0.61295      75.21       46.1       0   MOD_2   \n",
       "3            3   2003  0.61295      75.21       46.1       0   MOD_3   \n",
       "4            4   2003  0.61295      75.21       46.1       0   MOD_4   \n",
       "5            5   2003  0.61295      75.21       46.1       0   MOD_5   \n",
       "6            6   2003  0.61295      75.21       46.1       0   MOD_6   \n",
       "7            7   2003  0.61295      75.21       46.1       0   MOD_7   \n",
       "8            8   2003  0.61295      75.21       46.1       0   MOD_8   \n",
       "9            9   2003  0.61295      75.21       46.1       0   MOD_9   \n",
       "10          10   2003  0.61295      75.21       46.1       0  MOD_10   \n",
       "11          11   2003  0.61295      75.21       46.1       0  MOD_11   \n",
       "12          12   2003  0.61295      75.21       46.1       0  MOD_12   \n",
       "13          13   2003  0.61295      75.21       46.1       0  MOD_13   \n",
       "14          14   2003  0.61295      75.21       46.1       0  MOD_14   \n",
       "\n",
       "    time_idx  band_0_0  band_0_1  ...  band_8_502  band_8_503  band_8_504  \\\n",
       "0          0       0.0       0.0  ...         0.0         0.0         0.0   \n",
       "1          1       0.0       0.0  ...         0.0         0.0         0.0   \n",
       "2          2       0.0       0.0  ...         0.0         0.0         0.0   \n",
       "3          3       0.0       0.0  ...         0.0         0.0         0.0   \n",
       "4          4       0.0       0.0  ...         0.0         0.0         0.0   \n",
       "5          5       0.0       0.0  ...         0.0         0.0         0.0   \n",
       "6          6       0.0       0.0  ...         0.0         0.0         0.0   \n",
       "7          7       0.0       0.0  ...         0.0         0.0         0.0   \n",
       "8          8       0.0       0.0  ...         0.0         0.0         0.0   \n",
       "9          9       0.0       0.0  ...         0.0         0.0         0.0   \n",
       "10        10       0.0       0.0  ...         0.0         0.0         0.0   \n",
       "11        11       0.0       0.0  ...         0.0         0.0         0.0   \n",
       "12        12       0.0       0.0  ...         0.0         0.0         0.0   \n",
       "13        13       0.0       0.0  ...         0.0         0.0         0.0   \n",
       "14        14       0.0       0.0  ...         0.0         0.0         0.0   \n",
       "\n",
       "    band_8_505  band_8_506  band_8_507  band_8_508  band_8_509  band_8_510  \\\n",
       "0          0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "1          0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2          0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "3          0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "4          0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "5          0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "6          0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "7          0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "8          0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "9          0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "10         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "11         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "12         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "13         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "14         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "    band_8_511  \n",
       "0          0.0  \n",
       "1          0.0  \n",
       "2          0.0  \n",
       "3          0.0  \n",
       "4          0.0  \n",
       "5          0.0  \n",
       "6          0.0  \n",
       "7          0.0  \n",
       "8          0.0  \n",
       "9          0.0  \n",
       "10         0.0  \n",
       "11         0.0  \n",
       "12         0.0  \n",
       "13         0.0  \n",
       "14         0.0  \n",
       "\n",
       "[15 rows x 4616 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('corn_china_pandas_onebands.csv')  # encoding= 'unicode_escape')\n",
    "\n",
    "data[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TOncsJYonfF_"
   },
   "outputs": [],
   "source": [
    "# years = [x for x in range(2003, 2019)]\n",
    "\n",
    "# data.rename(columns={'time_idx' : 'time'}, inplace=True)  \n",
    "\n",
    "# # data[5:15]  \n",
    "# data.insert(1, \"time_idx\", data['years'])  \n",
    "# # df = data.assign(time_dx = data.time * 10)\n",
    "\n",
    "# time_idx = 0\n",
    "# for year in years:\n",
    "#     data['time_idx'] = data['time_idx'].replace([year], time_idx)\n",
    "#     time_idx = time_idx + 1\n",
    "    \n",
    "# data['years'] = data['years'].astype(str)\n",
    "# data['county'] = data['county'].astype(str)\n",
    "# data['time'] = data['time'].astype(str)\n",
    "\n",
    "# dff = data[ data['years'] == '2018' ]\n",
    "# dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "F-uXD_srqwLW",
    "outputId": "8a8a0424-2e67-4dd2-be62-b6b0bf46bfc3"
   },
   "outputs": [],
   "source": [
    "data['years'] = data['years'].astype(str)\n",
    "data['county'] = data['county'].astype(str)\n",
    "data['time_idx'] = data['time_idx'].astype(np.int64)\n",
    "# data.head()\n",
    "# print(type(data['bin500'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "LL4gooLRnkdv",
    "outputId": "cb1d012b-c336-4683-d2d9-f454d61c1cb1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>yield</th>\n",
       "      <th>sownareas</th>\n",
       "      <th>yieldvals</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>band_0_0</th>\n",
       "      <th>band_0_1</th>\n",
       "      <th>band_0_2</th>\n",
       "      <th>band_0_3</th>\n",
       "      <th>band_0_4</th>\n",
       "      <th>...</th>\n",
       "      <th>band_8_502</th>\n",
       "      <th>band_8_503</th>\n",
       "      <th>band_8_504</th>\n",
       "      <th>band_8_505</th>\n",
       "      <th>band_8_506</th>\n",
       "      <th>band_8_507</th>\n",
       "      <th>band_8_508</th>\n",
       "      <th>band_8_509</th>\n",
       "      <th>band_8_510</th>\n",
       "      <th>band_8_511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10208.000000</td>\n",
       "      <td>10208.000000</td>\n",
       "      <td>10208.000000</td>\n",
       "      <td>10208.000000</td>\n",
       "      <td>10208.000000</td>\n",
       "      <td>10208.000000</td>\n",
       "      <td>10208.000000</td>\n",
       "      <td>10208.000000</td>\n",
       "      <td>10208.000000</td>\n",
       "      <td>10208.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.020800e+04</td>\n",
       "      <td>1.020800e+04</td>\n",
       "      <td>1.020800e+04</td>\n",
       "      <td>1.020800e+04</td>\n",
       "      <td>1.020800e+04</td>\n",
       "      <td>1.020800e+04</td>\n",
       "      <td>1.020800e+04</td>\n",
       "      <td>1.020800e+04</td>\n",
       "      <td>1.020800e+04</td>\n",
       "      <td>1.020800e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>463.500000</td>\n",
       "      <td>0.517953</td>\n",
       "      <td>1054.189060</td>\n",
       "      <td>564.198213</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>...</td>\n",
       "      <td>2.092016e-07</td>\n",
       "      <td>2.026622e-07</td>\n",
       "      <td>2.181514e-07</td>\n",
       "      <td>1.555445e-07</td>\n",
       "      <td>1.194250e-07</td>\n",
       "      <td>1.453146e-07</td>\n",
       "      <td>1.647957e-07</td>\n",
       "      <td>2.702309e-07</td>\n",
       "      <td>1.788296e-07</td>\n",
       "      <td>5.195724e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>267.903492</td>\n",
       "      <td>0.106656</td>\n",
       "      <td>1126.135219</td>\n",
       "      <td>659.836500</td>\n",
       "      <td>9.233545</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>...</td>\n",
       "      <td>8.246547e-06</td>\n",
       "      <td>6.370645e-06</td>\n",
       "      <td>9.220619e-06</td>\n",
       "      <td>5.313741e-06</td>\n",
       "      <td>3.498986e-06</td>\n",
       "      <td>5.891810e-06</td>\n",
       "      <td>5.659541e-06</td>\n",
       "      <td>1.296903e-05</td>\n",
       "      <td>1.033253e-05</td>\n",
       "      <td>1.870616e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276526</td>\n",
       "      <td>1.810000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>231.750000</td>\n",
       "      <td>0.438383</td>\n",
       "      <td>192.340000</td>\n",
       "      <td>116.940000</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>463.500000</td>\n",
       "      <td>0.518895</td>\n",
       "      <td>577.010000</td>\n",
       "      <td>263.520000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>695.250000</td>\n",
       "      <td>0.574336</td>\n",
       "      <td>1559.340000</td>\n",
       "      <td>736.100000</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>927.000000</td>\n",
       "      <td>0.855448</td>\n",
       "      <td>4210.460000</td>\n",
       "      <td>2662.150000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.034574</td>\n",
       "      <td>0.005399</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.004608</td>\n",
       "      <td>...</td>\n",
       "      <td>7.321716e-04</td>\n",
       "      <td>4.791045e-04</td>\n",
       "      <td>7.695267e-04</td>\n",
       "      <td>3.547672e-04</td>\n",
       "      <td>2.012576e-04</td>\n",
       "      <td>5.226595e-04</td>\n",
       "      <td>2.943380e-04</td>\n",
       "      <td>1.221001e-03</td>\n",
       "      <td>1.013905e-03</td>\n",
       "      <td>1.319261e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 4613 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0         yield     sownareas     yieldvals      time_idx  \\\n",
       "count  10208.000000  10208.000000  10208.000000  10208.000000  10208.000000   \n",
       "mean     463.500000      0.517953   1054.189060    564.198213     15.500000   \n",
       "std      267.903492      0.106656   1126.135219    659.836500      9.233545   \n",
       "min        0.000000      0.276526      1.810000      1.250000      0.000000   \n",
       "25%      231.750000      0.438383    192.340000    116.940000      7.750000   \n",
       "50%      463.500000      0.518895    577.010000    263.520000     15.500000   \n",
       "75%      695.250000      0.574336   1559.340000    736.100000     23.250000   \n",
       "max      927.000000      0.855448   4210.460000   2662.150000     31.000000   \n",
       "\n",
       "           band_0_0      band_0_1      band_0_2      band_0_3      band_0_4  \\\n",
       "count  10208.000000  10208.000000  10208.000000  10208.000000  10208.000000   \n",
       "mean       0.000159      0.000013      0.000014      0.000015      0.000017   \n",
       "std        0.000836      0.000086      0.000073      0.000077      0.000096   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000041      0.000000      0.000000      0.000000      0.000000   \n",
       "max        0.034574      0.005399      0.002629      0.002457      0.004608   \n",
       "\n",
       "       ...    band_8_502    band_8_503    band_8_504    band_8_505  \\\n",
       "count  ...  1.020800e+04  1.020800e+04  1.020800e+04  1.020800e+04   \n",
       "mean   ...  2.092016e-07  2.026622e-07  2.181514e-07  1.555445e-07   \n",
       "std    ...  8.246547e-06  6.370645e-06  9.220619e-06  5.313741e-06   \n",
       "min    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max    ...  7.321716e-04  4.791045e-04  7.695267e-04  3.547672e-04   \n",
       "\n",
       "         band_8_506    band_8_507    band_8_508    band_8_509    band_8_510  \\\n",
       "count  1.020800e+04  1.020800e+04  1.020800e+04  1.020800e+04  1.020800e+04   \n",
       "mean   1.194250e-07  1.453146e-07  1.647957e-07  2.702309e-07  1.788296e-07   \n",
       "std    3.498986e-06  5.891810e-06  5.659541e-06  1.296903e-05  1.033253e-05   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max    2.012576e-04  5.226595e-04  2.943380e-04  1.221001e-03  1.013905e-03   \n",
       "\n",
       "         band_8_511  \n",
       "count  1.020800e+04  \n",
       "mean   5.195724e-08  \n",
       "std    1.870616e-06  \n",
       "min    0.000000e+00  \n",
       "25%    0.000000e+00  \n",
       "50%    0.000000e+00  \n",
       "75%    0.000000e+00  \n",
       "max    1.319261e-04  \n",
       "\n",
       "[8 rows x 4613 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 5767.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.utilities.model_summary import summarize\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "\n",
    "# create the dataset from the pandas dataframe\n",
    "train_data = data[ data[\"years\"] != \"2018\" ]\n",
    "valid_data = data[ data[\"years\"] == \"2018\" ]\n",
    "\n",
    "bins_name = list()   #list([\"yield\"])\n",
    "for bin in range(0, 512):\n",
    "    bins_name.append(f'bin{bin}')\n",
    "\n",
    "# print(bins_name)\n",
    "\n",
    "bins_name = list()\n",
    "for band in tqdm(range(0, 9)):\n",
    "    for bins in range(0, 512):\n",
    "        bins_name.append( f'band_{band}_{bins}' )\n",
    "\n",
    "encoder_length = 16\n",
    "group=[\"years\", \"county\"]\n",
    "unknown_categoricals=[\"years\", \"county\"]\n",
    "static_categoricals=[\"years\", \"county\"]\n",
    "known_reals=[\"sownareas\"]\n",
    "\n",
    "train_dataset_with_covariates = TimeSeriesDataSet(\n",
    "    train_data,\n",
    "    group_ids=group,\n",
    "    target=\"yield\",\n",
    "    time_idx=\"time_idx\",\n",
    "    min_encoder_length=encoder_length,\n",
    "    max_encoder_length=encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=1,\n",
    "    time_varying_unknown_reals=bins_name,  #[\"yield\"],\n",
    "    # time_varying_unknown_categoricals=unknown_categoricals,\n",
    "    # time_varying_known_reals=[\"sownareas\"],\n",
    "    time_varying_known_categoricals=[\"years\"],\n",
    "    # static_categoricals=static_categoricals,\n",
    ")\n",
    "\n",
    "valid_dataset_with_covariates = TimeSeriesDataSet(\n",
    "    valid_data,\n",
    "    group_ids=group,\n",
    "    target=\"yield\",\n",
    "    time_idx=\"time_idx\",\n",
    "    min_encoder_length=encoder_length,\n",
    "    max_encoder_length=encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=1,\n",
    "    time_varying_unknown_reals=bins_name,   #[\"yield\"],\n",
    "    # time_varying_unknown_categoricals=unknown_categoricals,\n",
    "    # time_varying_known_reals=[\"sownareas\"],\n",
    "    time_varying_known_categoricals=[\"years\"],\n",
    "    # static_categoricals=static_categoricals,\n",
    ")\n",
    "\n",
    "model = TemporalFusionTransformer.from_dataset(\n",
    "    train_dataset_with_covariates,\n",
    "    # learning_rate=0.03,\n",
    "    # hidden_size=16,\n",
    "    # attention_head_size=4,\n",
    "    # dropout=0.1,\n",
    "    # hidden_continuous_size=8,\n",
    "    # output_size=1,  # 7 quantiles by default\n",
    "    loss = MAE(),\n",
    "    # log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    # reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "# summarize(model,max_depth=-1)  # print model summary\n",
    "# model.hparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.metrics import MAE\n",
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "    \n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 48\n",
    "train_dataloader = train_dataset_with_covariates.to_dataloader(train=True,  batch_size=batch_size, num_workers=2)\n",
    "valid_dataloader = valid_dataset_with_covariates.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tencoder_cat = torch.Size([48, 16, 1])\n",
      "\tencoder_cont = torch.Size([48, 16, 4608])\n",
      "\tencoder_target = torch.Size([48, 16])\n",
      "\tencoder_lengths = torch.Size([48])\n",
      "\tdecoder_cat = torch.Size([48, 1, 1])\n",
      "\tdecoder_cont = torch.Size([48, 1, 4608])\n",
      "\tdecoder_target = torch.Size([48, 1])\n",
      "\tdecoder_lengths = torch.Size([48])\n",
      "\tdecoder_time_idx = torch.Size([48, 1])\n",
      "\tgroups = torch.Size([48, 2])\n",
      "\ttarget_scale = torch.Size([48, 2])\n"
     ]
    }
   ],
   "source": [
    "# convert the dataset to a dataloader\n",
    "# dataloader = dataset.to_dataloader(batch_size=4)\n",
    "\n",
    "# and load the first batch\n",
    "x, y = next(iter(valid_dataloader))\n",
    "# print(\"x =\", x)\n",
    "# print(\"\\ny =\", y)\n",
    "# print(\"\\nsizes of x =\")\n",
    "for key, value in x.items():\n",
    "    print(f\"\\t{key} = {value.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /tf_logs/TFT_2: batch_size=48, encoder_length=16, group=['years', 'county'], known_reals=['sownareas'], loss=MAE\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | MAE                             | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 105   \n",
      "3  | prescalers                         | ModuleDict                      | 73.7 K\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 0     \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 3.2 M \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 48    \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 676   \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "20 | output_layer                       | Linear                          | 17    \n",
      "----------------------------------------------------------------------------------------\n",
      "3.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 M     Total params\n",
      "12.869    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  93%|█████████▎| 100/107 [24:57<01:44, 14.97s/it, loss=0.089, v_num=0, train_loss_step=0.0929] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 101/107 [25:20<01:30, 15.06s/it, loss=0.089, v_num=0, train_loss_step=0.0929]\n",
      "Epoch 0:  95%|█████████▌| 102/107 [25:25<01:14, 14.96s/it, loss=0.089, v_num=0, train_loss_step=0.0929]\n",
      "Epoch 0:  96%|█████████▋| 103/107 [25:35<00:59, 14.91s/it, loss=0.089, v_num=0, train_loss_step=0.0929]\n",
      "Epoch 0:  97%|█████████▋| 104/107 [25:40<00:44, 14.81s/it, loss=0.089, v_num=0, train_loss_step=0.0929]\n",
      "Epoch 0:  98%|█████████▊| 105/107 [25:49<00:29, 14.76s/it, loss=0.089, v_num=0, train_loss_step=0.0929]\n",
      "Epoch 0:  99%|█████████▉| 106/107 [25:54<00:14, 14.66s/it, loss=0.089, v_num=0, train_loss_step=0.0929]\n",
      "Epoch 0: 100%|██████████| 107/107 [25:58<00:00, 14.56s/it, loss=0.089, v_num=0, train_loss_step=0.0929, val_loss=0.0828]\n",
      "Epoch 1:  93%|█████████▎| 100/107 [24:34<01:43, 14.75s/it, loss=0.0811, v_num=0, train_loss_step=0.0805, val_loss=0.0828, train_loss_epoch=0.0897]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  94%|█████████▍| 101/107 [24:58<01:28, 14.83s/it, loss=0.0811, v_num=0, train_loss_step=0.0805, val_loss=0.0828, train_loss_epoch=0.0897]\n",
      "Epoch 1:  95%|█████████▌| 102/107 [25:03<01:13, 14.74s/it, loss=0.0811, v_num=0, train_loss_step=0.0805, val_loss=0.0828, train_loss_epoch=0.0897]\n",
      "Epoch 1:  96%|█████████▋| 103/107 [25:13<00:58, 14.69s/it, loss=0.0811, v_num=0, train_loss_step=0.0805, val_loss=0.0828, train_loss_epoch=0.0897]\n",
      "Epoch 1:  97%|█████████▋| 104/107 [25:17<00:43, 14.59s/it, loss=0.0811, v_num=0, train_loss_step=0.0805, val_loss=0.0828, train_loss_epoch=0.0897]\n",
      "Epoch 1:  98%|█████████▊| 105/107 [25:27<00:29, 14.54s/it, loss=0.0811, v_num=0, train_loss_step=0.0805, val_loss=0.0828, train_loss_epoch=0.0897]\n",
      "Epoch 1:  99%|█████████▉| 106/107 [25:31<00:14, 14.45s/it, loss=0.0811, v_num=0, train_loss_step=0.0805, val_loss=0.0828, train_loss_epoch=0.0897]\n",
      "Epoch 1: 100%|██████████| 107/107 [25:35<00:00, 14.35s/it, loss=0.0811, v_num=0, train_loss_step=0.0805, val_loss=0.0791, train_loss_epoch=0.0897]\n",
      "Epoch 2:  93%|█████████▎| 100/107 [27:28<01:55, 16.48s/it, loss=0.0821, v_num=0, train_loss_step=0.0812, val_loss=0.0791, train_loss_epoch=0.0856]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  94%|█████████▍| 101/107 [27:51<01:39, 16.55s/it, loss=0.0821, v_num=0, train_loss_step=0.0812, val_loss=0.0791, train_loss_epoch=0.0856]\n",
      "Epoch 2:  95%|█████████▌| 102/107 [27:56<01:22, 16.43s/it, loss=0.0821, v_num=0, train_loss_step=0.0812, val_loss=0.0791, train_loss_epoch=0.0856]\n",
      "Epoch 2:  96%|█████████▋| 103/107 [28:06<01:05, 16.37s/it, loss=0.0821, v_num=0, train_loss_step=0.0812, val_loss=0.0791, train_loss_epoch=0.0856]\n",
      "Epoch 2:  97%|█████████▋| 104/107 [28:10<00:48, 16.26s/it, loss=0.0821, v_num=0, train_loss_step=0.0812, val_loss=0.0791, train_loss_epoch=0.0856]\n",
      "Epoch 2:  98%|█████████▊| 105/107 [28:20<00:32, 16.20s/it, loss=0.0821, v_num=0, train_loss_step=0.0812, val_loss=0.0791, train_loss_epoch=0.0856]\n",
      "Epoch 2:  99%|█████████▉| 106/107 [28:24<00:16, 16.08s/it, loss=0.0821, v_num=0, train_loss_step=0.0812, val_loss=0.0791, train_loss_epoch=0.0856]\n",
      "Epoch 2: 100%|██████████| 107/107 [28:29<00:00, 15.97s/it, loss=0.0821, v_num=0, train_loss_step=0.0812, val_loss=0.0722, train_loss_epoch=0.0856]\n",
      "Epoch 3:  93%|█████████▎| 100/107 [27:22<01:54, 16.42s/it, loss=0.0671, v_num=0, train_loss_step=0.0569, val_loss=0.0722, train_loss_epoch=0.0821]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  94%|█████████▍| 101/107 [27:46<01:38, 16.50s/it, loss=0.0671, v_num=0, train_loss_step=0.0569, val_loss=0.0722, train_loss_epoch=0.0821]\n",
      "Epoch 3:  95%|█████████▌| 102/107 [27:50<01:21, 16.38s/it, loss=0.0671, v_num=0, train_loss_step=0.0569, val_loss=0.0722, train_loss_epoch=0.0821]\n",
      "Epoch 3:  96%|█████████▋| 103/107 [28:00<01:05, 16.31s/it, loss=0.0671, v_num=0, train_loss_step=0.0569, val_loss=0.0722, train_loss_epoch=0.0821]\n",
      "Epoch 3:  97%|█████████▋| 104/107 [28:06<00:48, 16.21s/it, loss=0.0671, v_num=0, train_loss_step=0.0569, val_loss=0.0722, train_loss_epoch=0.0821]\n",
      "Epoch 3:  98%|█████████▊| 105/107 [28:16<00:32, 16.16s/it, loss=0.0671, v_num=0, train_loss_step=0.0569, val_loss=0.0722, train_loss_epoch=0.0821]\n",
      "Epoch 3:  99%|█████████▉| 106/107 [28:20<00:16, 16.05s/it, loss=0.0671, v_num=0, train_loss_step=0.0569, val_loss=0.0722, train_loss_epoch=0.0821]\n",
      "Epoch 3: 100%|██████████| 107/107 [28:25<00:00, 15.93s/it, loss=0.0671, v_num=0, train_loss_step=0.0569, val_loss=0.0594, train_loss_epoch=0.0821]\n",
      "Epoch 4:  93%|█████████▎| 100/107 [26:56<01:53, 16.17s/it, loss=0.057, v_num=0, train_loss_step=0.0715, val_loss=0.0594, train_loss_epoch=0.0718] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  94%|█████████▍| 101/107 [27:19<01:37, 16.24s/it, loss=0.057, v_num=0, train_loss_step=0.0715, val_loss=0.0594, train_loss_epoch=0.0718]\n",
      "Epoch 4:  95%|█████████▌| 102/107 [27:25<01:20, 16.13s/it, loss=0.057, v_num=0, train_loss_step=0.0715, val_loss=0.0594, train_loss_epoch=0.0718]\n",
      "Epoch 4:  96%|█████████▋| 103/107 [27:35<01:04, 16.07s/it, loss=0.057, v_num=0, train_loss_step=0.0715, val_loss=0.0594, train_loss_epoch=0.0718]\n",
      "Epoch 4:  97%|█████████▋| 104/107 [27:40<00:47, 15.96s/it, loss=0.057, v_num=0, train_loss_step=0.0715, val_loss=0.0594, train_loss_epoch=0.0718]\n",
      "Epoch 4:  98%|█████████▊| 105/107 [27:49<00:31, 15.90s/it, loss=0.057, v_num=0, train_loss_step=0.0715, val_loss=0.0594, train_loss_epoch=0.0718]\n",
      "Epoch 4:  99%|█████████▉| 106/107 [27:54<00:15, 15.79s/it, loss=0.057, v_num=0, train_loss_step=0.0715, val_loss=0.0594, train_loss_epoch=0.0718]\n",
      "Epoch 4: 100%|██████████| 107/107 [27:58<00:00, 15.69s/it, loss=0.057, v_num=0, train_loss_step=0.0715, val_loss=0.0522, train_loss_epoch=0.0718]\n",
      "Epoch 5:  93%|█████████▎| 100/107 [27:02<01:53, 16.22s/it, loss=0.0519, v_num=0, train_loss_step=0.0607, val_loss=0.0522, train_loss_epoch=0.0603]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  94%|█████████▍| 101/107 [27:25<01:37, 16.29s/it, loss=0.0519, v_num=0, train_loss_step=0.0607, val_loss=0.0522, train_loss_epoch=0.0603]\n",
      "Epoch 5:  95%|█████████▌| 102/107 [27:31<01:20, 16.19s/it, loss=0.0519, v_num=0, train_loss_step=0.0607, val_loss=0.0522, train_loss_epoch=0.0603]\n",
      "Epoch 5:  96%|█████████▋| 103/107 [27:40<01:04, 16.12s/it, loss=0.0519, v_num=0, train_loss_step=0.0607, val_loss=0.0522, train_loss_epoch=0.0603]\n",
      "Epoch 5:  97%|█████████▋| 104/107 [27:45<00:48, 16.01s/it, loss=0.0519, v_num=0, train_loss_step=0.0607, val_loss=0.0522, train_loss_epoch=0.0603]\n",
      "Epoch 5:  98%|█████████▊| 105/107 [27:55<00:31, 15.96s/it, loss=0.0519, v_num=0, train_loss_step=0.0607, val_loss=0.0522, train_loss_epoch=0.0603]\n",
      "Epoch 5:  99%|█████████▉| 106/107 [27:59<00:15, 15.85s/it, loss=0.0519, v_num=0, train_loss_step=0.0607, val_loss=0.0522, train_loss_epoch=0.0603]\n",
      "Epoch 5: 100%|██████████| 107/107 [28:04<00:00, 15.74s/it, loss=0.0519, v_num=0, train_loss_step=0.0607, val_loss=0.0519, train_loss_epoch=0.0603]\n",
      "Epoch 6:  93%|█████████▎| 100/107 [26:33<01:51, 15.93s/it, loss=0.0485, v_num=0, train_loss_step=0.0426, val_loss=0.0519, train_loss_epoch=0.0535]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  94%|█████████▍| 101/107 [26:57<01:36, 16.01s/it, loss=0.0485, v_num=0, train_loss_step=0.0426, val_loss=0.0519, train_loss_epoch=0.0535]\n",
      "Epoch 6:  95%|█████████▌| 102/107 [27:02<01:19, 15.91s/it, loss=0.0485, v_num=0, train_loss_step=0.0426, val_loss=0.0519, train_loss_epoch=0.0535]\n",
      "Epoch 6:  96%|█████████▋| 103/107 [27:12<01:03, 15.85s/it, loss=0.0485, v_num=0, train_loss_step=0.0426, val_loss=0.0519, train_loss_epoch=0.0535]\n",
      "Epoch 6:  97%|█████████▋| 104/107 [27:17<00:47, 15.75s/it, loss=0.0485, v_num=0, train_loss_step=0.0426, val_loss=0.0519, train_loss_epoch=0.0535]\n",
      "Epoch 6:  98%|█████████▊| 105/107 [27:27<00:31, 15.69s/it, loss=0.0485, v_num=0, train_loss_step=0.0426, val_loss=0.0519, train_loss_epoch=0.0535]\n",
      "Epoch 6:  99%|█████████▉| 106/107 [27:31<00:15, 15.58s/it, loss=0.0485, v_num=0, train_loss_step=0.0426, val_loss=0.0519, train_loss_epoch=0.0535]\n",
      "Epoch 6: 100%|██████████| 107/107 [27:35<00:00, 15.47s/it, loss=0.0485, v_num=0, train_loss_step=0.0426, val_loss=0.048, train_loss_epoch=0.0535] \n",
      "Epoch 7:  93%|█████████▎| 100/107 [27:24<01:55, 16.44s/it, loss=0.0401, v_num=0, train_loss_step=0.0387, val_loss=0.048, train_loss_epoch=0.0478]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  94%|█████████▍| 101/107 [27:46<01:38, 16.50s/it, loss=0.0401, v_num=0, train_loss_step=0.0387, val_loss=0.048, train_loss_epoch=0.0478]\n",
      "Epoch 7:  95%|█████████▌| 102/107 [27:51<01:21, 16.39s/it, loss=0.0401, v_num=0, train_loss_step=0.0387, val_loss=0.048, train_loss_epoch=0.0478]\n",
      "Epoch 7:  96%|█████████▋| 103/107 [28:02<01:05, 16.33s/it, loss=0.0401, v_num=0, train_loss_step=0.0387, val_loss=0.048, train_loss_epoch=0.0478]\n",
      "Epoch 7:  97%|█████████▋| 104/107 [28:07<00:48, 16.22s/it, loss=0.0401, v_num=0, train_loss_step=0.0387, val_loss=0.048, train_loss_epoch=0.0478]\n",
      "Epoch 7:  98%|█████████▊| 105/107 [28:16<00:32, 16.16s/it, loss=0.0401, v_num=0, train_loss_step=0.0387, val_loss=0.048, train_loss_epoch=0.0478]\n",
      "Epoch 7:  99%|█████████▉| 106/107 [28:20<00:16, 16.05s/it, loss=0.0401, v_num=0, train_loss_step=0.0387, val_loss=0.048, train_loss_epoch=0.0478]\n",
      "Epoch 7: 100%|██████████| 107/107 [28:25<00:00, 15.94s/it, loss=0.0401, v_num=0, train_loss_step=0.0387, val_loss=0.0459, train_loss_epoch=0.0478]\n",
      "Epoch 8:  93%|█████████▎| 100/107 [26:53<01:52, 16.14s/it, loss=0.0383, v_num=0, train_loss_step=0.0276, val_loss=0.0459, train_loss_epoch=0.0444]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  94%|█████████▍| 101/107 [27:15<01:37, 16.19s/it, loss=0.0383, v_num=0, train_loss_step=0.0276, val_loss=0.0459, train_loss_epoch=0.0444]\n",
      "Epoch 8:  95%|█████████▌| 102/107 [27:21<01:20, 16.09s/it, loss=0.0383, v_num=0, train_loss_step=0.0276, val_loss=0.0459, train_loss_epoch=0.0444]\n",
      "Epoch 8:  96%|█████████▋| 103/107 [27:31<01:04, 16.03s/it, loss=0.0383, v_num=0, train_loss_step=0.0276, val_loss=0.0459, train_loss_epoch=0.0444]\n",
      "Epoch 8:  97%|█████████▋| 104/107 [27:36<00:47, 15.93s/it, loss=0.0383, v_num=0, train_loss_step=0.0276, val_loss=0.0459, train_loss_epoch=0.0444]\n",
      "Epoch 8:  98%|█████████▊| 105/107 [27:47<00:31, 15.89s/it, loss=0.0383, v_num=0, train_loss_step=0.0276, val_loss=0.0459, train_loss_epoch=0.0444]\n",
      "Epoch 8:  99%|█████████▉| 106/107 [27:51<00:15, 15.77s/it, loss=0.0383, v_num=0, train_loss_step=0.0276, val_loss=0.0459, train_loss_epoch=0.0444]\n",
      "Epoch 8: 100%|██████████| 107/107 [27:56<00:00, 15.66s/it, loss=0.0383, v_num=0, train_loss_step=0.0276, val_loss=0.0437, train_loss_epoch=0.0444]\n",
      "Epoch 9:  93%|█████████▎| 100/107 [26:38<01:51, 15.98s/it, loss=0.0368, v_num=0, train_loss_step=0.0418, val_loss=0.0437, train_loss_epoch=0.0402]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  94%|█████████▍| 101/107 [27:01<01:36, 16.05s/it, loss=0.0368, v_num=0, train_loss_step=0.0418, val_loss=0.0437, train_loss_epoch=0.0402]\n",
      "Epoch 9:  95%|█████████▌| 102/107 [27:06<01:19, 15.94s/it, loss=0.0368, v_num=0, train_loss_step=0.0418, val_loss=0.0437, train_loss_epoch=0.0402]\n",
      "Epoch 9:  96%|█████████▋| 103/107 [27:16<01:03, 15.89s/it, loss=0.0368, v_num=0, train_loss_step=0.0418, val_loss=0.0437, train_loss_epoch=0.0402]\n",
      "Epoch 9:  97%|█████████▋| 104/107 [27:22<00:47, 15.79s/it, loss=0.0368, v_num=0, train_loss_step=0.0418, val_loss=0.0437, train_loss_epoch=0.0402]\n",
      "Epoch 9:  98%|█████████▊| 105/107 [27:33<00:31, 15.75s/it, loss=0.0368, v_num=0, train_loss_step=0.0418, val_loss=0.0437, train_loss_epoch=0.0402]\n",
      "Epoch 9:  99%|█████████▉| 106/107 [27:37<00:15, 15.64s/it, loss=0.0368, v_num=0, train_loss_step=0.0418, val_loss=0.0437, train_loss_epoch=0.0402]\n",
      "Epoch 9: 100%|██████████| 107/107 [27:41<00:00, 15.53s/it, loss=0.0368, v_num=0, train_loss_step=0.0418, val_loss=0.0471, train_loss_epoch=0.0402]\n",
      "Epoch 10:  93%|█████████▎| 100/107 [26:13<01:50, 15.73s/it, loss=0.033, v_num=0, train_loss_step=0.0214, val_loss=0.0471, train_loss_epoch=0.0374] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  94%|█████████▍| 101/107 [26:35<01:34, 15.80s/it, loss=0.033, v_num=0, train_loss_step=0.0214, val_loss=0.0471, train_loss_epoch=0.0374]\n",
      "Epoch 10:  95%|█████████▌| 102/107 [26:40<01:18, 15.69s/it, loss=0.033, v_num=0, train_loss_step=0.0214, val_loss=0.0471, train_loss_epoch=0.0374]\n",
      "Epoch 10:  96%|█████████▋| 103/107 [26:49<01:02, 15.63s/it, loss=0.033, v_num=0, train_loss_step=0.0214, val_loss=0.0471, train_loss_epoch=0.0374]\n",
      "Epoch 10:  97%|█████████▋| 104/107 [26:54<00:46, 15.53s/it, loss=0.033, v_num=0, train_loss_step=0.0214, val_loss=0.0471, train_loss_epoch=0.0374]\n",
      "Epoch 10:  98%|█████████▊| 105/107 [27:04<00:30, 15.47s/it, loss=0.033, v_num=0, train_loss_step=0.0214, val_loss=0.0471, train_loss_epoch=0.0374]\n",
      "Epoch 10:  99%|█████████▉| 106/107 [27:08<00:15, 15.37s/it, loss=0.033, v_num=0, train_loss_step=0.0214, val_loss=0.0471, train_loss_epoch=0.0374]\n",
      "Epoch 10: 100%|██████████| 107/107 [27:13<00:00, 15.27s/it, loss=0.033, v_num=0, train_loss_step=0.0214, val_loss=0.0459, train_loss_epoch=0.0374]\n",
      "Epoch 11:  93%|█████████▎| 100/107 [26:46<01:52, 16.07s/it, loss=0.0325, v_num=0, train_loss_step=0.036, val_loss=0.0459, train_loss_epoch=0.0345] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  94%|█████████▍| 101/107 [27:08<01:36, 16.12s/it, loss=0.0325, v_num=0, train_loss_step=0.036, val_loss=0.0459, train_loss_epoch=0.0345]\n",
      "Epoch 11:  95%|█████████▌| 102/107 [27:13<01:20, 16.01s/it, loss=0.0325, v_num=0, train_loss_step=0.036, val_loss=0.0459, train_loss_epoch=0.0345]\n",
      "Epoch 11:  96%|█████████▋| 103/107 [27:22<01:03, 15.95s/it, loss=0.0325, v_num=0, train_loss_step=0.036, val_loss=0.0459, train_loss_epoch=0.0345]\n",
      "Epoch 11:  97%|█████████▋| 104/107 [27:28<00:47, 15.85s/it, loss=0.0325, v_num=0, train_loss_step=0.036, val_loss=0.0459, train_loss_epoch=0.0345]\n",
      "Epoch 11:  98%|█████████▊| 105/107 [27:38<00:31, 15.79s/it, loss=0.0325, v_num=0, train_loss_step=0.036, val_loss=0.0459, train_loss_epoch=0.0345]\n",
      "Epoch 11:  99%|█████████▉| 106/107 [27:43<00:15, 15.69s/it, loss=0.0325, v_num=0, train_loss_step=0.036, val_loss=0.0459, train_loss_epoch=0.0345]\n",
      "Epoch 11: 100%|██████████| 107/107 [27:47<00:00, 15.58s/it, loss=0.0325, v_num=0, train_loss_step=0.036, val_loss=0.0449, train_loss_epoch=0.0345]\n",
      "Epoch 12:  93%|█████████▎| 100/107 [25:54<01:48, 15.55s/it, loss=0.0298, v_num=0, train_loss_step=0.0211, val_loss=0.0449, train_loss_epoch=0.0328]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  94%|█████████▍| 101/107 [26:17<01:33, 15.61s/it, loss=0.0298, v_num=0, train_loss_step=0.0211, val_loss=0.0449, train_loss_epoch=0.0328]\n",
      "Epoch 12:  95%|█████████▌| 102/107 [26:22<01:17, 15.51s/it, loss=0.0298, v_num=0, train_loss_step=0.0211, val_loss=0.0449, train_loss_epoch=0.0328]\n",
      "Epoch 12:  96%|█████████▋| 103/107 [26:30<01:01, 15.44s/it, loss=0.0298, v_num=0, train_loss_step=0.0211, val_loss=0.0449, train_loss_epoch=0.0328]\n",
      "Epoch 12:  97%|█████████▋| 104/107 [26:35<00:46, 15.34s/it, loss=0.0298, v_num=0, train_loss_step=0.0211, val_loss=0.0449, train_loss_epoch=0.0328]\n",
      "Epoch 12:  98%|█████████▊| 105/107 [26:44<00:30, 15.28s/it, loss=0.0298, v_num=0, train_loss_step=0.0211, val_loss=0.0449, train_loss_epoch=0.0328]\n",
      "Epoch 12:  99%|█████████▉| 106/107 [26:49<00:15, 15.18s/it, loss=0.0298, v_num=0, train_loss_step=0.0211, val_loss=0.0449, train_loss_epoch=0.0328]\n",
      "Epoch 12: 100%|██████████| 107/107 [26:53<00:00, 15.08s/it, loss=0.0298, v_num=0, train_loss_step=0.0211, val_loss=0.0435, train_loss_epoch=0.0328]\n",
      "Epoch 13:  93%|█████████▎| 100/107 [25:39<01:47, 15.39s/it, loss=0.0302, v_num=0, train_loss_step=0.0221, val_loss=0.0435, train_loss_epoch=0.0303]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  94%|█████████▍| 101/107 [26:01<01:32, 15.46s/it, loss=0.0302, v_num=0, train_loss_step=0.0221, val_loss=0.0435, train_loss_epoch=0.0303]\n",
      "Epoch 13:  95%|█████████▌| 102/107 [26:07<01:16, 15.36s/it, loss=0.0302, v_num=0, train_loss_step=0.0221, val_loss=0.0435, train_loss_epoch=0.0303]\n",
      "Epoch 13:  96%|█████████▋| 103/107 [26:17<01:01, 15.32s/it, loss=0.0302, v_num=0, train_loss_step=0.0221, val_loss=0.0435, train_loss_epoch=0.0303]\n",
      "Epoch 13:  97%|█████████▋| 104/107 [26:23<00:45, 15.22s/it, loss=0.0302, v_num=0, train_loss_step=0.0221, val_loss=0.0435, train_loss_epoch=0.0303]\n",
      "Epoch 13:  98%|█████████▊| 105/107 [26:32<00:30, 15.17s/it, loss=0.0302, v_num=0, train_loss_step=0.0221, val_loss=0.0435, train_loss_epoch=0.0303]\n",
      "Epoch 13:  99%|█████████▉| 106/107 [26:36<00:15, 15.07s/it, loss=0.0302, v_num=0, train_loss_step=0.0221, val_loss=0.0435, train_loss_epoch=0.0303]\n",
      "Epoch 13: 100%|██████████| 107/107 [26:41<00:00, 14.96s/it, loss=0.0302, v_num=0, train_loss_step=0.0221, val_loss=0.0477, train_loss_epoch=0.0303]\n",
      "Epoch 14:  93%|█████████▎| 100/107 [26:02<01:49, 15.63s/it, loss=0.0268, v_num=0, train_loss_step=0.0209, val_loss=0.0477, train_loss_epoch=0.0291]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  94%|█████████▍| 101/107 [26:25<01:34, 15.70s/it, loss=0.0268, v_num=0, train_loss_step=0.0209, val_loss=0.0477, train_loss_epoch=0.0291]\n",
      "Epoch 14:  95%|█████████▌| 102/107 [26:29<01:17, 15.59s/it, loss=0.0268, v_num=0, train_loss_step=0.0209, val_loss=0.0477, train_loss_epoch=0.0291]\n",
      "Epoch 14:  96%|█████████▋| 103/107 [26:39<01:02, 15.53s/it, loss=0.0268, v_num=0, train_loss_step=0.0209, val_loss=0.0477, train_loss_epoch=0.0291]\n",
      "Epoch 14:  97%|█████████▋| 104/107 [26:44<00:46, 15.43s/it, loss=0.0268, v_num=0, train_loss_step=0.0209, val_loss=0.0477, train_loss_epoch=0.0291]\n",
      "Epoch 14:  98%|█████████▊| 105/107 [26:53<00:30, 15.37s/it, loss=0.0268, v_num=0, train_loss_step=0.0209, val_loss=0.0477, train_loss_epoch=0.0291]\n",
      "Epoch 14:  99%|█████████▉| 106/107 [26:57<00:15, 15.26s/it, loss=0.0268, v_num=0, train_loss_step=0.0209, val_loss=0.0477, train_loss_epoch=0.0291]\n",
      "Epoch 14: 100%|██████████| 107/107 [27:01<00:00, 15.16s/it, loss=0.0268, v_num=0, train_loss_step=0.0209, val_loss=0.046, train_loss_epoch=0.0291] \n",
      "Epoch 15:  93%|█████████▎| 100/107 [26:05<01:49, 15.66s/it, loss=0.0264, v_num=0, train_loss_step=0.0299, val_loss=0.046, train_loss_epoch=0.0274]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  94%|█████████▍| 101/107 [26:29<01:34, 15.74s/it, loss=0.0264, v_num=0, train_loss_step=0.0299, val_loss=0.046, train_loss_epoch=0.0274]\n",
      "Epoch 15:  95%|█████████▌| 102/107 [26:33<01:18, 15.63s/it, loss=0.0264, v_num=0, train_loss_step=0.0299, val_loss=0.046, train_loss_epoch=0.0274]\n",
      "Epoch 15:  96%|█████████▋| 103/107 [26:43<01:02, 15.57s/it, loss=0.0264, v_num=0, train_loss_step=0.0299, val_loss=0.046, train_loss_epoch=0.0274]\n",
      "Epoch 15:  97%|█████████▋| 104/107 [26:48<00:46, 15.47s/it, loss=0.0264, v_num=0, train_loss_step=0.0299, val_loss=0.046, train_loss_epoch=0.0274]\n",
      "Epoch 15:  98%|█████████▊| 105/107 [26:58<00:30, 15.41s/it, loss=0.0264, v_num=0, train_loss_step=0.0299, val_loss=0.046, train_loss_epoch=0.0274]\n",
      "Epoch 15:  99%|█████████▉| 106/107 [27:02<00:15, 15.31s/it, loss=0.0264, v_num=0, train_loss_step=0.0299, val_loss=0.046, train_loss_epoch=0.0274]\n",
      "Epoch 15: 100%|██████████| 107/107 [27:06<00:00, 15.20s/it, loss=0.0264, v_num=0, train_loss_step=0.0299, val_loss=0.0549, train_loss_epoch=0.0274]\n",
      "Epoch 16:  93%|█████████▎| 100/107 [26:01<01:49, 15.62s/it, loss=0.026, v_num=0, train_loss_step=0.0246, val_loss=0.0549, train_loss_epoch=0.0257] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  94%|█████████▍| 101/107 [26:25<01:34, 15.70s/it, loss=0.026, v_num=0, train_loss_step=0.0246, val_loss=0.0549, train_loss_epoch=0.0257]\n",
      "Epoch 16:  95%|█████████▌| 102/107 [26:29<01:17, 15.58s/it, loss=0.026, v_num=0, train_loss_step=0.0246, val_loss=0.0549, train_loss_epoch=0.0257]\n",
      "Epoch 16:  96%|█████████▋| 103/107 [26:38<01:02, 15.52s/it, loss=0.026, v_num=0, train_loss_step=0.0246, val_loss=0.0549, train_loss_epoch=0.0257]\n",
      "Epoch 16:  97%|█████████▋| 104/107 [26:43<00:46, 15.42s/it, loss=0.026, v_num=0, train_loss_step=0.0246, val_loss=0.0549, train_loss_epoch=0.0257]\n",
      "Epoch 16:  98%|█████████▊| 105/107 [26:52<00:30, 15.36s/it, loss=0.026, v_num=0, train_loss_step=0.0246, val_loss=0.0549, train_loss_epoch=0.0257]\n",
      "Epoch 16:  99%|█████████▉| 106/107 [26:57<00:15, 15.26s/it, loss=0.026, v_num=0, train_loss_step=0.0246, val_loss=0.0549, train_loss_epoch=0.0257]\n",
      "Epoch 16: 100%|██████████| 107/107 [27:01<00:00, 15.15s/it, loss=0.026, v_num=0, train_loss_step=0.0246, val_loss=0.0458, train_loss_epoch=0.0257]\n",
      "Epoch 17:  93%|█████████▎| 100/107 [26:25<01:50, 15.86s/it, loss=0.0228, v_num=0, train_loss_step=0.0241, val_loss=0.0458, train_loss_epoch=0.0248]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  94%|█████████▍| 101/107 [26:49<01:35, 15.94s/it, loss=0.0228, v_num=0, train_loss_step=0.0241, val_loss=0.0458, train_loss_epoch=0.0248]\n",
      "Epoch 17:  95%|█████████▌| 102/107 [26:54<01:19, 15.83s/it, loss=0.0228, v_num=0, train_loss_step=0.0241, val_loss=0.0458, train_loss_epoch=0.0248]\n",
      "Epoch 17:  96%|█████████▋| 103/107 [27:05<01:03, 15.78s/it, loss=0.0228, v_num=0, train_loss_step=0.0241, val_loss=0.0458, train_loss_epoch=0.0248]\n",
      "Epoch 17:  97%|█████████▋| 104/107 [27:09<00:47, 15.67s/it, loss=0.0228, v_num=0, train_loss_step=0.0241, val_loss=0.0458, train_loss_epoch=0.0248]\n",
      "Epoch 17:  98%|█████████▊| 105/107 [27:19<00:31, 15.61s/it, loss=0.0228, v_num=0, train_loss_step=0.0241, val_loss=0.0458, train_loss_epoch=0.0248]\n",
      "Epoch 17:  99%|█████████▉| 106/107 [27:23<00:15, 15.51s/it, loss=0.0228, v_num=0, train_loss_step=0.0241, val_loss=0.0458, train_loss_epoch=0.0248]\n",
      "Epoch 17: 100%|██████████| 107/107 [27:28<00:00, 15.40s/it, loss=0.0228, v_num=0, train_loss_step=0.0241, val_loss=0.0479, train_loss_epoch=0.0248]\n",
      "Epoch 18:  93%|█████████▎| 100/107 [26:26<01:51, 15.86s/it, loss=0.0225, v_num=0, train_loss_step=0.0213, val_loss=0.0479, train_loss_epoch=0.0232]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  94%|█████████▍| 101/107 [26:48<01:35, 15.92s/it, loss=0.0225, v_num=0, train_loss_step=0.0213, val_loss=0.0479, train_loss_epoch=0.0232]\n",
      "Epoch 18:  95%|█████████▌| 102/107 [26:54<01:19, 15.82s/it, loss=0.0225, v_num=0, train_loss_step=0.0213, val_loss=0.0479, train_loss_epoch=0.0232]\n",
      "Epoch 18:  96%|█████████▋| 103/107 [27:04<01:03, 15.77s/it, loss=0.0225, v_num=0, train_loss_step=0.0213, val_loss=0.0479, train_loss_epoch=0.0232]\n",
      "Epoch 18:  97%|█████████▋| 104/107 [27:09<00:46, 15.66s/it, loss=0.0225, v_num=0, train_loss_step=0.0213, val_loss=0.0479, train_loss_epoch=0.0232]\n",
      "Epoch 18:  98%|█████████▊| 105/107 [27:19<00:31, 15.61s/it, loss=0.0225, v_num=0, train_loss_step=0.0213, val_loss=0.0479, train_loss_epoch=0.0232]\n",
      "Epoch 18:  99%|█████████▉| 106/107 [27:23<00:15, 15.50s/it, loss=0.0225, v_num=0, train_loss_step=0.0213, val_loss=0.0479, train_loss_epoch=0.0232]\n",
      "Epoch 18: 100%|██████████| 107/107 [27:27<00:00, 15.40s/it, loss=0.0225, v_num=0, train_loss_step=0.0213, val_loss=0.0467, train_loss_epoch=0.0232]\n",
      "Epoch 19:  93%|█████████▎| 100/107 [24:50<01:44, 14.91s/it, loss=0.0228, v_num=0, train_loss_step=0.0182, val_loss=0.0467, train_loss_epoch=0.0226]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  94%|█████████▍| 101/107 [25:14<01:29, 14.99s/it, loss=0.0228, v_num=0, train_loss_step=0.0182, val_loss=0.0467, train_loss_epoch=0.0226]\n",
      "Epoch 19:  95%|█████████▌| 102/107 [25:19<01:14, 14.90s/it, loss=0.0228, v_num=0, train_loss_step=0.0182, val_loss=0.0467, train_loss_epoch=0.0226]\n",
      "Epoch 19:  96%|█████████▋| 103/107 [25:29<00:59, 14.85s/it, loss=0.0228, v_num=0, train_loss_step=0.0182, val_loss=0.0467, train_loss_epoch=0.0226]\n",
      "Epoch 19:  97%|█████████▋| 104/107 [25:35<00:44, 14.76s/it, loss=0.0228, v_num=0, train_loss_step=0.0182, val_loss=0.0467, train_loss_epoch=0.0226]\n",
      "Epoch 19:  98%|█████████▊| 105/107 [25:44<00:29, 14.71s/it, loss=0.0228, v_num=0, train_loss_step=0.0182, val_loss=0.0467, train_loss_epoch=0.0226]\n",
      "Epoch 19:  99%|█████████▉| 106/107 [25:48<00:14, 14.61s/it, loss=0.0228, v_num=0, train_loss_step=0.0182, val_loss=0.0467, train_loss_epoch=0.0226]\n",
      "Epoch 19: 100%|██████████| 107/107 [25:52<00:00, 14.51s/it, loss=0.0228, v_num=0, train_loss_step=0.0182, val_loss=0.0445, train_loss_epoch=0.0226]\n",
      "Epoch 20:  93%|█████████▎| 100/107 [24:36<01:43, 14.77s/it, loss=0.0213, v_num=0, train_loss_step=0.0278, val_loss=0.0445, train_loss_epoch=0.022] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20:  94%|█████████▍| 101/107 [25:00<01:29, 14.85s/it, loss=0.0213, v_num=0, train_loss_step=0.0278, val_loss=0.0445, train_loss_epoch=0.022]\n",
      "Epoch 20:  95%|█████████▌| 102/107 [25:05<01:13, 14.76s/it, loss=0.0213, v_num=0, train_loss_step=0.0278, val_loss=0.0445, train_loss_epoch=0.022]\n",
      "Epoch 20:  96%|█████████▋| 103/107 [25:15<00:58, 14.72s/it, loss=0.0213, v_num=0, train_loss_step=0.0278, val_loss=0.0445, train_loss_epoch=0.022]\n",
      "Epoch 20:  97%|█████████▋| 104/107 [25:21<00:43, 14.63s/it, loss=0.0213, v_num=0, train_loss_step=0.0278, val_loss=0.0445, train_loss_epoch=0.022]\n",
      "Epoch 20:  98%|█████████▊| 105/107 [25:31<00:29, 14.58s/it, loss=0.0213, v_num=0, train_loss_step=0.0278, val_loss=0.0445, train_loss_epoch=0.022]\n",
      "Epoch 20:  99%|█████████▉| 106/107 [25:35<00:14, 14.48s/it, loss=0.0213, v_num=0, train_loss_step=0.0278, val_loss=0.0445, train_loss_epoch=0.022]\n",
      "Epoch 20: 100%|██████████| 107/107 [25:39<00:00, 14.38s/it, loss=0.0213, v_num=0, train_loss_step=0.0278, val_loss=0.052, train_loss_epoch=0.022] \n",
      "Epoch 21:  93%|█████████▎| 100/107 [24:24<01:42, 14.65s/it, loss=0.02, v_num=0, train_loss_step=0.0204, val_loss=0.052, train_loss_epoch=0.0217]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21:  94%|█████████▍| 101/107 [24:46<01:28, 14.72s/it, loss=0.02, v_num=0, train_loss_step=0.0204, val_loss=0.052, train_loss_epoch=0.0217]\n",
      "Epoch 21:  95%|█████████▌| 102/107 [24:52<01:13, 14.63s/it, loss=0.02, v_num=0, train_loss_step=0.0204, val_loss=0.052, train_loss_epoch=0.0217]\n",
      "Epoch 21:  96%|█████████▋| 103/107 [25:02<00:58, 14.58s/it, loss=0.02, v_num=0, train_loss_step=0.0204, val_loss=0.052, train_loss_epoch=0.0217]\n",
      "Epoch 21:  97%|█████████▋| 104/107 [25:07<00:43, 14.49s/it, loss=0.02, v_num=0, train_loss_step=0.0204, val_loss=0.052, train_loss_epoch=0.0217]\n",
      "Epoch 21:  98%|█████████▊| 105/107 [25:17<00:28, 14.45s/it, loss=0.02, v_num=0, train_loss_step=0.0204, val_loss=0.052, train_loss_epoch=0.0217]\n",
      "Epoch 21:  99%|█████████▉| 106/107 [25:21<00:14, 14.36s/it, loss=0.02, v_num=0, train_loss_step=0.0204, val_loss=0.052, train_loss_epoch=0.0217]\n",
      "Epoch 21: 100%|██████████| 107/107 [25:25<00:00, 14.26s/it, loss=0.02, v_num=0, train_loss_step=0.0204, val_loss=0.0523, train_loss_epoch=0.0217]\n",
      "Epoch 22:  93%|█████████▎| 100/107 [26:47<01:52, 16.07s/it, loss=0.0201, v_num=0, train_loss_step=0.0241, val_loss=0.0523, train_loss_epoch=0.0211]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22:  94%|█████████▍| 101/107 [27:09<01:36, 16.13s/it, loss=0.0201, v_num=0, train_loss_step=0.0241, val_loss=0.0523, train_loss_epoch=0.0211]\n",
      "Epoch 22:  95%|█████████▌| 102/107 [27:15<01:20, 16.03s/it, loss=0.0201, v_num=0, train_loss_step=0.0241, val_loss=0.0523, train_loss_epoch=0.0211]\n",
      "Epoch 22:  96%|█████████▋| 103/107 [27:24<01:03, 15.97s/it, loss=0.0201, v_num=0, train_loss_step=0.0241, val_loss=0.0523, train_loss_epoch=0.0211]\n",
      "Epoch 22:  97%|█████████▋| 104/107 [27:29<00:47, 15.86s/it, loss=0.0201, v_num=0, train_loss_step=0.0241, val_loss=0.0523, train_loss_epoch=0.0211]\n",
      "Epoch 22:  98%|█████████▊| 105/107 [27:38<00:31, 15.80s/it, loss=0.0201, v_num=0, train_loss_step=0.0241, val_loss=0.0523, train_loss_epoch=0.0211]\n",
      "Epoch 22:  99%|█████████▉| 106/107 [27:43<00:15, 15.69s/it, loss=0.0201, v_num=0, train_loss_step=0.0241, val_loss=0.0523, train_loss_epoch=0.0211]\n",
      "Epoch 22: 100%|██████████| 107/107 [27:47<00:00, 15.58s/it, loss=0.0201, v_num=0, train_loss_step=0.0241, val_loss=0.0484, train_loss_epoch=0.0211]\n",
      "Epoch 23:  93%|█████████▎| 100/107 [25:16<01:46, 15.16s/it, loss=0.0202, v_num=0, train_loss_step=0.0207, val_loss=0.0484, train_loss_epoch=0.0209]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23:  94%|█████████▍| 101/107 [25:38<01:31, 15.24s/it, loss=0.0202, v_num=0, train_loss_step=0.0207, val_loss=0.0484, train_loss_epoch=0.0209]\n",
      "Epoch 23:  95%|█████████▌| 102/107 [25:43<01:15, 15.13s/it, loss=0.0202, v_num=0, train_loss_step=0.0207, val_loss=0.0484, train_loss_epoch=0.0209]\n",
      "Epoch 23:  96%|█████████▋| 103/107 [25:52<01:00, 15.07s/it, loss=0.0202, v_num=0, train_loss_step=0.0207, val_loss=0.0484, train_loss_epoch=0.0209]\n",
      "Epoch 23:  97%|█████████▋| 104/107 [25:56<00:44, 14.97s/it, loss=0.0202, v_num=0, train_loss_step=0.0207, val_loss=0.0484, train_loss_epoch=0.0209]\n",
      "Epoch 23:  98%|█████████▊| 105/107 [26:06<00:29, 14.92s/it, loss=0.0202, v_num=0, train_loss_step=0.0207, val_loss=0.0484, train_loss_epoch=0.0209]\n",
      "Epoch 23:  99%|█████████▉| 106/107 [26:10<00:14, 14.82s/it, loss=0.0202, v_num=0, train_loss_step=0.0207, val_loss=0.0484, train_loss_epoch=0.0209]\n",
      "Epoch 23: 100%|██████████| 107/107 [26:15<00:00, 14.72s/it, loss=0.0202, v_num=0, train_loss_step=0.0207, val_loss=0.0465, train_loss_epoch=0.0209]\n",
      "Epoch 24:  93%|█████████▎| 100/107 [26:09<01:49, 15.70s/it, loss=0.0192, v_num=0, train_loss_step=0.0159, val_loss=0.0465, train_loss_epoch=0.0201]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24:  94%|█████████▍| 101/107 [26:33<01:34, 15.78s/it, loss=0.0192, v_num=0, train_loss_step=0.0159, val_loss=0.0465, train_loss_epoch=0.0201]\n",
      "Epoch 24:  95%|█████████▌| 102/107 [26:38<01:18, 15.67s/it, loss=0.0192, v_num=0, train_loss_step=0.0159, val_loss=0.0465, train_loss_epoch=0.0201]\n",
      "Epoch 24:  96%|█████████▋| 103/107 [26:47<01:02, 15.61s/it, loss=0.0192, v_num=0, train_loss_step=0.0159, val_loss=0.0465, train_loss_epoch=0.0201]\n",
      "Epoch 24:  97%|█████████▋| 104/107 [26:52<00:46, 15.50s/it, loss=0.0192, v_num=0, train_loss_step=0.0159, val_loss=0.0465, train_loss_epoch=0.0201]\n",
      "Epoch 24:  98%|█████████▊| 105/107 [27:02<00:30, 15.46s/it, loss=0.0192, v_num=0, train_loss_step=0.0159, val_loss=0.0465, train_loss_epoch=0.0201]\n",
      "Epoch 24:  99%|█████████▉| 106/107 [27:06<00:15, 15.35s/it, loss=0.0192, v_num=0, train_loss_step=0.0159, val_loss=0.0465, train_loss_epoch=0.0201]\n",
      "Epoch 24: 100%|██████████| 107/107 [27:11<00:00, 15.24s/it, loss=0.0192, v_num=0, train_loss_step=0.0159, val_loss=0.0434, train_loss_epoch=0.0201]\n",
      "Epoch 25:  93%|█████████▎| 100/107 [25:51<01:48, 15.51s/it, loss=0.0178, v_num=0, train_loss_step=0.0165, val_loss=0.0434, train_loss_epoch=0.0188]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25:  94%|█████████▍| 101/107 [26:12<01:33, 15.57s/it, loss=0.0178, v_num=0, train_loss_step=0.0165, val_loss=0.0434, train_loss_epoch=0.0188]\n",
      "Epoch 25:  95%|█████████▌| 102/107 [26:17<01:17, 15.47s/it, loss=0.0178, v_num=0, train_loss_step=0.0165, val_loss=0.0434, train_loss_epoch=0.0188]\n",
      "Epoch 25:  96%|█████████▋| 103/107 [26:26<01:01, 15.40s/it, loss=0.0178, v_num=0, train_loss_step=0.0165, val_loss=0.0434, train_loss_epoch=0.0188]\n",
      "Epoch 25:  97%|█████████▋| 104/107 [26:31<00:45, 15.30s/it, loss=0.0178, v_num=0, train_loss_step=0.0165, val_loss=0.0434, train_loss_epoch=0.0188]\n",
      "Epoch 25:  98%|█████████▊| 105/107 [26:40<00:30, 15.24s/it, loss=0.0178, v_num=0, train_loss_step=0.0165, val_loss=0.0434, train_loss_epoch=0.0188]\n",
      "Epoch 25:  99%|█████████▉| 106/107 [26:45<00:15, 15.14s/it, loss=0.0178, v_num=0, train_loss_step=0.0165, val_loss=0.0434, train_loss_epoch=0.0188]\n",
      "Epoch 25: 100%|██████████| 107/107 [26:49<00:00, 15.04s/it, loss=0.0178, v_num=0, train_loss_step=0.0165, val_loss=0.0447, train_loss_epoch=0.0188]\n",
      "Epoch 26:  93%|█████████▎| 100/107 [26:00<01:49, 15.61s/it, loss=0.0176, v_num=0, train_loss_step=0.0147, val_loss=0.0447, train_loss_epoch=0.0185]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26:  94%|█████████▍| 101/107 [26:25<01:34, 15.69s/it, loss=0.0176, v_num=0, train_loss_step=0.0147, val_loss=0.0447, train_loss_epoch=0.0185]\n",
      "Epoch 26:  95%|█████████▌| 102/107 [26:29<01:17, 15.58s/it, loss=0.0176, v_num=0, train_loss_step=0.0147, val_loss=0.0447, train_loss_epoch=0.0185]\n",
      "Epoch 26:  96%|█████████▋| 103/107 [26:39<01:02, 15.53s/it, loss=0.0176, v_num=0, train_loss_step=0.0147, val_loss=0.0447, train_loss_epoch=0.0185]\n",
      "Epoch 26:  97%|█████████▋| 104/107 [26:44<00:46, 15.43s/it, loss=0.0176, v_num=0, train_loss_step=0.0147, val_loss=0.0447, train_loss_epoch=0.0185]\n",
      "Epoch 26:  98%|█████████▊| 105/107 [26:54<00:30, 15.38s/it, loss=0.0176, v_num=0, train_loss_step=0.0147, val_loss=0.0447, train_loss_epoch=0.0185]\n",
      "Epoch 26:  99%|█████████▉| 106/107 [26:58<00:15, 15.27s/it, loss=0.0176, v_num=0, train_loss_step=0.0147, val_loss=0.0447, train_loss_epoch=0.0185]\n",
      "Epoch 26: 100%|██████████| 107/107 [27:03<00:00, 15.17s/it, loss=0.0176, v_num=0, train_loss_step=0.0147, val_loss=0.0461, train_loss_epoch=0.0185]\n",
      "Epoch 27:  93%|█████████▎| 100/107 [25:05<01:45, 15.06s/it, loss=0.0171, v_num=0, train_loss_step=0.0142, val_loss=0.0461, train_loss_epoch=0.0179]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27:  94%|█████████▍| 101/107 [25:26<01:30, 15.12s/it, loss=0.0171, v_num=0, train_loss_step=0.0142, val_loss=0.0461, train_loss_epoch=0.0179]\n",
      "Epoch 27:  95%|█████████▌| 102/107 [25:31<01:15, 15.01s/it, loss=0.0171, v_num=0, train_loss_step=0.0142, val_loss=0.0461, train_loss_epoch=0.0179]\n",
      "Epoch 27:  96%|█████████▋| 103/107 [25:39<00:59, 14.95s/it, loss=0.0171, v_num=0, train_loss_step=0.0142, val_loss=0.0461, train_loss_epoch=0.0179]\n",
      "Epoch 27:  97%|█████████▋| 104/107 [25:44<00:44, 14.85s/it, loss=0.0171, v_num=0, train_loss_step=0.0142, val_loss=0.0461, train_loss_epoch=0.0179]\n",
      "Epoch 27:  98%|█████████▊| 105/107 [25:53<00:29, 14.80s/it, loss=0.0171, v_num=0, train_loss_step=0.0142, val_loss=0.0461, train_loss_epoch=0.0179]\n",
      "Epoch 27:  99%|█████████▉| 106/107 [25:57<00:14, 14.70s/it, loss=0.0171, v_num=0, train_loss_step=0.0142, val_loss=0.0461, train_loss_epoch=0.0179]\n",
      "Epoch 27: 100%|██████████| 107/107 [26:02<00:00, 14.60s/it, loss=0.0171, v_num=0, train_loss_step=0.0142, val_loss=0.052, train_loss_epoch=0.0179] \n",
      "Epoch 28:  93%|█████████▎| 100/107 [24:19<01:42, 14.60s/it, loss=0.0173, v_num=0, train_loss_step=0.019, val_loss=0.052, train_loss_epoch=0.0172] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 28:  94%|█████████▍| 101/107 [24:41<01:28, 14.67s/it, loss=0.0173, v_num=0, train_loss_step=0.019, val_loss=0.052, train_loss_epoch=0.0172]\n",
      "Epoch 28:  95%|█████████▌| 102/107 [24:46<01:12, 14.58s/it, loss=0.0173, v_num=0, train_loss_step=0.019, val_loss=0.052, train_loss_epoch=0.0172]\n",
      "Epoch 28:  96%|█████████▋| 103/107 [24:56<00:58, 14.53s/it, loss=0.0173, v_num=0, train_loss_step=0.019, val_loss=0.052, train_loss_epoch=0.0172]\n",
      "Epoch 28:  97%|█████████▋| 104/107 [25:01<00:43, 14.43s/it, loss=0.0173, v_num=0, train_loss_step=0.019, val_loss=0.052, train_loss_epoch=0.0172]\n",
      "Epoch 28:  98%|█████████▊| 105/107 [25:10<00:28, 14.39s/it, loss=0.0173, v_num=0, train_loss_step=0.019, val_loss=0.052, train_loss_epoch=0.0172]\n",
      "Epoch 28:  99%|█████████▉| 106/107 [25:15<00:14, 14.29s/it, loss=0.0173, v_num=0, train_loss_step=0.019, val_loss=0.052, train_loss_epoch=0.0172]\n",
      "Epoch 28: 100%|██████████| 107/107 [25:19<00:00, 14.20s/it, loss=0.0173, v_num=0, train_loss_step=0.019, val_loss=0.0457, train_loss_epoch=0.0172]\n",
      "Epoch 29:  93%|█████████▎| 100/107 [24:23<01:42, 14.63s/it, loss=0.0154, v_num=0, train_loss_step=0.0174, val_loss=0.0457, train_loss_epoch=0.0173]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29:  94%|█████████▍| 101/107 [24:45<01:28, 14.71s/it, loss=0.0154, v_num=0, train_loss_step=0.0174, val_loss=0.0457, train_loss_epoch=0.0173]\n",
      "Epoch 29:  95%|█████████▌| 102/107 [24:50<01:13, 14.61s/it, loss=0.0154, v_num=0, train_loss_step=0.0174, val_loss=0.0457, train_loss_epoch=0.0173]\n",
      "Epoch 29:  96%|█████████▋| 103/107 [24:59<00:58, 14.56s/it, loss=0.0154, v_num=0, train_loss_step=0.0174, val_loss=0.0457, train_loss_epoch=0.0173]\n",
      "Epoch 29:  97%|█████████▋| 104/107 [25:03<00:43, 14.46s/it, loss=0.0154, v_num=0, train_loss_step=0.0174, val_loss=0.0457, train_loss_epoch=0.0173]\n",
      "Epoch 29:  98%|█████████▊| 105/107 [25:14<00:28, 14.42s/it, loss=0.0154, v_num=0, train_loss_step=0.0174, val_loss=0.0457, train_loss_epoch=0.0173]\n",
      "Epoch 29:  99%|█████████▉| 106/107 [25:18<00:14, 14.32s/it, loss=0.0154, v_num=0, train_loss_step=0.0174, val_loss=0.0457, train_loss_epoch=0.0173]\n",
      "Epoch 29: 100%|██████████| 107/107 [25:22<00:00, 14.23s/it, loss=0.0154, v_num=0, train_loss_step=0.0174, val_loss=0.0487, train_loss_epoch=0.0173]\n",
      "Epoch 30:  93%|█████████▎| 100/107 [24:21<01:42, 14.62s/it, loss=0.0161, v_num=0, train_loss_step=0.0154, val_loss=0.0487, train_loss_epoch=0.0165]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 30:  94%|█████████▍| 101/107 [24:44<01:28, 14.70s/it, loss=0.0161, v_num=0, train_loss_step=0.0154, val_loss=0.0487, train_loss_epoch=0.0165]\n",
      "Epoch 30:  95%|█████████▌| 102/107 [24:48<01:12, 14.60s/it, loss=0.0161, v_num=0, train_loss_step=0.0154, val_loss=0.0487, train_loss_epoch=0.0165]\n",
      "Epoch 30:  96%|█████████▋| 103/107 [24:58<00:58, 14.55s/it, loss=0.0161, v_num=0, train_loss_step=0.0154, val_loss=0.0487, train_loss_epoch=0.0165]\n",
      "Epoch 30:  97%|█████████▋| 104/107 [25:03<00:43, 14.45s/it, loss=0.0161, v_num=0, train_loss_step=0.0154, val_loss=0.0487, train_loss_epoch=0.0165]\n",
      "Epoch 30:  98%|█████████▊| 105/107 [25:13<00:28, 14.41s/it, loss=0.0161, v_num=0, train_loss_step=0.0154, val_loss=0.0487, train_loss_epoch=0.0165]\n",
      "Epoch 30:  99%|█████████▉| 106/107 [25:17<00:14, 14.31s/it, loss=0.0161, v_num=0, train_loss_step=0.0154, val_loss=0.0487, train_loss_epoch=0.0165]\n",
      "Epoch 30: 100%|██████████| 107/107 [25:21<00:00, 14.22s/it, loss=0.0161, v_num=0, train_loss_step=0.0154, val_loss=0.0585, train_loss_epoch=0.0165]\n",
      "Epoch 31:  93%|█████████▎| 100/107 [24:47<01:44, 14.88s/it, loss=0.0163, v_num=0, train_loss_step=0.0151, val_loss=0.0585, train_loss_epoch=0.0161]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 31:  94%|█████████▍| 101/107 [25:10<01:29, 14.95s/it, loss=0.0163, v_num=0, train_loss_step=0.0151, val_loss=0.0585, train_loss_epoch=0.0161]\n",
      "Epoch 31:  95%|█████████▌| 102/107 [25:15<01:14, 14.86s/it, loss=0.0163, v_num=0, train_loss_step=0.0151, val_loss=0.0585, train_loss_epoch=0.0161]\n",
      "Epoch 31:  96%|█████████▋| 103/107 [25:25<00:59, 14.81s/it, loss=0.0163, v_num=0, train_loss_step=0.0151, val_loss=0.0585, train_loss_epoch=0.0161]\n",
      "Epoch 31:  97%|█████████▋| 104/107 [25:29<00:44, 14.71s/it, loss=0.0163, v_num=0, train_loss_step=0.0151, val_loss=0.0585, train_loss_epoch=0.0161]\n",
      "Epoch 31:  98%|█████████▊| 105/107 [25:39<00:29, 14.66s/it, loss=0.0163, v_num=0, train_loss_step=0.0151, val_loss=0.0585, train_loss_epoch=0.0161]\n",
      "Epoch 31:  99%|█████████▉| 106/107 [25:43<00:14, 14.57s/it, loss=0.0163, v_num=0, train_loss_step=0.0151, val_loss=0.0585, train_loss_epoch=0.0161]\n",
      "Epoch 31: 100%|██████████| 107/107 [25:48<00:00, 14.47s/it, loss=0.0163, v_num=0, train_loss_step=0.0151, val_loss=0.0502, train_loss_epoch=0.0161]\n",
      "Epoch 32:  93%|█████████▎| 100/107 [26:00<01:49, 15.61s/it, loss=0.0155, v_num=0, train_loss_step=0.020, val_loss=0.0502, train_loss_epoch=0.0165] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 32:  94%|█████████▍| 101/107 [26:24<01:34, 15.69s/it, loss=0.0155, v_num=0, train_loss_step=0.020, val_loss=0.0502, train_loss_epoch=0.0165]\n",
      "Epoch 32:  95%|█████████▌| 102/107 [26:30<01:17, 15.59s/it, loss=0.0155, v_num=0, train_loss_step=0.020, val_loss=0.0502, train_loss_epoch=0.0165]\n",
      "Epoch 32:  96%|█████████▋| 103/107 [26:40<01:02, 15.54s/it, loss=0.0155, v_num=0, train_loss_step=0.020, val_loss=0.0502, train_loss_epoch=0.0165]\n",
      "Epoch 32:  97%|█████████▋| 104/107 [26:45<00:46, 15.44s/it, loss=0.0155, v_num=0, train_loss_step=0.020, val_loss=0.0502, train_loss_epoch=0.0165]\n",
      "Epoch 32:  98%|█████████▊| 105/107 [26:54<00:30, 15.38s/it, loss=0.0155, v_num=0, train_loss_step=0.020, val_loss=0.0502, train_loss_epoch=0.0165]\n",
      "Epoch 32:  99%|█████████▉| 106/107 [26:59<00:15, 15.27s/it, loss=0.0155, v_num=0, train_loss_step=0.020, val_loss=0.0502, train_loss_epoch=0.0165]\n",
      "Epoch 32: 100%|██████████| 107/107 [27:03<00:00, 15.17s/it, loss=0.0155, v_num=0, train_loss_step=0.020, val_loss=0.0512, train_loss_epoch=0.0165]\n",
      "Epoch 33:  93%|█████████▎| 100/107 [27:56<01:57, 16.77s/it, loss=0.0167, v_num=0, train_loss_step=0.0197, val_loss=0.0512, train_loss_epoch=0.0161]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 33:  94%|█████████▍| 101/107 [28:18<01:40, 16.81s/it, loss=0.0167, v_num=0, train_loss_step=0.0197, val_loss=0.0512, train_loss_epoch=0.0161]\n",
      "Epoch 33:  95%|█████████▌| 102/107 [28:22<01:23, 16.69s/it, loss=0.0167, v_num=0, train_loss_step=0.0197, val_loss=0.0512, train_loss_epoch=0.0161]\n",
      "Epoch 33:  96%|█████████▋| 103/107 [28:31<01:06, 16.62s/it, loss=0.0167, v_num=0, train_loss_step=0.0197, val_loss=0.0512, train_loss_epoch=0.0161]\n",
      "Epoch 33:  97%|█████████▋| 104/107 [28:35<00:49, 16.50s/it, loss=0.0167, v_num=0, train_loss_step=0.0197, val_loss=0.0512, train_loss_epoch=0.0161]\n",
      "Epoch 33:  98%|█████████▊| 105/107 [28:45<00:32, 16.43s/it, loss=0.0167, v_num=0, train_loss_step=0.0197, val_loss=0.0512, train_loss_epoch=0.0161]\n",
      "Epoch 33:  99%|█████████▉| 106/107 [28:49<00:16, 16.32s/it, loss=0.0167, v_num=0, train_loss_step=0.0197, val_loss=0.0512, train_loss_epoch=0.0161]\n",
      "Epoch 33: 100%|██████████| 107/107 [28:54<00:00, 16.21s/it, loss=0.0167, v_num=0, train_loss_step=0.0197, val_loss=0.0489, train_loss_epoch=0.0161]\n",
      "Epoch 34:  93%|█████████▎| 100/107 [26:04<01:49, 15.64s/it, loss=0.0163, v_num=0, train_loss_step=0.0178, val_loss=0.0489, train_loss_epoch=0.0156]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 34:  94%|█████████▍| 101/107 [26:26<01:34, 15.71s/it, loss=0.0163, v_num=0, train_loss_step=0.0178, val_loss=0.0489, train_loss_epoch=0.0156]\n",
      "Epoch 34:  95%|█████████▌| 102/107 [26:30<01:17, 15.60s/it, loss=0.0163, v_num=0, train_loss_step=0.0178, val_loss=0.0489, train_loss_epoch=0.0156]\n",
      "Epoch 34:  96%|█████████▋| 103/107 [26:39<01:02, 15.53s/it, loss=0.0163, v_num=0, train_loss_step=0.0178, val_loss=0.0489, train_loss_epoch=0.0156]\n",
      "Epoch 34:  97%|█████████▋| 104/107 [26:44<00:46, 15.43s/it, loss=0.0163, v_num=0, train_loss_step=0.0178, val_loss=0.0489, train_loss_epoch=0.0156]\n",
      "Epoch 34:  98%|█████████▊| 105/107 [26:53<00:30, 15.37s/it, loss=0.0163, v_num=0, train_loss_step=0.0178, val_loss=0.0489, train_loss_epoch=0.0156]\n",
      "Epoch 34:  99%|█████████▉| 106/107 [26:57<00:15, 15.26s/it, loss=0.0163, v_num=0, train_loss_step=0.0178, val_loss=0.0489, train_loss_epoch=0.0156]\n",
      "Epoch 34: 100%|██████████| 107/107 [27:01<00:00, 15.16s/it, loss=0.0163, v_num=0, train_loss_step=0.0178, val_loss=0.0524, train_loss_epoch=0.0156]\n",
      "Epoch 34: 100%|██████████| 107/107 [27:02<00:00, 15.16s/it, loss=0.0163, v_num=0, train_loss_step=0.0178, val_loss=0.0524, train_loss_epoch=0.0154]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=35` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 107/107 [27:24<00:00, 15.37s/it, loss=0.0163, v_num=0, train_loss_step=0.0178, val_loss=0.0524, train_loss_epoch=0.0154]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger_name = f\"TFT_2: batch_size={batch_size}, encoder_length={encoder_length}, group={group}, known_reals={known_reals}, loss=MAE\"\n",
    "\n",
    "logger = TensorBoardLogger('/tf_logs', name=logger_name)\n",
    "\n",
    "# trainer = Trainer(gpus=1, max_epochs=100, limit_train_batches=2606, logger=logger)\n",
    "trainer = Trainer(accelerator='gpu', devices=1, logger=logger, max_epochs=35)\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader)\n",
    "# trainer.validate(model=model, dataloaders=valid_dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model according to the validation loss\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "# best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "# best_tft = FullyConnectedModelWithCovariates.load_from_checkpoint(best_model_path)\n",
    "trainer.save_checkpoint(\"tft_best_model2.ckpt\")\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(\"tft_best_model2.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0524)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calcualte mean absolute error on validation set\n",
    "actuals = torch.cat([y[0] for x, y in iter(valid_dataloader)])\n",
    "predictions = best_tft.predict(valid_dataloader)\n",
    "(actuals - predictions).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([304, 1]) <class 'torch.Tensor'> torch.Size([304, 1])\n",
      "tensor([[0.8284],\n",
      "        [0.8284],\n",
      "        [0.8284],\n",
      "        [0.8284],\n",
      "        [0.8284],\n",
      "        [0.8284],\n",
      "        [0.8284],\n",
      "        [0.8284],\n",
      "        [0.8284],\n",
      "        [0.8284],\n",
      "        [0.8284],\n",
      "        [0.8284],\n",
      "        [0.8284],\n",
      "        [0.8284],\n",
      "        [0.8284],\n",
      "        [0.8284],\n",
      "        [0.5919],\n",
      "        [0.5919],\n",
      "        [0.5919],\n",
      "        [0.5919],\n",
      "        [0.5919],\n",
      "        [0.5919],\n",
      "        [0.5919],\n",
      "        [0.5919],\n",
      "        [0.5919],\n",
      "        [0.5919],\n",
      "        [0.5919],\n",
      "        [0.5919],\n",
      "        [0.5919],\n",
      "        [0.5919],\n",
      "        [0.5919],\n",
      "        [0.5919],\n",
      "        [0.5231],\n",
      "        [0.5231],\n",
      "        [0.5231],\n",
      "        [0.5231],\n",
      "        [0.5231],\n",
      "        [0.5231],\n",
      "        [0.5231],\n",
      "        [0.5231],\n",
      "        [0.5231],\n",
      "        [0.5231],\n",
      "        [0.5231],\n",
      "        [0.5231],\n",
      "        [0.5231],\n",
      "        [0.5231],\n",
      "        [0.5231],\n",
      "        [0.5231],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.4471],\n",
      "        [0.4471],\n",
      "        [0.4471],\n",
      "        [0.4471],\n",
      "        [0.4471],\n",
      "        [0.4471],\n",
      "        [0.4471],\n",
      "        [0.4471],\n",
      "        [0.4471],\n",
      "        [0.4471],\n",
      "        [0.4471],\n",
      "        [0.4471],\n",
      "        [0.4471],\n",
      "        [0.4471],\n",
      "        [0.4471],\n",
      "        [0.4471],\n",
      "        [0.6626],\n",
      "        [0.6626],\n",
      "        [0.6626],\n",
      "        [0.6626],\n",
      "        [0.6626],\n",
      "        [0.6626],\n",
      "        [0.6626],\n",
      "        [0.6626],\n",
      "        [0.6626],\n",
      "        [0.6626],\n",
      "        [0.6626],\n",
      "        [0.6626],\n",
      "        [0.6626],\n",
      "        [0.6626],\n",
      "        [0.6626],\n",
      "        [0.6626],\n",
      "        [0.6000],\n",
      "        [0.6000],\n",
      "        [0.6000],\n",
      "        [0.6000],\n",
      "        [0.6000],\n",
      "        [0.6000],\n",
      "        [0.6000],\n",
      "        [0.6000],\n",
      "        [0.6000],\n",
      "        [0.6000],\n",
      "        [0.6000],\n",
      "        [0.6000],\n",
      "        [0.6000],\n",
      "        [0.6000],\n",
      "        [0.6000],\n",
      "        [0.6000],\n",
      "        [0.4140],\n",
      "        [0.4140],\n",
      "        [0.4140],\n",
      "        [0.4140],\n",
      "        [0.4140],\n",
      "        [0.4140],\n",
      "        [0.4140],\n",
      "        [0.4140],\n",
      "        [0.4140],\n",
      "        [0.4140],\n",
      "        [0.4140],\n",
      "        [0.4140],\n",
      "        [0.4140],\n",
      "        [0.4140],\n",
      "        [0.4140],\n",
      "        [0.4140],\n",
      "        [0.5646],\n",
      "        [0.5646],\n",
      "        [0.5646],\n",
      "        [0.5646],\n",
      "        [0.5646],\n",
      "        [0.5646],\n",
      "        [0.5646],\n",
      "        [0.5646],\n",
      "        [0.5646],\n",
      "        [0.5646],\n",
      "        [0.5646],\n",
      "        [0.5646],\n",
      "        [0.5646],\n",
      "        [0.5646],\n",
      "        [0.5646],\n",
      "        [0.5646],\n",
      "        [0.4678],\n",
      "        [0.4678],\n",
      "        [0.4678],\n",
      "        [0.4678],\n",
      "        [0.4678],\n",
      "        [0.4678],\n",
      "        [0.4678],\n",
      "        [0.4678],\n",
      "        [0.4678],\n",
      "        [0.4678],\n",
      "        [0.4678],\n",
      "        [0.4678],\n",
      "        [0.4678],\n",
      "        [0.4678],\n",
      "        [0.4678],\n",
      "        [0.4678],\n",
      "        [0.5647],\n",
      "        [0.5647],\n",
      "        [0.5647],\n",
      "        [0.5647],\n",
      "        [0.5647],\n",
      "        [0.5647],\n",
      "        [0.5647],\n",
      "        [0.5647],\n",
      "        [0.5647],\n",
      "        [0.5647],\n",
      "        [0.5647],\n",
      "        [0.5647],\n",
      "        [0.5647],\n",
      "        [0.5647],\n",
      "        [0.5647],\n",
      "        [0.5647],\n",
      "        [0.5682],\n",
      "        [0.5682],\n",
      "        [0.5682],\n",
      "        [0.5682],\n",
      "        [0.5682],\n",
      "        [0.5682],\n",
      "        [0.5682],\n",
      "        [0.5682],\n",
      "        [0.5682],\n",
      "        [0.5682],\n",
      "        [0.5682],\n",
      "        [0.5682],\n",
      "        [0.5682],\n",
      "        [0.5682],\n",
      "        [0.5682],\n",
      "        [0.5682],\n",
      "        [0.4301],\n",
      "        [0.4301],\n",
      "        [0.4301],\n",
      "        [0.4301],\n",
      "        [0.4301],\n",
      "        [0.4301],\n",
      "        [0.4301],\n",
      "        [0.4301],\n",
      "        [0.4301],\n",
      "        [0.4301],\n",
      "        [0.4301],\n",
      "        [0.4301],\n",
      "        [0.4301],\n",
      "        [0.4301],\n",
      "        [0.4301],\n",
      "        [0.4301],\n",
      "        [0.5187],\n",
      "        [0.5187],\n",
      "        [0.5187],\n",
      "        [0.5187],\n",
      "        [0.5187],\n",
      "        [0.5187],\n",
      "        [0.5187],\n",
      "        [0.5187],\n",
      "        [0.5187],\n",
      "        [0.5187],\n",
      "        [0.5187],\n",
      "        [0.5187],\n",
      "        [0.5187],\n",
      "        [0.5187],\n",
      "        [0.5187],\n",
      "        [0.5187],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.5617],\n",
      "        [0.5617],\n",
      "        [0.5617],\n",
      "        [0.5617],\n",
      "        [0.5617],\n",
      "        [0.5617],\n",
      "        [0.5617],\n",
      "        [0.5617],\n",
      "        [0.5617],\n",
      "        [0.5617],\n",
      "        [0.5617],\n",
      "        [0.5617],\n",
      "        [0.5617],\n",
      "        [0.5617],\n",
      "        [0.5617],\n",
      "        [0.5617],\n",
      "        [0.6129],\n",
      "        [0.6129],\n",
      "        [0.6129],\n",
      "        [0.6129],\n",
      "        [0.6129],\n",
      "        [0.6129],\n",
      "        [0.6129],\n",
      "        [0.6129],\n",
      "        [0.6129],\n",
      "        [0.6129],\n",
      "        [0.6129],\n",
      "        [0.6129],\n",
      "        [0.6129],\n",
      "        [0.6129],\n",
      "        [0.6129],\n",
      "        [0.6129],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.5816],\n",
      "        [0.5816],\n",
      "        [0.5816],\n",
      "        [0.5816],\n",
      "        [0.5816],\n",
      "        [0.5816],\n",
      "        [0.5816],\n",
      "        [0.5816],\n",
      "        [0.5816],\n",
      "        [0.5816],\n",
      "        [0.5816],\n",
      "        [0.5816],\n",
      "        [0.5816],\n",
      "        [0.5816],\n",
      "        [0.5816],\n",
      "        [0.5816]]) tensor([[0.7182],\n",
      "        [0.7109],\n",
      "        [0.7137],\n",
      "        [0.7144],\n",
      "        [0.7155],\n",
      "        [0.7156],\n",
      "        [0.7128],\n",
      "        [0.7096],\n",
      "        [0.7084],\n",
      "        [0.7012],\n",
      "        [0.6953],\n",
      "        [0.6895],\n",
      "        [0.6800],\n",
      "        [0.6696],\n",
      "        [0.6729],\n",
      "        [0.6665],\n",
      "        [0.5802],\n",
      "        [0.5337],\n",
      "        [0.5347],\n",
      "        [0.5239],\n",
      "        [0.5108],\n",
      "        [0.5153],\n",
      "        [0.5647],\n",
      "        [0.5633],\n",
      "        [0.5631],\n",
      "        [0.5625],\n",
      "        [0.5639],\n",
      "        [0.5646],\n",
      "        [0.5647],\n",
      "        [0.5657],\n",
      "        [0.5830],\n",
      "        [0.5761],\n",
      "        [0.5039],\n",
      "        [0.4942],\n",
      "        [0.4856],\n",
      "        [0.4890],\n",
      "        [0.4915],\n",
      "        [0.4902],\n",
      "        [0.4934],\n",
      "        [0.4863],\n",
      "        [0.4886],\n",
      "        [0.4955],\n",
      "        [0.4985],\n",
      "        [0.4965],\n",
      "        [0.5030],\n",
      "        [0.5197],\n",
      "        [0.5147],\n",
      "        [0.4979],\n",
      "        [0.4078],\n",
      "        [0.4019],\n",
      "        [0.4028],\n",
      "        [0.3943],\n",
      "        [0.3887],\n",
      "        [0.3867],\n",
      "        [0.3846],\n",
      "        [0.3834],\n",
      "        [0.3850],\n",
      "        [0.3839],\n",
      "        [0.3876],\n",
      "        [0.3784],\n",
      "        [0.3804],\n",
      "        [0.3770],\n",
      "        [0.3761],\n",
      "        [0.3794],\n",
      "        [0.4027],\n",
      "        [0.4038],\n",
      "        [0.4003],\n",
      "        [0.4003],\n",
      "        [0.3969],\n",
      "        [0.3935],\n",
      "        [0.3922],\n",
      "        [0.3892],\n",
      "        [0.3873],\n",
      "        [0.3893],\n",
      "        [0.3879],\n",
      "        [0.3872],\n",
      "        [0.3908],\n",
      "        [0.3863],\n",
      "        [0.3980],\n",
      "        [0.4104],\n",
      "        [0.6284],\n",
      "        [0.6316],\n",
      "        [0.6377],\n",
      "        [0.6368],\n",
      "        [0.6449],\n",
      "        [0.6410],\n",
      "        [0.6357],\n",
      "        [0.6291],\n",
      "        [0.6267],\n",
      "        [0.6191],\n",
      "        [0.6130],\n",
      "        [0.6076],\n",
      "        [0.6268],\n",
      "        [0.6311],\n",
      "        [0.6290],\n",
      "        [0.6715],\n",
      "        [0.5585],\n",
      "        [0.5561],\n",
      "        [0.5555],\n",
      "        [0.5551],\n",
      "        [0.5601],\n",
      "        [0.5485],\n",
      "        [0.5461],\n",
      "        [0.5452],\n",
      "        [0.5492],\n",
      "        [0.5512],\n",
      "        [0.5531],\n",
      "        [0.5476],\n",
      "        [0.5553],\n",
      "        [0.5440],\n",
      "        [0.5379],\n",
      "        [0.5259],\n",
      "        [0.4271],\n",
      "        [0.4224],\n",
      "        [0.4233],\n",
      "        [0.4267],\n",
      "        [0.4278],\n",
      "        [0.4321],\n",
      "        [0.4368],\n",
      "        [0.4401],\n",
      "        [0.4355],\n",
      "        [0.4359],\n",
      "        [0.4165],\n",
      "        [0.4118],\n",
      "        [0.4125],\n",
      "        [0.4082],\n",
      "        [0.4113],\n",
      "        [0.4216],\n",
      "        [0.4562],\n",
      "        [0.4718],\n",
      "        [0.4713],\n",
      "        [0.4667],\n",
      "        [0.4613],\n",
      "        [0.4550],\n",
      "        [0.4548],\n",
      "        [0.4610],\n",
      "        [0.4553],\n",
      "        [0.4499],\n",
      "        [0.4577],\n",
      "        [0.4631],\n",
      "        [0.4714],\n",
      "        [0.4827],\n",
      "        [0.4817],\n",
      "        [0.4826],\n",
      "        [0.3920],\n",
      "        [0.4066],\n",
      "        [0.4038],\n",
      "        [0.4022],\n",
      "        [0.4016],\n",
      "        [0.4018],\n",
      "        [0.3985],\n",
      "        [0.3957],\n",
      "        [0.3964],\n",
      "        [0.4021],\n",
      "        [0.4040],\n",
      "        [0.4051],\n",
      "        [0.4087],\n",
      "        [0.4052],\n",
      "        [0.4195],\n",
      "        [0.4007],\n",
      "        [0.5639],\n",
      "        [0.5921],\n",
      "        [0.5863],\n",
      "        [0.5803],\n",
      "        [0.5587],\n",
      "        [0.5585],\n",
      "        [0.5633],\n",
      "        [0.5648],\n",
      "        [0.5708],\n",
      "        [0.5755],\n",
      "        [0.5815],\n",
      "        [0.5819],\n",
      "        [0.5787],\n",
      "        [0.5999],\n",
      "        [0.6226],\n",
      "        [0.6420],\n",
      "        [0.5846],\n",
      "        [0.5298],\n",
      "        [0.5545],\n",
      "        [0.5469],\n",
      "        [0.5747],\n",
      "        [0.5568],\n",
      "        [0.5433],\n",
      "        [0.5178],\n",
      "        [0.5111],\n",
      "        [0.4999],\n",
      "        [0.4979],\n",
      "        [0.5351],\n",
      "        [0.5270],\n",
      "        [0.5537],\n",
      "        [0.5410],\n",
      "        [0.5164],\n",
      "        [0.3711],\n",
      "        [0.3702],\n",
      "        [0.3685],\n",
      "        [0.3863],\n",
      "        [0.3839],\n",
      "        [0.3866],\n",
      "        [0.4070],\n",
      "        [0.3829],\n",
      "        [0.3845],\n",
      "        [0.3856],\n",
      "        [0.3843],\n",
      "        [0.3891],\n",
      "        [0.3953],\n",
      "        [0.3905],\n",
      "        [0.3903],\n",
      "        [0.4059],\n",
      "        [0.3986],\n",
      "        [0.3970],\n",
      "        [0.3923],\n",
      "        [0.3900],\n",
      "        [0.3880],\n",
      "        [0.3866],\n",
      "        [0.4054],\n",
      "        [0.4080],\n",
      "        [0.4090],\n",
      "        [0.4030],\n",
      "        [0.4046],\n",
      "        [0.4045],\n",
      "        [0.4074],\n",
      "        [0.4178],\n",
      "        [0.4249],\n",
      "        [0.4290],\n",
      "        [0.4489],\n",
      "        [0.4482],\n",
      "        [0.4452],\n",
      "        [0.4442],\n",
      "        [0.4430],\n",
      "        [0.4441],\n",
      "        [0.4474],\n",
      "        [0.4563],\n",
      "        [0.4587],\n",
      "        [0.4556],\n",
      "        [0.4587],\n",
      "        [0.4603],\n",
      "        [0.4615],\n",
      "        [0.4695],\n",
      "        [0.4743],\n",
      "        [0.4782],\n",
      "        [0.5403],\n",
      "        [0.5396],\n",
      "        [0.5381],\n",
      "        [0.5380],\n",
      "        [0.5380],\n",
      "        [0.5369],\n",
      "        [0.5331],\n",
      "        [0.5249],\n",
      "        [0.5224],\n",
      "        [0.5227],\n",
      "        [0.5204],\n",
      "        [0.5192],\n",
      "        [0.5170],\n",
      "        [0.5138],\n",
      "        [0.5393],\n",
      "        [0.5373],\n",
      "        [0.6752],\n",
      "        [0.6743],\n",
      "        [0.6784],\n",
      "        [0.6801],\n",
      "        [0.6778],\n",
      "        [0.6782],\n",
      "        [0.6801],\n",
      "        [0.6790],\n",
      "        [0.6713],\n",
      "        [0.6650],\n",
      "        [0.6573],\n",
      "        [0.6585],\n",
      "        [0.6665],\n",
      "        [0.6700],\n",
      "        [0.6649],\n",
      "        [0.6486],\n",
      "        [0.6312],\n",
      "        [0.6392],\n",
      "        [0.6294],\n",
      "        [0.6252],\n",
      "        [0.6289],\n",
      "        [0.6327],\n",
      "        [0.6326],\n",
      "        [0.6317],\n",
      "        [0.6277],\n",
      "        [0.6174],\n",
      "        [0.6156],\n",
      "        [0.6145],\n",
      "        [0.6304],\n",
      "        [0.6270],\n",
      "        [0.6097],\n",
      "        [0.5957],\n",
      "        [0.5553],\n",
      "        [0.5542],\n",
      "        [0.5514],\n",
      "        [0.5511],\n",
      "        [0.5508],\n",
      "        [0.5428],\n",
      "        [0.5478],\n",
      "        [0.5461],\n",
      "        [0.5441],\n",
      "        [0.5467],\n",
      "        [0.5456],\n",
      "        [0.5435],\n",
      "        [0.5444],\n",
      "        [0.5469],\n",
      "        [0.5321],\n",
      "        [0.5185]])\n"
     ]
    }
   ],
   "source": [
    "print(type(actuals), actuals.shape, type(predictions), predictions.shape)\n",
    "print(actuals, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X = [X for X in range(0, actuals.shape[0])]\n",
    "\n",
    "fig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(20,5))\n",
    "\n",
    "ax1.plot(X, actuals, color='b', label=\"Actual\")\n",
    "ax1.plot(X, predictions, color='r', label=\"Predicted\")\n",
    "ax1.set_title(logger_name)\n",
    "\n",
    "files = os.path.join(home_dir, f'TFT_2.png')\n",
    "plt.savefig(files, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfn\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fn' is not defined"
     ]
    }
   ],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xm65lJFvnnLs",
    "outputId": "ffd88daa-bbd8-4b3a-c1f8-e693f3ffece0"
   },
   "outputs": [],
   "source": [
    "max_prediction_length = 3\n",
    "max_encoder_length = 8\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "# print(training_cutoff)\n",
    "# x = data[lambda x: x.time_idx <= training_cutoff]\n",
    "# print()\n",
    "\n",
    "bins_name = list([\"yield\"])\n",
    "for bin in range(0, 512):\n",
    "  bins_name.append(f'bin{bin}')\n",
    "\n",
    "print(bins_name)\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx = \"time_idx\",\n",
    "    target = \"yield\",\n",
    "    group_ids = [\"county\", \"bands\", \"time\"],\n",
    "    min_encoder_length = max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length = max_encoder_length,\n",
    "    min_prediction_length = 1,\n",
    "    max_prediction_length = max_prediction_length,\n",
    "    # static_categoricals = [\"county\"],\n",
    "    # static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],\n",
    "    # time_varying_known_categoricals=[\"special_days\", \"month\"],\n",
    "    # variable_groups={\"special_days\": special_days},  # group of categorical variables can be treated as one variable\n",
    "    # time_varying_known_reals=[\"time_idx\", \"price_regular\", \"discount_in_percent\"],\n",
    "    # time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals = bins_name,\n",
    "    allow_missing_timesteps = True,\n",
    "    # target_normalizer=GroupNormalizer(\n",
    "    #     groups=[\"agency\", \"sku\"], transformation=\"softplus\"\n",
    "    # ),  # use softplus and normalize by group\n",
    "    # add_relative_time_idx = True,\n",
    "    # add_target_scales = True,\n",
    "    # add_encoder_length = True,\n",
    ")\n",
    "\n",
    "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "# for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 1  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train =  True, batch_size = batch_size, num_workers = 0)\n",
    "val_dataloader = validation.to_dataloader(train = False, batch_size = batch_size, num_workers = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gThPXEaensCI",
    "outputId": "45c75a22-e2c8-4538-f11e-eaf1aea74050"
   },
   "outputs": [],
   "source": [
    "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "actuals = torch.cat([y for x, (y, weight) in iter(valid_dataloader)])\n",
    "baseline_predictions = Baseline().predict(valid_dataloader)\n",
    "(actuals - baseline_predictions).abs().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BR2PH1m8opGv"
   },
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "# pl.seed_everything(42)\n",
    "# trainer = pl.Trainer(\n",
    "#     gpus=0,\n",
    "#     # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "#     # of the gradient for recurrent neural networks\n",
    "#     gradient_clip_val=0.1,\n",
    "#     auto_lr_find = True,\n",
    "# )\n",
    "\n",
    "\n",
    "# tft = TemporalFusionTransformer.from_dataset(\n",
    "#     training,\n",
    "#     # not meaningful for finding the learning rate but otherwise very important\n",
    "#     learning_rate=0.03,\n",
    "#     hidden_size=16,  # most important hyperparameter apart from learning rate\n",
    "#     # number of attention heads. Set to up to 4 for large datasets\n",
    "#     attention_head_size=1,\n",
    "#     dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "#     hidden_continuous_size=8,  # set to <= hidden_size\n",
    "#     output_size=1,  # 7 quantiles by default\n",
    "#     loss=QuantileLoss(),\n",
    "#     # reduce learning rate if no improvement in validation loss after x epochs\n",
    "#     reduce_on_plateau_patience=4,\n",
    "# )\n",
    "# print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQexRoPJopqa"
   },
   "outputs": [],
   "source": [
    "# # find optimal learning rate\n",
    "# res = trainer.tuner.lr_find(\n",
    "#     tft,\n",
    "#     train_dataloaders=train_dataloader,\n",
    "#     val_dataloaders=val_dataloader,\n",
    "#     max_lr=10.0,\n",
    "#     min_lr=1e-6,\n",
    "# )\n",
    "\n",
    "# # res = trainer.tuner.lr_find(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader,)\n",
    "\n",
    "# print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "# fig = res.plot(show=True, suggest=True)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x8ohrZoqosSX",
    "outputId": "4f54da2d-924c-4403-93ee-6ce56f78cc4c"
   },
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=300,\n",
    "    gpus=0,\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=1,  # 7 quantiles by default\n",
    "    loss=MAPE(),  #QuantileLoss(), #MAPE(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zcFzxHVrVQ-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorboard as tb \n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 939,
     "referenced_widgets": [
      "14d07476dd484e5e8a7bca7bf59ceae5",
      "c56fd12d72db4e4caf6d5ef0ade8b9f8",
      "fa10f2402f224290815d92ee89213162",
      "daf65e7e84e34266929b680250bc7a46",
      "ba34c1e6d0cb4b03870cb4aef1b6c32c",
      "cdd9d4ddccdc4e06a76aafddce2701f0",
      "ae9c80f166694da09e56af90999a7c6b",
      "5eeae71bc8384c98921f6ead0351b047",
      "74f31ab6b038471ca73833d340ba5606",
      "88d278124f704f90a67d37da11072342",
      "325556a59ba44f10bea30d875b032bcd"
     ]
    },
    "id": "L3O1F_Joo0YG",
    "outputId": "a2851582-3695-4376-9460-bdf095cbcd12"
   },
   "outputs": [],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyhBzIoi5cqP"
   },
   "outputs": [],
   "source": [
    "# load the best model according to the validation loss\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtZA-Mom5gaW"
   },
   "outputs": [],
   "source": [
    "# calcualte mean absolute error on validation set\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "predictions = best_tft.predict(val_dataloader)\n",
    "(actuals - predictions).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXtVdTBs5jZP"
   },
   "outputs": [],
   "source": [
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoYevMoT5wGA"
   },
   "outputs": [],
   "source": [
    "for idx in range(10):  # plot 10 examples\n",
    "    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQT6Kxwl53gu"
   },
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cufHwzV8pLiR"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "# create study\n",
    "study = optimize_hyperparameters(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=200,\n",
    "    max_epochs=50,\n",
    "    gradient_clip_val_range=(0.01, 1.0),\n",
    "    hidden_size_range=(8, 128),\n",
    "    hidden_continuous_size_range=(8, 128),\n",
    "    attention_head_size_range=(1, 4),\n",
    "    learning_rate_range=(0.001, 0.1),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=30),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08YeXDvqo3ZE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "14d07476dd484e5e8a7bca7bf59ceae5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c56fd12d72db4e4caf6d5ef0ade8b9f8",
       "IPY_MODEL_fa10f2402f224290815d92ee89213162",
       "IPY_MODEL_daf65e7e84e34266929b680250bc7a46"
      ],
      "layout": "IPY_MODEL_ba34c1e6d0cb4b03870cb4aef1b6c32c"
     }
    },
    "325556a59ba44f10bea30d875b032bcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5eeae71bc8384c98921f6ead0351b047": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74f31ab6b038471ca73833d340ba5606": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "88d278124f704f90a67d37da11072342": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae9c80f166694da09e56af90999a7c6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba34c1e6d0cb4b03870cb4aef1b6c32c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "c56fd12d72db4e4caf6d5ef0ade8b9f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdd9d4ddccdc4e06a76aafddce2701f0",
      "placeholder": "​",
      "style": "IPY_MODEL_ae9c80f166694da09e56af90999a7c6b",
      "value": "Sanity Checking DataLoader 0:   0%"
     }
    },
    "cdd9d4ddccdc4e06a76aafddce2701f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "daf65e7e84e34266929b680250bc7a46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88d278124f704f90a67d37da11072342",
      "placeholder": "​",
      "style": "IPY_MODEL_325556a59ba44f10bea30d875b032bcd",
      "value": " 0/2 [00:00&lt;?, ?it/s]"
     }
    },
    "fa10f2402f224290815d92ee89213162": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5eeae71bc8384c98921f6ead0351b047",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_74f31ab6b038471ca73833d340ba5606",
      "value": 0
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
