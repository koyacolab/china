{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvpNHOs_nTQK",
    "outputId": "58e5e73c-d790-40a6-a1dc-1f6638a75f55"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive/')\n",
    "\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pDXvdYFuLYSI",
    "outputId": "db9c1dcc-d2d2-450c-bab9-29f775f571d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hy-tmp\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "home_dir = '/content/gdrive/My Drive/AChina' \n",
    "home_dir = '/hy-tmp'\n",
    "os.chdir(home_dir)\n",
    "!pwd\n",
    "\n",
    "!pip install tqdm \n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WPwRIbsMnaq8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "\n",
    "# os.chdir(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYbhRWaxneDl",
    "outputId": "c79dccdb-c661-4f1c-ca13-a14d7bd2c1c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (1.8.1)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scipy) (1.22.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.12.0+cu113)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.8/dist-packages (1.8.5)\n",
      "Requirement already satisfied: pytorch_forecasting in /usr/local/lib/python3.8/dist-packages (0.10.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.64.1)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (0.11.0)\n",
      "Requirement already satisfied: tensorboardX>=2.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2.5.1)\n",
      "Requirement already satisfied: lightning-utilities!=0.4.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (0.4.2)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2022.11.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.22.3)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: optuna<3.0.0,>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (2.10.1)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (1.4.3)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (0.13.5)\n",
      "Requirement already satisfied: scipy<2.0,>=1.8 in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (1.8.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (3.5.2)\n",
      "Requirement already satisfied: scikit-learn<1.2,>=0.24 in /usr/local/lib/python3.8/dist-packages (from pytorch_forecasting) (1.1.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.28.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.3)\n",
      "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch_forecasting) (0.9.0)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.8/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch_forecasting) (6.7.0)\n",
      "Requirement already satisfied: cliff in /usr/local/lib/python3.8/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch_forecasting) (4.1.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch_forecasting) (1.4.45)\n",
      "Requirement already satisfied: alembic in /usr/local/lib/python3.8/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch_forecasting) (1.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas<2.0.0,>=1.3.0->pytorch_forecasting) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas<2.0.0,>=1.3.0->pytorch_forecasting) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<1.2,>=0.24->pytorch_forecasting) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<1.2,>=0.24->pytorch_forecasting) (1.1.0)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=2.2->pytorch-lightning) (3.19.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pytorch_forecasting) (1.4.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pytorch_forecasting) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pytorch_forecasting) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pytorch_forecasting) (4.34.4)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.8/dist-packages (from statsmodels->pytorch_forecasting) (0.5.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from patsy>=0.5.2->statsmodels->pytorch_forecasting) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy>=1.1.0->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (2.0.1)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (5.8.0)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.8/dist-packages (from alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (1.2.4)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (4.12.0)\n",
      "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (0.5.1)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (2.4.2)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (4.1.1)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (3.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.10)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (0.2.5)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (1.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (3.8.1)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from stevedore>=2.0.1->cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (5.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from Mako->alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (2.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install torch torchvision\n",
    "# !pip install pytorch_lightning\n",
    "# !pip install pytorch_forecasting\n",
    "\n",
    "# !pip install torch -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install pytorch-forecasting\n",
    "\n",
    "!pip install scipy\n",
    "!pip install torch pytorch-lightning pytorch_forecasting\n",
    "\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAPE, SMAPE, PoissonLoss, QuantileLoss\n",
    "# from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "65_fJ6MIbncZ",
    "outputId": "06e9c0ab-8d58-480b-86d1-4ca275159e4a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>years</th>\n",
       "      <th>yield</th>\n",
       "      <th>county</th>\n",
       "      <th>bands</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>bin0</th>\n",
       "      <th>bin1</th>\n",
       "      <th>bin2</th>\n",
       "      <th>bin3</th>\n",
       "      <th>...</th>\n",
       "      <th>bin502</th>\n",
       "      <th>bin503</th>\n",
       "      <th>bin504</th>\n",
       "      <th>bin505</th>\n",
       "      <th>bin506</th>\n",
       "      <th>bin507</th>\n",
       "      <th>bin508</th>\n",
       "      <th>bin509</th>\n",
       "      <th>bin510</th>\n",
       "      <th>bin511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2003</td>\n",
       "      <td>0.61295</td>\n",
       "      <td>0</td>\n",
       "      <td>band_5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 518 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  years    yield  county   bands  time_idx  bin0  bin1  bin2  \\\n",
       "0            0   2003  0.61295       0  band_0         0   0.0   0.0   0.0   \n",
       "1            1   2003  0.61295       0  band_1         0   0.0   0.0   0.0   \n",
       "2            2   2003  0.61295       0  band_2         0   0.0   0.0   0.0   \n",
       "3            3   2003  0.61295       0  band_3         0   0.0   0.0   0.0   \n",
       "4            4   2003  0.61295       0  band_4         0   0.0   0.0   0.0   \n",
       "5            5   2003  0.61295       0  band_5         0   0.0   0.0   0.0   \n",
       "6            6   2003  0.61295       0  band_6         0   0.0   0.0   0.0   \n",
       "7            7   2003  0.61295       0  band_7         0   0.0   0.0   0.0   \n",
       "8            8   2003  0.61295       0  band_8         0   0.0   0.0   0.0   \n",
       "9            9   2003  0.61295       0  band_0         1   0.0   0.0   0.0   \n",
       "10          10   2003  0.61295       0  band_1         1   0.0   0.0   0.0   \n",
       "11          11   2003  0.61295       0  band_2         1   0.0   0.0   0.0   \n",
       "12          12   2003  0.61295       0  band_3         1   0.0   0.0   0.0   \n",
       "13          13   2003  0.61295       0  band_4         1   0.0   0.0   0.0   \n",
       "14          14   2003  0.61295       0  band_5         1   0.0   0.0   0.0   \n",
       "\n",
       "    bin3  ...    bin502    bin503    bin504    bin505    bin506    bin507  \\\n",
       "0    0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1    0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2    0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3    0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4    0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5    0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6    0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7    0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8    0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9    0.0  ...  0.001077  0.000833  0.001126  0.000098  0.000784  0.000881   \n",
       "10   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "12   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "13   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "14   0.0  ...  0.000097  0.000484  0.000242  0.000339  0.000048  0.000000   \n",
       "\n",
       "      bin508    bin509    bin510    bin511  \n",
       "0   0.000000  0.000000  0.000000  0.000000  \n",
       "1   0.000000  0.000000  0.000000  0.000000  \n",
       "2   0.000000  0.000000  0.000000  0.000000  \n",
       "3   0.000000  0.000000  0.000000  0.000000  \n",
       "4   0.000000  0.000000  0.000000  0.000000  \n",
       "5   0.000000  0.000000  0.000000  0.000000  \n",
       "6   0.000000  0.000000  0.000000  0.000000  \n",
       "7   0.000000  0.000000  0.000000  0.000000  \n",
       "8   0.000000  0.000000  0.000000  0.000000  \n",
       "9   0.000979  0.000441  0.000294  0.000196  \n",
       "10  0.000000  0.000000  0.000000  0.000000  \n",
       "11  0.000000  0.000000  0.000000  0.000000  \n",
       "12  0.000000  0.000000  0.000000  0.000000  \n",
       "13  0.000000  0.000000  0.000000  0.000000  \n",
       "14  0.000145  0.000194  0.000000  0.000000  \n",
       "\n",
       "[15 rows x 518 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('corn_china_pandas.csv')  # encoding= 'unicode_escape')\n",
    "\n",
    "data[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TOncsJYonfF_"
   },
   "outputs": [],
   "source": [
    "# years = [x for x in range(2003, 2019)]\n",
    "\n",
    "# data.rename(columns={'time_idx' : 'time'}, inplace=True)  \n",
    "\n",
    "# # data[5:15]  \n",
    "# data.insert(1, \"time_idx\", data['years'])  \n",
    "# # df = data.assign(time_dx = data.time * 10)\n",
    "\n",
    "# time_idx = 0\n",
    "# for year in years:\n",
    "#     data['time_idx'] = data['time_idx'].replace([year], time_idx)\n",
    "#     time_idx = time_idx + 1\n",
    "    \n",
    "# data['years'] = data['years'].astype(str)\n",
    "# data['county'] = data['county'].astype(str)\n",
    "# data['time'] = data['time'].astype(str)\n",
    "\n",
    "# dff = data[ data['years'] == '2018' ]\n",
    "# dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "F-uXD_srqwLW",
    "outputId": "8a8a0424-2e67-4dd2-be62-b6b0bf46bfc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "data['years'] = data['years'].astype(str)\n",
    "data['county'] = data['county'].astype(str)\n",
    "data['time_idx'] = data['time_idx'].astype(np.int64)\n",
    "# data.head()\n",
    "print(type(data['bin500'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "LL4gooLRnkdv",
    "outputId": "cb1d012b-c336-4683-d2d9-f454d61c1cb1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>yield</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>bin0</th>\n",
       "      <th>bin1</th>\n",
       "      <th>bin2</th>\n",
       "      <th>bin3</th>\n",
       "      <th>bin4</th>\n",
       "      <th>bin5</th>\n",
       "      <th>bin6</th>\n",
       "      <th>...</th>\n",
       "      <th>bin502</th>\n",
       "      <th>bin503</th>\n",
       "      <th>bin504</th>\n",
       "      <th>bin505</th>\n",
       "      <th>bin506</th>\n",
       "      <th>bin507</th>\n",
       "      <th>bin508</th>\n",
       "      <th>bin509</th>\n",
       "      <th>bin510</th>\n",
       "      <th>bin511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "      <td>91872.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4175.500000</td>\n",
       "      <td>0.517953</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2411.027829</td>\n",
       "      <td>0.106652</td>\n",
       "      <td>9.233143</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.003375</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2087.750000</td>\n",
       "      <td>0.438383</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4175.500000</td>\n",
       "      <td>0.518895</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6263.250000</td>\n",
       "      <td>0.574336</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8351.000000</td>\n",
       "      <td>0.855448</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.025479</td>\n",
       "      <td>0.027569</td>\n",
       "      <td>0.024157</td>\n",
       "      <td>0.028934</td>\n",
       "      <td>0.022859</td>\n",
       "      <td>0.028043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026096</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.024242</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.021858</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.032663</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 515 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0         yield      time_idx          bin0          bin1  \\\n",
       "count  91872.000000  91872.000000  91872.000000  91872.000000  91872.000000   \n",
       "mean    4175.500000      0.517953     15.500000      0.000733      0.000152   \n",
       "std     2411.027829      0.106652      9.233143      0.003606      0.000685   \n",
       "min        0.000000      0.276526      0.000000      0.000000      0.000000   \n",
       "25%     2087.750000      0.438383      7.750000      0.000000      0.000000   \n",
       "50%     4175.500000      0.518895     15.500000      0.000000      0.000000   \n",
       "75%     6263.250000      0.574336     23.250000      0.000059      0.000020   \n",
       "max     8351.000000      0.855448     31.000000      0.111111      0.025479   \n",
       "\n",
       "               bin2          bin3          bin4          bin5          bin6  \\\n",
       "count  91872.000000  91872.000000  91872.000000  91872.000000  91872.000000   \n",
       "mean       0.000182      0.000171      0.000206      0.000194      0.000231   \n",
       "std        0.000838      0.000762      0.000930      0.000845      0.001023   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000023      0.000023      0.000026      0.000026      0.000028   \n",
       "max        0.027569      0.024157      0.028934      0.022859      0.028043   \n",
       "\n",
       "       ...        bin502        bin503        bin504        bin505  \\\n",
       "count  ...  91872.000000  91872.000000  91872.000000  91872.000000   \n",
       "mean   ...      0.000247      0.000256      0.000257      0.000240   \n",
       "std    ...      0.000711      0.000745      0.000783      0.000701   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      0.000148      0.000156      0.000151      0.000142   \n",
       "max    ...      0.026096      0.040541      0.054054      0.024242   \n",
       "\n",
       "             bin506        bin507        bin508        bin509        bin510  \\\n",
       "count  91872.000000  91872.000000  91872.000000  91872.000000  91872.000000   \n",
       "mean       0.000252      0.000254      0.000228      0.000244      0.000231   \n",
       "std        0.003375      0.001853      0.000673      0.000899      0.000685   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000141      0.000144      0.000127      0.000135      0.000129   \n",
       "max        1.000000      0.500000      0.021858      0.157895      0.032663   \n",
       "\n",
       "             bin511  \n",
       "count  91872.000000  \n",
       "mean       0.000254  \n",
       "std        0.000792  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000141  \n",
       "max        0.066667  \n",
       "\n",
       "[8 rows x 515 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities.model_summary import summarize\n",
    "# create the dataset from the pandas dataframe\n",
    "train_data = data[ data[\"years\"] != \"2018\" ]\n",
    "valid_data = data[ data[\"years\"] == \"2018\" ]\n",
    "\n",
    "bins_name = list()   #list([\"yield\"])\n",
    "for bin in range(0, 512):\n",
    "    bins_name.append(f'bin{bin}')\n",
    "\n",
    "# print(bins_name)\n",
    "\n",
    "train_dataset_with_covariates = TimeSeriesDataSet(\n",
    "    train_data,\n",
    "    group_ids=[\"years\", \"county\", \"bands\"],\n",
    "    target=\"yield\",\n",
    "    time_idx=\"time_idx\",\n",
    "    min_encoder_length=16,\n",
    "    max_encoder_length=16,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=1,\n",
    "    time_varying_unknown_reals=bins_name,  #[\"yield\"],\n",
    "    # time_varying_known_reals=[\"real_covariate\"],\n",
    "    time_varying_known_categoricals=[\"years\", \"county\"],\n",
    "    # static_categoricals=[\"years\", \"county\"],\n",
    ")\n",
    "\n",
    "valid_dataset_with_covariates = TimeSeriesDataSet(\n",
    "    valid_data,\n",
    "    group_ids=[\"years\", \"county\", \"bands\"],\n",
    "    target=\"yield\",\n",
    "    time_idx=\"time_idx\",\n",
    "    min_encoder_length=16,\n",
    "    max_encoder_length=16,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=1,\n",
    "    time_varying_unknown_reals=bins_name,   #[\"yield\"],\n",
    "    # time_varying_known_reals=[\"real_covariate\"],\n",
    "    time_varying_known_categoricals=[\"years\", \"county\"],\n",
    "    # static_categoricals=[\"years\", \"county\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.metrics import MAE\n",
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "\n",
    "# model = TemporalFusionTransformer.from_dataset(train_dataset_with_covariates, hidden_size=20, n_hidden_layers=5)\n",
    "\n",
    "model = TemporalFusionTransformer.from_dataset(\n",
    "    train_dataset_with_covariates,\n",
    "    # learning_rate=0.03,\n",
    "    # hidden_size=8,\n",
    "    # attention_head_size=4,\n",
    "    # dropout=0.1,\n",
    "    # hidden_continuous_size=8,\n",
    "    # output_size=1,  # 7 quantiles by default\n",
    "    loss=MAE(),\n",
    "    # log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    # reduce_on_plateau_patience=4,\n",
    ")\n",
    "    \n",
    "# convert datasets to dataloaders for training\n",
    "batch_size = 128\n",
    "train_dataloader = train_dataset_with_covariates.to_dataloader(train=True,  batch_size=batch_size, num_workers=2)\n",
    "valid_dataloader = valid_dataset_with_covariates.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tencoder_cat = torch.Size([128, 16, 2])\n",
      "\tencoder_cont = torch.Size([128, 16, 512])\n",
      "\tencoder_target = torch.Size([128, 16])\n",
      "\tencoder_lengths = torch.Size([128])\n",
      "\tdecoder_cat = torch.Size([128, 1, 2])\n",
      "\tdecoder_cont = torch.Size([128, 1, 512])\n",
      "\tdecoder_target = torch.Size([128, 1])\n",
      "\tdecoder_lengths = torch.Size([128])\n",
      "\tdecoder_time_idx = torch.Size([128, 1])\n",
      "\tgroups = torch.Size([128, 3])\n",
      "\ttarget_scale = torch.Size([128, 2])\n"
     ]
    }
   ],
   "source": [
    "# convert the dataset to a dataloader\n",
    "# dataloader = dataset.to_dataloader(batch_size=4)\n",
    "\n",
    "# and load the first batch\n",
    "x, y = next(iter(valid_dataloader))\n",
    "# print(\"x =\", x)\n",
    "# print(\"\\ny =\", y)\n",
    "# print(\"\\nsizes of x =\")\n",
    "for key, value in x.items():\n",
    "    print(f\"\\t{key} = {value.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /tf_logs/TFT: comment, batch-128\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | MAE                             | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 285   \n",
      "3  | prescalers                         | ModuleDict                      | 8.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 0     \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 356 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 190   \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 676   \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "20 | output_layer                       | Linear                          | 17    \n",
      "----------------------------------------------------------------------------------------\n",
      "371 K     Trainable params\n",
      "0         Non-trainable params\n",
      "371 K     Total params\n",
      "1.484     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  94%|█████████▍| 337/359 [10:05<00:39,  1.80s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 338/359 [10:07<00:37,  1.80s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0:  94%|█████████▍| 339/359 [10:08<00:35,  1.80s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0:  95%|█████████▍| 340/359 [10:09<00:34,  1.79s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0:  95%|█████████▍| 341/359 [10:09<00:32,  1.79s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0:  95%|█████████▌| 342/359 [10:10<00:30,  1.79s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0:  96%|█████████▌| 343/359 [10:11<00:28,  1.78s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0:  96%|█████████▌| 344/359 [10:11<00:26,  1.78s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0:  96%|█████████▌| 345/359 [10:12<00:24,  1.78s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0:  96%|█████████▋| 346/359 [10:13<00:23,  1.77s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0:  97%|█████████▋| 347/359 [10:13<00:21,  1.77s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0:  97%|█████████▋| 348/359 [10:13<00:19,  1.76s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0:  97%|█████████▋| 349/359 [10:14<00:17,  1.76s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0:  97%|█████████▋| 350/359 [10:15<00:15,  1.76s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0:  98%|█████████▊| 351/359 [10:15<00:14,  1.75s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0:  98%|█████████▊| 352/359 [10:16<00:12,  1.75s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0:  98%|█████████▊| 353/359 [10:16<00:10,  1.75s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0:  99%|█████████▊| 354/359 [10:17<00:08,  1.74s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0:  99%|█████████▉| 355/359 [10:17<00:06,  1.74s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0:  99%|█████████▉| 356/359 [10:18<00:05,  1.74s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0:  99%|█████████▉| 357/359 [10:18<00:03,  1.73s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0: 100%|█████████▉| 358/359 [10:19<00:01,  1.73s/it, loss=0.0692, v_num=0, train_loss_step=0.0662]\n",
      "Epoch 0: 100%|██████████| 359/359 [10:19<00:00,  1.73s/it, loss=0.0692, v_num=0, train_loss_step=0.0662, val_loss=0.0548]\n",
      "Epoch 1:  94%|█████████▍| 337/359 [10:23<00:40,  1.85s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  94%|█████████▍| 338/359 [10:25<00:38,  1.85s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1:  94%|█████████▍| 339/359 [10:26<00:36,  1.85s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1:  95%|█████████▍| 340/359 [10:27<00:35,  1.84s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1:  95%|█████████▍| 341/359 [10:27<00:33,  1.84s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1:  95%|█████████▌| 342/359 [10:28<00:31,  1.84s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1:  96%|█████████▌| 343/359 [10:28<00:29,  1.83s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1:  96%|█████████▌| 344/359 [10:29<00:27,  1.83s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1:  96%|█████████▌| 345/359 [10:30<00:25,  1.83s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1:  96%|█████████▋| 346/359 [10:30<00:23,  1.82s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1:  97%|█████████▋| 347/359 [10:31<00:21,  1.82s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1:  97%|█████████▋| 348/359 [10:31<00:19,  1.82s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1:  97%|█████████▋| 349/359 [10:32<00:18,  1.81s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1:  97%|█████████▋| 350/359 [10:33<00:16,  1.81s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1:  98%|█████████▊| 351/359 [10:33<00:14,  1.80s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1:  98%|█████████▊| 352/359 [10:33<00:12,  1.80s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1:  98%|█████████▊| 353/359 [10:34<00:10,  1.80s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1:  99%|█████████▊| 354/359 [10:35<00:08,  1.79s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1:  99%|█████████▉| 355/359 [10:35<00:07,  1.79s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1:  99%|█████████▉| 356/359 [10:36<00:05,  1.79s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1:  99%|█████████▉| 357/359 [10:36<00:03,  1.78s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1: 100%|█████████▉| 358/359 [10:37<00:01,  1.78s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0548, train_loss_epoch=0.0803]\n",
      "Epoch 1: 100%|██████████| 359/359 [10:37<00:00,  1.78s/it, loss=0.054, v_num=0, train_loss_step=0.0479, val_loss=0.0488, train_loss_epoch=0.0803]\n",
      "Epoch 2:  94%|█████████▍| 337/359 [10:30<00:41,  1.87s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  94%|█████████▍| 338/359 [10:32<00:39,  1.87s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2:  94%|█████████▍| 339/359 [10:33<00:37,  1.87s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2:  95%|█████████▍| 340/359 [10:34<00:35,  1.87s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2:  95%|█████████▍| 341/359 [10:34<00:33,  1.86s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2:  95%|█████████▌| 342/359 [10:35<00:31,  1.86s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2:  96%|█████████▌| 343/359 [10:36<00:29,  1.85s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2:  96%|█████████▌| 344/359 [10:36<00:27,  1.85s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2:  96%|█████████▌| 345/359 [10:37<00:25,  1.85s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2:  96%|█████████▋| 346/359 [10:38<00:23,  1.84s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2:  97%|█████████▋| 347/359 [10:38<00:22,  1.84s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2:  97%|█████████▋| 348/359 [10:38<00:20,  1.84s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2:  97%|█████████▋| 349/359 [10:39<00:18,  1.83s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2:  97%|█████████▋| 350/359 [10:40<00:16,  1.83s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2:  98%|█████████▊| 351/359 [10:40<00:14,  1.83s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2:  98%|█████████▊| 352/359 [10:41<00:12,  1.82s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2:  98%|█████████▊| 353/359 [10:41<00:10,  1.82s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2:  99%|█████████▊| 354/359 [10:42<00:09,  1.82s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2:  99%|█████████▉| 355/359 [10:43<00:07,  1.81s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2:  99%|█████████▉| 356/359 [10:43<00:05,  1.81s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2:  99%|█████████▉| 357/359 [10:43<00:03,  1.80s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2: 100%|█████████▉| 358/359 [10:44<00:01,  1.80s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0488, train_loss_epoch=0.061]\n",
      "Epoch 2: 100%|██████████| 359/359 [10:44<00:00,  1.80s/it, loss=0.0392, v_num=0, train_loss_step=0.0348, val_loss=0.0506, train_loss_epoch=0.061]\n",
      "Epoch 3:  94%|█████████▍| 337/359 [10:44<00:42,  1.91s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  94%|█████████▍| 338/359 [10:47<00:40,  1.91s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3:  94%|█████████▍| 339/359 [10:47<00:38,  1.91s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3:  95%|█████████▍| 340/359 [10:48<00:36,  1.91s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3:  95%|█████████▍| 341/359 [10:48<00:34,  1.90s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3:  95%|█████████▌| 342/359 [10:49<00:32,  1.90s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3:  96%|█████████▌| 343/359 [10:49<00:30,  1.89s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3:  96%|█████████▌| 344/359 [10:50<00:28,  1.89s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3:  96%|█████████▌| 345/359 [10:51<00:26,  1.89s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3:  96%|█████████▋| 346/359 [10:51<00:24,  1.88s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3:  97%|█████████▋| 347/359 [10:52<00:22,  1.88s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3:  97%|█████████▋| 348/359 [10:52<00:20,  1.88s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3:  97%|█████████▋| 349/359 [10:53<00:18,  1.87s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3:  97%|█████████▋| 350/359 [10:53<00:16,  1.87s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3:  98%|█████████▊| 351/359 [10:54<00:14,  1.86s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3:  98%|█████████▊| 352/359 [10:55<00:13,  1.86s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3:  98%|█████████▊| 353/359 [10:55<00:11,  1.86s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3:  99%|█████████▊| 354/359 [10:55<00:09,  1.85s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3:  99%|█████████▉| 355/359 [10:56<00:07,  1.85s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3:  99%|█████████▉| 356/359 [10:57<00:05,  1.85s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3:  99%|█████████▉| 357/359 [10:57<00:03,  1.84s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3: 100%|█████████▉| 358/359 [10:58<00:01,  1.84s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0506, train_loss_epoch=0.0446]\n",
      "Epoch 3: 100%|██████████| 359/359 [10:58<00:00,  1.83s/it, loss=0.0331, v_num=0, train_loss_step=0.0302, val_loss=0.0533, train_loss_epoch=0.0446]\n",
      "Epoch 4:  94%|█████████▍| 337/359 [10:36<00:41,  1.89s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  94%|█████████▍| 338/359 [10:38<00:39,  1.89s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4:  94%|█████████▍| 339/359 [10:39<00:37,  1.89s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4:  95%|█████████▍| 340/359 [10:40<00:35,  1.88s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4:  95%|█████████▍| 341/359 [10:40<00:33,  1.88s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4:  95%|█████████▌| 342/359 [10:40<00:31,  1.87s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4:  96%|█████████▌| 343/359 [10:41<00:29,  1.87s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4:  96%|█████████▌| 344/359 [10:42<00:27,  1.87s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4:  96%|█████████▌| 345/359 [10:42<00:26,  1.86s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4:  96%|█████████▋| 346/359 [10:43<00:24,  1.86s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4:  97%|█████████▋| 347/359 [10:43<00:22,  1.85s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4:  97%|█████████▋| 348/359 [10:44<00:20,  1.85s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4:  97%|█████████▋| 349/359 [10:44<00:18,  1.85s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4:  97%|█████████▋| 350/359 [10:45<00:16,  1.84s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4:  98%|█████████▊| 351/359 [10:45<00:14,  1.84s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4:  98%|█████████▊| 352/359 [10:46<00:12,  1.84s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4:  98%|█████████▊| 353/359 [10:47<00:11,  1.83s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4:  99%|█████████▊| 354/359 [10:47<00:09,  1.83s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4:  99%|█████████▉| 355/359 [10:48<00:07,  1.83s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4:  99%|█████████▉| 356/359 [10:48<00:05,  1.82s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4:  99%|█████████▉| 357/359 [10:49<00:03,  1.82s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4: 100%|█████████▉| 358/359 [10:49<00:01,  1.82s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.0533, train_loss_epoch=0.036]\n",
      "Epoch 4: 100%|██████████| 359/359 [10:50<00:00,  1.81s/it, loss=0.0281, v_num=0, train_loss_step=0.0298, val_loss=0.055, train_loss_epoch=0.036] \n",
      "Epoch 5:  94%|█████████▍| 337/359 [10:31<00:41,  1.87s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  94%|█████████▍| 338/359 [10:33<00:39,  1.88s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5:  94%|█████████▍| 339/359 [10:34<00:37,  1.87s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5:  95%|█████████▍| 340/359 [10:35<00:35,  1.87s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5:  95%|█████████▍| 341/359 [10:35<00:33,  1.86s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5:  95%|█████████▌| 342/359 [10:36<00:31,  1.86s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5:  96%|█████████▌| 343/359 [10:37<00:29,  1.86s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5:  96%|█████████▌| 344/359 [10:37<00:27,  1.85s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5:  96%|█████████▌| 345/359 [10:38<00:25,  1.85s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5:  96%|█████████▋| 346/359 [10:38<00:24,  1.85s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5:  97%|█████████▋| 347/359 [10:39<00:22,  1.84s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5:  97%|█████████▋| 348/359 [10:39<00:20,  1.84s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5:  97%|█████████▋| 349/359 [10:40<00:18,  1.84s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5:  97%|█████████▋| 350/359 [10:40<00:16,  1.83s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5:  98%|█████████▊| 351/359 [10:41<00:14,  1.83s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5:  98%|█████████▊| 352/359 [10:42<00:12,  1.82s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5:  98%|█████████▊| 353/359 [10:42<00:10,  1.82s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5:  99%|█████████▊| 354/359 [10:43<00:09,  1.82s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5:  99%|█████████▉| 355/359 [10:43<00:07,  1.81s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5:  99%|█████████▉| 356/359 [10:44<00:05,  1.81s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5:  99%|█████████▉| 357/359 [10:44<00:03,  1.81s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5: 100%|█████████▉| 358/359 [10:45<00:01,  1.80s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.055, train_loss_epoch=0.0307]\n",
      "Epoch 5: 100%|██████████| 359/359 [10:45<00:00,  1.80s/it, loss=0.0243, v_num=0, train_loss_step=0.0206, val_loss=0.0568, train_loss_epoch=0.0307]\n",
      "Epoch 6:  94%|█████████▍| 337/359 [10:44<00:42,  1.91s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  94%|█████████▍| 338/359 [10:46<00:40,  1.91s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6:  94%|█████████▍| 339/359 [10:47<00:38,  1.91s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6:  95%|█████████▍| 340/359 [10:48<00:36,  1.91s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6:  95%|█████████▍| 341/359 [10:48<00:34,  1.90s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6:  95%|█████████▌| 342/359 [10:49<00:32,  1.90s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6:  96%|█████████▌| 343/359 [10:49<00:30,  1.89s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6:  96%|█████████▌| 344/359 [10:50<00:28,  1.89s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6:  96%|█████████▌| 345/359 [10:50<00:26,  1.89s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6:  96%|█████████▋| 346/359 [10:51<00:24,  1.88s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6:  97%|█████████▋| 347/359 [10:52<00:22,  1.88s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6:  97%|█████████▋| 348/359 [10:52<00:20,  1.88s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6:  97%|█████████▋| 349/359 [10:52<00:18,  1.87s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6:  97%|█████████▋| 350/359 [10:53<00:16,  1.87s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6:  98%|█████████▊| 351/359 [10:54<00:14,  1.86s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6:  98%|█████████▊| 352/359 [10:54<00:13,  1.86s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6:  98%|█████████▊| 353/359 [10:55<00:11,  1.86s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6:  99%|█████████▊| 354/359 [10:55<00:09,  1.85s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6:  99%|█████████▉| 355/359 [10:56<00:07,  1.85s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6:  99%|█████████▉| 356/359 [10:56<00:05,  1.85s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6:  99%|█████████▉| 357/359 [10:57<00:03,  1.84s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6: 100%|█████████▉| 358/359 [10:57<00:01,  1.84s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.0568, train_loss_epoch=0.0264]\n",
      "Epoch 6: 100%|██████████| 359/359 [10:58<00:00,  1.83s/it, loss=0.0217, v_num=0, train_loss_step=0.0193, val_loss=0.063, train_loss_epoch=0.0264] \n",
      "Epoch 7:  94%|█████████▍| 337/359 [10:38<00:41,  1.89s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  94%|█████████▍| 338/359 [10:40<00:39,  1.89s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7:  94%|█████████▍| 339/359 [10:40<00:37,  1.89s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7:  95%|█████████▍| 340/359 [10:41<00:35,  1.89s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7:  95%|█████████▍| 341/359 [10:42<00:33,  1.88s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7:  95%|█████████▌| 342/359 [10:43<00:31,  1.88s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7:  96%|█████████▌| 343/359 [10:43<00:30,  1.88s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7:  96%|█████████▌| 344/359 [10:43<00:28,  1.87s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7:  96%|█████████▌| 345/359 [10:44<00:26,  1.87s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7:  96%|█████████▋| 346/359 [10:45<00:24,  1.86s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7:  97%|█████████▋| 347/359 [10:45<00:22,  1.86s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7:  97%|█████████▋| 348/359 [10:46<00:20,  1.86s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7:  97%|█████████▋| 349/359 [10:46<00:18,  1.85s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7:  97%|█████████▋| 350/359 [10:47<00:16,  1.85s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7:  98%|█████████▊| 351/359 [10:47<00:14,  1.85s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7:  98%|█████████▊| 352/359 [10:48<00:12,  1.84s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7:  98%|█████████▊| 353/359 [10:49<00:11,  1.84s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7:  99%|█████████▊| 354/359 [10:49<00:09,  1.84s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7:  99%|█████████▉| 355/359 [10:50<00:07,  1.83s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7:  99%|█████████▉| 356/359 [10:50<00:05,  1.83s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7:  99%|█████████▉| 357/359 [10:51<00:03,  1.82s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7: 100%|█████████▉| 358/359 [10:51<00:01,  1.82s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.063, train_loss_epoch=0.0233]\n",
      "Epoch 7: 100%|██████████| 359/359 [10:52<00:00,  1.82s/it, loss=0.0197, v_num=0, train_loss_step=0.0197, val_loss=0.0663, train_loss_epoch=0.0233]\n",
      "Epoch 8:  94%|█████████▍| 337/359 [10:49<00:42,  1.93s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  94%|█████████▍| 338/359 [10:51<00:40,  1.93s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8:  94%|█████████▍| 339/359 [10:51<00:38,  1.92s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8:  95%|█████████▍| 340/359 [10:52<00:36,  1.92s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8:  95%|█████████▍| 341/359 [10:53<00:34,  1.92s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8:  95%|█████████▌| 342/359 [10:53<00:32,  1.91s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8:  96%|█████████▌| 343/359 [10:54<00:30,  1.91s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8:  96%|█████████▌| 344/359 [10:54<00:28,  1.90s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8:  96%|█████████▌| 345/359 [10:55<00:26,  1.90s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8:  96%|█████████▋| 346/359 [10:55<00:24,  1.90s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8:  97%|█████████▋| 347/359 [10:56<00:22,  1.89s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8:  97%|█████████▋| 348/359 [10:57<00:20,  1.89s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8:  97%|█████████▋| 349/359 [10:57<00:18,  1.88s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8:  97%|█████████▋| 350/359 [10:58<00:16,  1.88s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8:  98%|█████████▊| 351/359 [10:58<00:15,  1.88s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8:  98%|█████████▊| 352/359 [10:59<00:13,  1.87s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8:  98%|█████████▊| 353/359 [10:59<00:11,  1.87s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8:  99%|█████████▊| 354/359 [11:00<00:09,  1.87s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8:  99%|█████████▉| 355/359 [11:01<00:07,  1.86s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8:  99%|█████████▉| 356/359 [11:01<00:05,  1.86s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8:  99%|█████████▉| 357/359 [11:01<00:03,  1.85s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8: 100%|█████████▉| 358/359 [11:02<00:01,  1.85s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0663, train_loss_epoch=0.0208]\n",
      "Epoch 8: 100%|██████████| 359/359 [11:03<00:00,  1.85s/it, loss=0.0177, v_num=0, train_loss_step=0.017, val_loss=0.0708, train_loss_epoch=0.0208]\n",
      "Epoch 9:  94%|█████████▍| 337/359 [11:34<00:45,  2.06s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  94%|█████████▍| 338/359 [11:37<00:43,  2.06s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9:  94%|█████████▍| 339/359 [11:37<00:41,  2.06s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9:  95%|█████████▍| 340/359 [11:38<00:39,  2.05s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9:  95%|█████████▍| 341/359 [11:38<00:36,  2.05s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9:  95%|█████████▌| 342/359 [11:39<00:34,  2.05s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9:  96%|█████████▌| 343/359 [11:40<00:32,  2.04s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9:  96%|█████████▌| 344/359 [11:40<00:30,  2.04s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9:  96%|█████████▌| 345/359 [11:41<00:28,  2.03s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9:  96%|█████████▋| 346/359 [11:42<00:26,  2.03s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9:  97%|█████████▋| 347/359 [11:42<00:24,  2.03s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9:  97%|█████████▋| 348/359 [11:43<00:22,  2.02s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9:  97%|█████████▋| 349/359 [11:43<00:20,  2.02s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9:  97%|█████████▋| 350/359 [11:44<00:18,  2.01s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9:  98%|█████████▊| 351/359 [11:44<00:16,  2.01s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9:  98%|█████████▊| 352/359 [11:45<00:14,  2.00s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9:  98%|█████████▊| 353/359 [11:45<00:11,  2.00s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9:  99%|█████████▊| 354/359 [11:46<00:09,  2.00s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9:  99%|█████████▉| 355/359 [11:47<00:07,  1.99s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9:  99%|█████████▉| 356/359 [11:47<00:05,  1.99s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9:  99%|█████████▉| 357/359 [11:47<00:03,  1.98s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9: 100%|█████████▉| 358/359 [11:48<00:01,  1.98s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0708, train_loss_epoch=0.0188]\n",
      "Epoch 9: 100%|██████████| 359/359 [11:49<00:00,  1.97s/it, loss=0.0163, v_num=0, train_loss_step=0.0148, val_loss=0.0809, train_loss_epoch=0.0188]\n",
      "Epoch 10:  94%|█████████▍| 337/359 [11:10<00:43,  1.99s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  94%|█████████▍| 338/359 [11:12<00:41,  1.99s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10:  94%|█████████▍| 339/359 [11:13<00:39,  1.99s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10:  95%|█████████▍| 340/359 [11:14<00:37,  1.98s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10:  95%|█████████▍| 341/359 [11:14<00:35,  1.98s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10:  95%|█████████▌| 342/359 [11:15<00:33,  1.97s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10:  96%|█████████▌| 343/359 [11:15<00:31,  1.97s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10:  96%|█████████▌| 344/359 [11:16<00:29,  1.97s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10:  96%|█████████▌| 345/359 [11:17<00:27,  1.96s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10:  96%|█████████▋| 346/359 [11:17<00:25,  1.96s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10:  97%|█████████▋| 347/359 [11:18<00:23,  1.95s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10:  97%|█████████▋| 348/359 [11:19<00:21,  1.95s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10:  97%|█████████▋| 349/359 [11:19<00:19,  1.95s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10:  97%|█████████▋| 350/359 [11:19<00:17,  1.94s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10:  98%|█████████▊| 351/359 [11:20<00:15,  1.94s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10:  98%|█████████▊| 352/359 [11:21<00:13,  1.94s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10:  98%|█████████▊| 353/359 [11:21<00:11,  1.93s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10:  99%|█████████▊| 354/359 [11:22<00:09,  1.93s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10:  99%|█████████▉| 355/359 [11:22<00:07,  1.92s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10:  99%|█████████▉| 356/359 [11:23<00:05,  1.92s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10:  99%|█████████▉| 357/359 [11:23<00:03,  1.92s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10: 100%|█████████▉| 358/359 [11:24<00:01,  1.91s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0809, train_loss_epoch=0.017]\n",
      "Epoch 10: 100%|██████████| 359/359 [11:24<00:00,  1.91s/it, loss=0.015, v_num=0, train_loss_step=0.0164, val_loss=0.0853, train_loss_epoch=0.017]\n",
      "Epoch 11:  94%|█████████▍| 337/359 [10:38<00:41,  1.89s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  94%|█████████▍| 338/359 [10:40<00:39,  1.90s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11:  94%|█████████▍| 339/359 [10:41<00:37,  1.89s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11:  95%|█████████▍| 340/359 [10:41<00:35,  1.89s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11:  95%|█████████▍| 341/359 [10:42<00:33,  1.88s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11:  95%|█████████▌| 342/359 [10:43<00:31,  1.88s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11:  96%|█████████▌| 343/359 [10:43<00:30,  1.88s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11:  96%|█████████▌| 344/359 [10:44<00:28,  1.87s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11:  96%|█████████▌| 345/359 [10:44<00:26,  1.87s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11:  96%|█████████▋| 346/359 [10:45<00:24,  1.87s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11:  97%|█████████▋| 347/359 [10:45<00:22,  1.86s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11:  97%|█████████▋| 348/359 [10:46<00:20,  1.86s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11:  97%|█████████▋| 349/359 [10:47<00:18,  1.85s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11:  97%|█████████▋| 350/359 [10:47<00:16,  1.85s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11:  98%|█████████▊| 351/359 [10:48<00:14,  1.85s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11:  98%|█████████▊| 352/359 [10:48<00:12,  1.84s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11:  98%|█████████▊| 353/359 [10:49<00:11,  1.84s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11:  99%|█████████▊| 354/359 [10:50<00:09,  1.84s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11:  99%|█████████▉| 355/359 [10:50<00:07,  1.83s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11:  99%|█████████▉| 356/359 [10:50<00:05,  1.83s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11:  99%|█████████▉| 357/359 [10:51<00:03,  1.83s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11: 100%|█████████▉| 358/359 [10:52<00:01,  1.82s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0853, train_loss_epoch=0.0157]\n",
      "Epoch 11: 100%|██████████| 359/359 [10:52<00:00,  1.82s/it, loss=0.0141, v_num=0, train_loss_step=0.0163, val_loss=0.0895, train_loss_epoch=0.0157]\n",
      "Epoch 12:  94%|█████████▍| 337/359 [10:38<00:41,  1.90s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  94%|█████████▍| 338/359 [10:41<00:39,  1.90s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12:  94%|█████████▍| 339/359 [10:41<00:37,  1.89s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12:  95%|█████████▍| 340/359 [10:42<00:35,  1.89s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12:  95%|█████████▍| 341/359 [10:43<00:33,  1.89s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12:  95%|█████████▌| 342/359 [10:43<00:31,  1.88s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12:  96%|█████████▌| 343/359 [10:43<00:30,  1.88s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12:  96%|█████████▌| 344/359 [10:44<00:28,  1.87s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12:  96%|█████████▌| 345/359 [10:45<00:26,  1.87s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12:  96%|█████████▋| 346/359 [10:45<00:24,  1.87s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12:  97%|█████████▋| 347/359 [10:45<00:22,  1.86s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12:  97%|█████████▋| 348/359 [10:46<00:20,  1.86s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12:  97%|█████████▋| 349/359 [10:47<00:18,  1.85s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12:  97%|█████████▋| 350/359 [10:47<00:16,  1.85s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12:  98%|█████████▊| 351/359 [10:48<00:14,  1.85s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12:  98%|█████████▊| 352/359 [10:48<00:12,  1.84s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12:  98%|█████████▊| 353/359 [10:49<00:11,  1.84s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12:  99%|█████████▊| 354/359 [10:49<00:09,  1.84s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12:  99%|█████████▉| 355/359 [10:50<00:07,  1.83s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12:  99%|█████████▉| 356/359 [10:51<00:05,  1.83s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12:  99%|█████████▉| 357/359 [10:51<00:03,  1.83s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12: 100%|█████████▉| 358/359 [10:52<00:01,  1.82s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0895, train_loss_epoch=0.0143]\n",
      "Epoch 12: 100%|██████████| 359/359 [10:52<00:00,  1.82s/it, loss=0.0124, v_num=0, train_loss_step=0.0123, val_loss=0.0862, train_loss_epoch=0.0143]\n",
      "Epoch 13:  94%|█████████▍| 337/359 [10:46<00:42,  1.92s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  94%|█████████▍| 338/359 [10:48<00:40,  1.92s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13:  94%|█████████▍| 339/359 [10:49<00:38,  1.91s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13:  95%|█████████▍| 340/359 [10:49<00:36,  1.91s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13:  95%|█████████▍| 341/359 [10:50<00:34,  1.91s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13:  95%|█████████▌| 342/359 [10:50<00:32,  1.90s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13:  96%|█████████▌| 343/359 [10:51<00:30,  1.90s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13:  96%|█████████▌| 344/359 [10:51<00:28,  1.89s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13:  96%|█████████▌| 345/359 [10:52<00:26,  1.89s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13:  96%|█████████▋| 346/359 [10:52<00:24,  1.89s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13:  97%|█████████▋| 347/359 [10:53<00:22,  1.88s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13:  97%|█████████▋| 348/359 [10:53<00:20,  1.88s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13:  97%|█████████▋| 349/359 [10:54<00:18,  1.87s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13:  97%|█████████▋| 350/359 [10:55<00:16,  1.87s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13:  98%|█████████▊| 351/359 [10:55<00:14,  1.87s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13:  98%|█████████▊| 352/359 [10:56<00:13,  1.86s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13:  98%|█████████▊| 353/359 [10:56<00:11,  1.86s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13:  99%|█████████▊| 354/359 [10:57<00:09,  1.86s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13:  99%|█████████▉| 355/359 [10:58<00:07,  1.85s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13:  99%|█████████▉| 356/359 [10:58<00:05,  1.85s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13:  99%|█████████▉| 357/359 [10:58<00:03,  1.85s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13: 100%|█████████▉| 358/359 [10:59<00:01,  1.84s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0862, train_loss_epoch=0.013]\n",
      "Epoch 13: 100%|██████████| 359/359 [10:59<00:00,  1.84s/it, loss=0.0119, v_num=0, train_loss_step=0.0143, val_loss=0.0907, train_loss_epoch=0.013]\n",
      "Epoch 14:  94%|█████████▍| 337/359 [10:40<00:41,  1.90s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  94%|█████████▍| 338/359 [10:42<00:39,  1.90s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14:  94%|█████████▍| 339/359 [10:43<00:37,  1.90s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14:  95%|█████████▍| 340/359 [10:43<00:35,  1.89s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14:  95%|█████████▍| 341/359 [10:44<00:34,  1.89s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14:  95%|█████████▌| 342/359 [10:45<00:32,  1.89s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14:  96%|█████████▌| 343/359 [10:45<00:30,  1.88s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14:  96%|█████████▌| 344/359 [10:46<00:28,  1.88s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14:  96%|█████████▌| 345/359 [10:47<00:26,  1.88s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14:  96%|█████████▋| 346/359 [10:47<00:24,  1.87s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14:  97%|█████████▋| 347/359 [10:48<00:22,  1.87s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14:  97%|█████████▋| 348/359 [10:48<00:20,  1.86s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14:  97%|█████████▋| 349/359 [10:49<00:18,  1.86s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14:  97%|█████████▋| 350/359 [10:49<00:16,  1.86s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14:  98%|█████████▊| 351/359 [10:50<00:14,  1.85s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14:  98%|█████████▊| 352/359 [10:51<00:12,  1.85s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14:  98%|█████████▊| 353/359 [10:51<00:11,  1.85s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14:  99%|█████████▊| 354/359 [10:51<00:09,  1.84s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14:  99%|█████████▉| 355/359 [10:52<00:07,  1.84s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14:  99%|█████████▉| 356/359 [10:53<00:05,  1.83s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14:  99%|█████████▉| 357/359 [10:53<00:03,  1.83s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14: 100%|█████████▉| 358/359 [10:54<00:01,  1.83s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0907, train_loss_epoch=0.012]\n",
      "Epoch 14: 100%|██████████| 359/359 [10:54<00:00,  1.82s/it, loss=0.0111, v_num=0, train_loss_step=0.0094, val_loss=0.0901, train_loss_epoch=0.012]\n",
      "Epoch 15:  94%|█████████▍| 337/359 [10:57<00:42,  1.95s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  94%|█████████▍| 338/359 [10:59<00:40,  1.95s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15:  94%|█████████▍| 339/359 [11:00<00:38,  1.95s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15:  95%|█████████▍| 340/359 [11:01<00:36,  1.95s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15:  95%|█████████▍| 341/359 [11:02<00:34,  1.94s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15:  95%|█████████▌| 342/359 [11:03<00:32,  1.94s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15:  96%|█████████▌| 343/359 [11:03<00:30,  1.94s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15:  96%|█████████▌| 344/359 [11:04<00:28,  1.93s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15:  96%|█████████▌| 345/359 [11:05<00:26,  1.93s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15:  96%|█████████▋| 346/359 [11:05<00:25,  1.92s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15:  97%|█████████▋| 347/359 [11:06<00:23,  1.92s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15:  97%|█████████▋| 348/359 [11:07<00:21,  1.92s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15:  97%|█████████▋| 349/359 [11:07<00:19,  1.91s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15:  97%|█████████▋| 350/359 [11:08<00:17,  1.91s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15:  98%|█████████▊| 351/359 [11:08<00:15,  1.91s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15:  98%|█████████▊| 352/359 [11:09<00:13,  1.90s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15:  98%|█████████▊| 353/359 [11:10<00:11,  1.90s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15:  99%|█████████▊| 354/359 [11:10<00:09,  1.89s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15:  99%|█████████▉| 355/359 [11:11<00:07,  1.89s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15:  99%|█████████▉| 356/359 [11:12<00:05,  1.89s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15:  99%|█████████▉| 357/359 [11:12<00:03,  1.88s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15: 100%|█████████▉| 358/359 [11:13<00:01,  1.88s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0901, train_loss_epoch=0.0112]\n",
      "Epoch 15: 100%|██████████| 359/359 [11:13<00:00,  1.88s/it, loss=0.0103, v_num=0, train_loss_step=0.0106, val_loss=0.0891, train_loss_epoch=0.0112]\n",
      "Epoch 16:  94%|█████████▍| 337/359 [11:17<00:44,  2.01s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  94%|█████████▍| 338/359 [11:19<00:42,  2.01s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16:  94%|█████████▍| 339/359 [11:19<00:40,  2.00s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16:  95%|█████████▍| 340/359 [11:20<00:38,  2.00s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16:  95%|█████████▍| 341/359 [11:21<00:35,  2.00s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16:  95%|█████████▌| 342/359 [11:21<00:33,  1.99s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16:  96%|█████████▌| 343/359 [11:22<00:31,  1.99s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16:  96%|█████████▌| 344/359 [11:22<00:29,  1.98s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16:  96%|█████████▌| 345/359 [11:23<00:27,  1.98s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16:  96%|█████████▋| 346/359 [11:23<00:25,  1.98s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16:  97%|█████████▋| 347/359 [11:24<00:23,  1.97s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16:  97%|█████████▋| 348/359 [11:24<00:21,  1.97s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16:  97%|█████████▋| 349/359 [11:24<00:19,  1.96s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16:  97%|█████████▋| 350/359 [11:25<00:17,  1.96s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16:  98%|█████████▊| 351/359 [11:25<00:15,  1.95s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16:  98%|█████████▊| 352/359 [11:26<00:13,  1.95s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16:  98%|█████████▊| 353/359 [11:26<00:11,  1.95s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16:  99%|█████████▊| 354/359 [11:27<00:09,  1.94s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16:  99%|█████████▉| 355/359 [11:28<00:07,  1.94s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16:  99%|█████████▉| 356/359 [11:28<00:05,  1.93s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16:  99%|█████████▉| 357/359 [11:29<00:03,  1.93s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16: 100%|█████████▉| 358/359 [11:30<00:01,  1.93s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0891, train_loss_epoch=0.0105]\n",
      "Epoch 16: 100%|██████████| 359/359 [11:30<00:00,  1.92s/it, loss=0.00915, v_num=0, train_loss_step=0.00963, val_loss=0.0884, train_loss_epoch=0.0105]\n",
      "Epoch 17:  94%|█████████▍| 337/359 [10:52<00:42,  1.94s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  94%|█████████▍| 338/359 [10:54<00:40,  1.94s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17:  94%|█████████▍| 339/359 [10:55<00:38,  1.93s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17:  95%|█████████▍| 340/359 [10:56<00:36,  1.93s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17:  95%|█████████▍| 341/359 [10:56<00:34,  1.93s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17:  95%|█████████▌| 342/359 [10:57<00:32,  1.92s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17:  96%|█████████▌| 343/359 [10:57<00:30,  1.92s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17:  96%|█████████▌| 344/359 [10:58<00:28,  1.91s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17:  96%|█████████▌| 345/359 [10:59<00:26,  1.91s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17:  96%|█████████▋| 346/359 [10:59<00:24,  1.91s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17:  97%|█████████▋| 347/359 [11:00<00:22,  1.90s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17:  97%|█████████▋| 348/359 [11:00<00:20,  1.90s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17:  97%|█████████▋| 349/359 [11:01<00:18,  1.90s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17:  97%|█████████▋| 350/359 [11:02<00:17,  1.89s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17:  98%|█████████▊| 351/359 [11:02<00:15,  1.89s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17:  98%|█████████▊| 352/359 [11:03<00:13,  1.88s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17:  98%|█████████▊| 353/359 [11:03<00:11,  1.88s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17:  99%|█████████▊| 354/359 [11:04<00:09,  1.88s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17:  99%|█████████▉| 355/359 [11:04<00:07,  1.87s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17:  99%|█████████▉| 356/359 [11:05<00:05,  1.87s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17:  99%|█████████▉| 357/359 [11:05<00:03,  1.86s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17: 100%|█████████▉| 358/359 [11:05<00:01,  1.86s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0884, train_loss_epoch=0.00973]\n",
      "Epoch 17: 100%|██████████| 359/359 [11:06<00:00,  1.86s/it, loss=0.00901, v_num=0, train_loss_step=0.00851, val_loss=0.0875, train_loss_epoch=0.00973]\n",
      "Epoch 18:  85%|████████▍ | 304/359 [09:57<01:48,  1.97s/it, loss=0.00833, v_num=0, train_loss_step=0.00798, val_loss=0.0875, train_loss_epoch=0.00908]"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger('/tf_logs', name=\"TFT: comment, batch-128\")\n",
    "\n",
    "# trainer = Trainer(gpus=1, max_epochs=100, limit_train_batches=2606, logger=logger)\n",
    "trainer = Trainer(accelerator='gpu', devices=1, logger=logger)\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader)\n",
    "# trainer.validate(model=model, dataloaders=valid_dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model according to the validation loss\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "# best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "# best_tft = FullyConnectedModelWithCovariates.load_from_checkpoint(best_model_path)\n",
    "trainer.save_checkpoint(\"tft_best_model2.ckpt\")\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(\"tft_best_model2.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0883)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calcualte mean absolute error on validation set\n",
    "actuals = torch.cat([y[0] for x, y in iter(valid_dataloader)])\n",
    "predictions = best_tft.predict(valid_dataloader)\n",
    "(actuals - predictions).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([2736, 1]) <class 'torch.Tensor'> torch.Size([2736, 1])\n",
      "tensor([[0.8284],\n",
      "        [0.8284],\n",
      "        [0.8284],\n",
      "        ...,\n",
      "        [0.5816],\n",
      "        [0.5816],\n",
      "        [0.5816]]) tensor([[0.6622],\n",
      "        [0.6617],\n",
      "        [0.6604],\n",
      "        ...,\n",
      "        [0.7140],\n",
      "        [0.7148],\n",
      "        [0.7146]])\n"
     ]
    }
   ],
   "source": [
    "print(type(actuals), actuals.shape, type(predictions), predictions.shape)\n",
    "print(actuals, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X = [X for X in range(0, actuals.shape[0])]\n",
    "\n",
    "plt.plot(X, actuals, 'b')\n",
    "plt.plot(X, predictions, 'r')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xm65lJFvnnLs",
    "outputId": "ffd88daa-bbd8-4b3a-c1f8-e693f3ffece0"
   },
   "outputs": [],
   "source": [
    "max_prediction_length = 3\n",
    "max_encoder_length = 8\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "# print(training_cutoff)\n",
    "# x = data[lambda x: x.time_idx <= training_cutoff]\n",
    "# print()\n",
    "\n",
    "bins_name = list([\"yield\"])\n",
    "for bin in range(0, 512):\n",
    "  bins_name.append(f'bin{bin}')\n",
    "\n",
    "print(bins_name)\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx = \"time_idx\",\n",
    "    target = \"yield\",\n",
    "    group_ids = [\"county\", \"bands\", \"time\"],\n",
    "    min_encoder_length = max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length = max_encoder_length,\n",
    "    min_prediction_length = 1,\n",
    "    max_prediction_length = max_prediction_length,\n",
    "    # static_categoricals = [\"county\"],\n",
    "    # static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],\n",
    "    # time_varying_known_categoricals=[\"special_days\", \"month\"],\n",
    "    # variable_groups={\"special_days\": special_days},  # group of categorical variables can be treated as one variable\n",
    "    # time_varying_known_reals=[\"time_idx\", \"price_regular\", \"discount_in_percent\"],\n",
    "    # time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals = bins_name,\n",
    "    allow_missing_timesteps = True,\n",
    "    # target_normalizer=GroupNormalizer(\n",
    "    #     groups=[\"agency\", \"sku\"], transformation=\"softplus\"\n",
    "    # ),  # use softplus and normalize by group\n",
    "    # add_relative_time_idx = True,\n",
    "    # add_target_scales = True,\n",
    "    # add_encoder_length = True,\n",
    ")\n",
    "\n",
    "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "# for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 1  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train =  True, batch_size = batch_size, num_workers = 0)\n",
    "val_dataloader = validation.to_dataloader(train = False, batch_size = batch_size, num_workers = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gThPXEaensCI",
    "outputId": "45c75a22-e2c8-4538-f11e-eaf1aea74050"
   },
   "outputs": [],
   "source": [
    "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "actuals = torch.cat([y for x, (y, weight) in iter(valid_dataloader)])\n",
    "baseline_predictions = Baseline().predict(valid_dataloader)\n",
    "(actuals - baseline_predictions).abs().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BR2PH1m8opGv"
   },
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "# pl.seed_everything(42)\n",
    "# trainer = pl.Trainer(\n",
    "#     gpus=0,\n",
    "#     # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "#     # of the gradient for recurrent neural networks\n",
    "#     gradient_clip_val=0.1,\n",
    "#     auto_lr_find = True,\n",
    "# )\n",
    "\n",
    "\n",
    "# tft = TemporalFusionTransformer.from_dataset(\n",
    "#     training,\n",
    "#     # not meaningful for finding the learning rate but otherwise very important\n",
    "#     learning_rate=0.03,\n",
    "#     hidden_size=16,  # most important hyperparameter apart from learning rate\n",
    "#     # number of attention heads. Set to up to 4 for large datasets\n",
    "#     attention_head_size=1,\n",
    "#     dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "#     hidden_continuous_size=8,  # set to <= hidden_size\n",
    "#     output_size=1,  # 7 quantiles by default\n",
    "#     loss=QuantileLoss(),\n",
    "#     # reduce learning rate if no improvement in validation loss after x epochs\n",
    "#     reduce_on_plateau_patience=4,\n",
    "# )\n",
    "# print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQexRoPJopqa"
   },
   "outputs": [],
   "source": [
    "# # find optimal learning rate\n",
    "# res = trainer.tuner.lr_find(\n",
    "#     tft,\n",
    "#     train_dataloaders=train_dataloader,\n",
    "#     val_dataloaders=val_dataloader,\n",
    "#     max_lr=10.0,\n",
    "#     min_lr=1e-6,\n",
    "# )\n",
    "\n",
    "# # res = trainer.tuner.lr_find(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader,)\n",
    "\n",
    "# print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "# fig = res.plot(show=True, suggest=True)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x8ohrZoqosSX",
    "outputId": "4f54da2d-924c-4403-93ee-6ce56f78cc4c"
   },
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=300,\n",
    "    gpus=0,\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=1,  # 7 quantiles by default\n",
    "    loss=MAPE(),  #QuantileLoss(), #MAPE(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zcFzxHVrVQ-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorboard as tb \n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 939,
     "referenced_widgets": [
      "14d07476dd484e5e8a7bca7bf59ceae5",
      "c56fd12d72db4e4caf6d5ef0ade8b9f8",
      "fa10f2402f224290815d92ee89213162",
      "daf65e7e84e34266929b680250bc7a46",
      "ba34c1e6d0cb4b03870cb4aef1b6c32c",
      "cdd9d4ddccdc4e06a76aafddce2701f0",
      "ae9c80f166694da09e56af90999a7c6b",
      "5eeae71bc8384c98921f6ead0351b047",
      "74f31ab6b038471ca73833d340ba5606",
      "88d278124f704f90a67d37da11072342",
      "325556a59ba44f10bea30d875b032bcd"
     ]
    },
    "id": "L3O1F_Joo0YG",
    "outputId": "a2851582-3695-4376-9460-bdf095cbcd12"
   },
   "outputs": [],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyhBzIoi5cqP"
   },
   "outputs": [],
   "source": [
    "# load the best model according to the validation loss\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtZA-Mom5gaW"
   },
   "outputs": [],
   "source": [
    "# calcualte mean absolute error on validation set\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "predictions = best_tft.predict(val_dataloader)\n",
    "(actuals - predictions).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXtVdTBs5jZP"
   },
   "outputs": [],
   "source": [
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoYevMoT5wGA"
   },
   "outputs": [],
   "source": [
    "for idx in range(10):  # plot 10 examples\n",
    "    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQT6Kxwl53gu"
   },
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cufHwzV8pLiR"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "# create study\n",
    "study = optimize_hyperparameters(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=200,\n",
    "    max_epochs=50,\n",
    "    gradient_clip_val_range=(0.01, 1.0),\n",
    "    hidden_size_range=(8, 128),\n",
    "    hidden_continuous_size_range=(8, 128),\n",
    "    attention_head_size_range=(1, 4),\n",
    "    learning_rate_range=(0.001, 0.1),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=30),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08YeXDvqo3ZE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "14d07476dd484e5e8a7bca7bf59ceae5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c56fd12d72db4e4caf6d5ef0ade8b9f8",
       "IPY_MODEL_fa10f2402f224290815d92ee89213162",
       "IPY_MODEL_daf65e7e84e34266929b680250bc7a46"
      ],
      "layout": "IPY_MODEL_ba34c1e6d0cb4b03870cb4aef1b6c32c"
     }
    },
    "325556a59ba44f10bea30d875b032bcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5eeae71bc8384c98921f6ead0351b047": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74f31ab6b038471ca73833d340ba5606": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "88d278124f704f90a67d37da11072342": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae9c80f166694da09e56af90999a7c6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba34c1e6d0cb4b03870cb4aef1b6c32c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "c56fd12d72db4e4caf6d5ef0ade8b9f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdd9d4ddccdc4e06a76aafddce2701f0",
      "placeholder": "​",
      "style": "IPY_MODEL_ae9c80f166694da09e56af90999a7c6b",
      "value": "Sanity Checking DataLoader 0:   0%"
     }
    },
    "cdd9d4ddccdc4e06a76aafddce2701f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "daf65e7e84e34266929b680250bc7a46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88d278124f704f90a67d37da11072342",
      "placeholder": "​",
      "style": "IPY_MODEL_325556a59ba44f10bea30d875b032bcd",
      "value": " 0/2 [00:00&lt;?, ?it/s]"
     }
    },
    "fa10f2402f224290815d92ee89213162": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5eeae71bc8384c98921f6ead0351b047",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_74f31ab6b038471ca73833d340ba5606",
      "value": 0
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
